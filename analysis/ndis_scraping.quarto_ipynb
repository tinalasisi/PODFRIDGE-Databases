{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"NDIS Database Analysis\"\n",
        "subtitle: \"Parsing FBI National DNA Index System Statistics from Wayback Machine\"\n",
        "author: \"Tina Lasisi\"\n",
        "date: today\n",
        "format:\n",
        "  html:\n",
        "    code-fold: false\n",
        "    toc: true\n",
        "    toc-depth: 3\n",
        "execute:\n",
        "  echo: true\n",
        "  warning: false\n",
        "  freeze: auto  # prevents re-execution unless code changes\n",
        "---\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This analysis processes NDIS (National DNA Index System) statistics from archived FBI web pages. We parse 300+ HTML snapshots from the Wayback Machine to track how the DNA database has grown from 2010 to 2025.\n",
        "\n",
        "## Setup and Configuration\n"
      ],
      "id": "6599788c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# List of required packages\n",
        "required_packages = ['requests', 'beautifulsoup4', 'lxml', 'pandas', 'matplotlib', 'tqdm', 'numpy']\n",
        "\n",
        "# Install packages if not already installed\n",
        "for package in required_packages:\n",
        "    try:\n",
        "        __import__(package)\n",
        "    except ImportError:\n",
        "        print(f\"Installing {package}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])"
      ],
      "id": "8e2575ae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pathlib import Path\n",
        "import re, json, requests, time\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Configuration\n",
        "# Path setup - using current directory as base\n",
        "# This assumes you run the notebook from the project root (PODFRIDGE-Databases)\n",
        "BASE_DIR = Path(\"..\")  # Current working directory\n",
        "HTML_DIR = BASE_DIR / \"raw\" / \"wayback_html\"\n",
        "META_DIR = BASE_DIR / \"raw\" / \"wayback_meta\"\n",
        "OUTPUT_DIR = BASE_DIR / \"output\" / \"ndis\"\n",
        "\n",
        "# Create directories if they don't exist\n",
        "HTML_DIR.mkdir(parents=True, exist_ok=True)\n",
        "META_DIR.mkdir(parents=True, exist_ok=True)\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Jurisdiction name standardization mapping\n",
        "JURISDICTION_NAME_MAP = {\n",
        "    'D.C./FBI Lab': 'DC/FBI Lab',\n",
        "    'US Army': 'U.S. Army'\n",
        "}\n",
        "\n",
        "# Known data typos to fix\n",
        "KNOWN_TYPOS = [\n",
        "    {\n",
        "        'timestamp': '20250105164014',\n",
        "        'jurisdiction': 'California', \n",
        "        'field': 'investigations_aided',\n",
        "        'wrong_value': '1304657',  # How it parses\n",
        "        'correct_value': '130465'   # What it should be\n",
        "    },\n",
        "    {\n",
        "        'timestamp': '20250116205311',\n",
        "        'jurisdiction': 'California',\n",
        "        'field': 'investigations_aided', \n",
        "        'wrong_value': '1304657',\n",
        "        'correct_value': '130465'\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"Working directory: {BASE_DIR.resolve()}\")\n",
        "print(f\"HTML directory: {HTML_DIR}\")\n",
        "print(f\"Meta directory: {META_DIR}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")"
      ],
      "id": "6becfdf2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Wayback Machine Functions\n"
      ],
      "id": "a95979c1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def make_request_with_retry(params, max_retries=3, initial_delay=5):\n",
        "    \"\"\"Make a request with exponential backoff retry logic\"\"\"\n",
        "    base = \"https://web.archive.org/cdx/search/cdx\"\n",
        "    \n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            r = requests.get(base, params=params, timeout=30)\n",
        "            if r.status_code == 200:\n",
        "                return r\n",
        "            elif r.status_code == 429:  # Rate limited\n",
        "                wait_time = initial_delay * (2 ** attempt)\n",
        "                print(f\"    Rate limited. Waiting {wait_time} seconds...\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                return r\n",
        "        except requests.exceptions.ConnectionError as e:\n",
        "            wait_time = initial_delay * (2 ** attempt)\n",
        "            print(f\"    Connection error. Waiting {wait_time} seconds before retry {attempt + 1}/{max_retries}\")\n",
        "            time.sleep(wait_time)\n",
        "        except Exception as e:\n",
        "            print(f\"    Unexpected error: {e}\")\n",
        "            return None\n",
        "    return None"
      ],
      "id": "94729740",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Search for All NDIS Snapshots\n"
      ],
      "id": "ca70d4e4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| code-summary: \"Show search function code\"\n",
        "def search_all_ndis_snapshots():\n",
        "    \"\"\"Search for NDIS snapshots across all known URL variations\"\"\"\n",
        "    \n",
        "    # Search for both http and https variants\n",
        "    protocols = [\"http://\", \"https://\"]\n",
        "    subdomains = [\"www\", \"le\", \"*\"]  # Known subdomains plus wildcard\n",
        "    \n",
        "    all_rows = []\n",
        "    seen_timestamps = set()\n",
        "    \n",
        "    # First, try broad searches with protocol wildcards\n",
        "    print(\"Starting wildcard searches...\")\n",
        "    for protocol in protocols:\n",
        "        for subdomain in subdomains:\n",
        "            pattern = f\"{protocol}{subdomain}.fbi.gov/*ndis-statistics*\"\n",
        "            print(f\"\\nSearching: {pattern}\")\n",
        "            \n",
        "            params = {\n",
        "                \"url\":         pattern,\n",
        "                \"matchType\":   \"wildcard\",\n",
        "                \"output\":      \"json\",\n",
        "                \"fl\":          \"timestamp,original,mimetype,statuscode\",\n",
        "                \"filter\":      [\"statuscode:200\", \"mimetype:text/html\"],\n",
        "                \"limit\":       \"10000\",\n",
        "            }\n",
        "            \n",
        "            r = make_request_with_retry(params)\n",
        "            if r and r.status_code == 200:\n",
        "                data = json.loads(r.text)\n",
        "                if len(data) > 1:\n",
        "                    new_rows = 0\n",
        "                    for row in data[1:]:\n",
        "                        if row[0] not in seen_timestamps:\n",
        "                            all_rows.append(row)\n",
        "                            seen_timestamps.add(row[0])\n",
        "                            new_rows += 1\n",
        "                    print(f\"  → Found {new_rows} new snapshots\")\n",
        "                else:\n",
        "                    print(f\"  → No results\")\n",
        "            else:\n",
        "                print(f\"  → Failed after retries\")\n",
        "            \n",
        "            # Always wait between requests to avoid rate limiting\n",
        "            time.sleep(2)\n",
        "    \n",
        "    # Also search your specific known URLs with both protocols\n",
        "    known_paths = [\n",
        "        \"www.fbi.gov/about-us/lab/codis/ndis-statistics\",\n",
        "        \"www.fbi.gov/about-us/laboratory/biometric-analysis/codis/ndis-statistics\", \n",
        "        \"www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics\",\n",
        "        \"le.fbi.gov/science-and-lab/biometrics-and-fingerprints/codis/codis-ndis-statistics\",\n",
        "    ]\n",
        "    \n",
        "    print(\"\\n\\nStarting exact URL searches...\")\n",
        "    for path in known_paths:\n",
        "        for protocol in protocols:\n",
        "            url = f\"{protocol}{path}\"\n",
        "            print(f\"\\nSearching: {url}\")\n",
        "            \n",
        "            params = {\n",
        "                \"url\":         url,\n",
        "                \"matchType\":   \"exact\",\n",
        "                \"output\":      \"json\",\n",
        "                \"fl\":          \"timestamp,original,mimetype,statuscode\",\n",
        "                \"filter\":      [\"statuscode:200\", \"mimetype:text/html\"],\n",
        "                \"limit\":       \"10000\",\n",
        "            }\n",
        "            \n",
        "            r = make_request_with_retry(params)\n",
        "            if r and r.status_code == 200:\n",
        "                data = json.loads(r.text)\n",
        "                if len(data) > 1:\n",
        "                    new_rows = 0\n",
        "                    for row in data[1:]:\n",
        "                        if row[0] not in seen_timestamps:\n",
        "                            all_rows.append(row)\n",
        "                            seen_timestamps.add(row[0])\n",
        "                            new_rows += 1\n",
        "                    print(f\"  → Found {new_rows} new snapshots\")\n",
        "                else:\n",
        "                    print(f\"  → No results\")\n",
        "            else:\n",
        "                print(f\"  → Failed after retries\")\n",
        "            \n",
        "            # Always wait between requests\n",
        "            time.sleep(2)\n",
        "    \n",
        "    # Create DataFrame\n",
        "    snap_df = (pd.DataFrame(\n",
        "                    all_rows,\n",
        "                    columns=[\"timestamp\", \"original\", \"mimetype\", \"status\"])\n",
        "               .sort_values(\"timestamp\")\n",
        "               .reset_index(drop=True))\n",
        "    \n",
        "    return snap_df\n",
        "\n",
        "# Check if we already have snapshot data or need to search\n",
        "snapshot_csv = META_DIR / 'snapshots_found.csv'\n",
        "if snapshot_csv.exists():\n",
        "    print(\"Loading existing snapshot list...\")\n",
        "    snap_df = pd.read_csv(snapshot_csv)\n",
        "    print(f\"Loaded {len(snap_df)} snapshots\")\n",
        "else:\n",
        "    print(\"Searching for all NDIS snapshots...\")\n",
        "    snap_df = search_all_ndis_snapshots()\n",
        "    if len(snap_df) > 0:\n",
        "        snap_df.to_csv(snapshot_csv, index=False)\n",
        "        print(f\"\\nSaved {len(snap_df)} snapshots to {snapshot_csv}\")\n",
        "\n",
        "if len(snap_df) > 0:\n",
        "    print(f\"\\nTotal unique snapshots found: {len(snap_df):,}\")\n",
        "    print(f\"Unique URLs found: {snap_df['original'].nunique()}\")\n",
        "    print(\"\\nUnique URL patterns found:\")\n",
        "    for url in sorted(snap_df['original'].unique()):\n",
        "        print(f\"  {url}\")"
      ],
      "id": "20b88146",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Functions\n"
      ],
      "id": "f77a871a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def download_with_retry(url, max_retries=3, initial_delay=5, consecutive_failures=0):\n",
        "    \"\"\"Download with adaptive retry logic based on consecutive failures\"\"\"\n",
        "    if consecutive_failures > 0:\n",
        "        extra_wait = consecutive_failures * 10\n",
        "        print(f\"\\n    Adding {extra_wait}s cooldown due to {consecutive_failures} consecutive failures...\")\n",
        "        time.sleep(extra_wait)\n",
        "    \n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.get(url, timeout=30)\n",
        "            response.raise_for_status()\n",
        "            return response, True\n",
        "        except requests.exceptions.ConnectionError as e:\n",
        "            wait_time = initial_delay * (2 ** attempt)\n",
        "            print(f\"\\n    Connection error. Waiting {wait_time} seconds before retry {attempt + 1}/{max_retries}\")\n",
        "            time.sleep(wait_time)\n",
        "        except requests.exceptions.HTTPError as e:\n",
        "            if e.response.status_code == 429:\n",
        "                wait_time = initial_delay * (2 ** attempt) * 2\n",
        "                print(f\"\\n    Rate limited (429). Waiting {wait_time} seconds...\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                print(f\"\\n    HTTP Error: {e}\")\n",
        "                return None, False\n",
        "        except Exception as e:\n",
        "            print(f\"\\n    Unexpected error: {e}\")\n",
        "            return None, False\n",
        "    return None, False\n",
        "\n",
        "def download_missing_snapshots(snap_df, output_folder):\n",
        "    \"\"\"Download HTML snapshots with resume capability and detailed logging\"\"\"\n",
        "    \n",
        "    # Create run-specific log file\n",
        "    run_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    run_log_file = META_DIR / f\"download_log_{run_timestamp}.txt\"\n",
        "    \n",
        "    # Check what we already have\n",
        "    existing_files = list(output_folder.glob(\"*.html\"))\n",
        "    existing_timestamps = {f.stem for f in existing_files}\n",
        "    print(f\"\\nFiles already downloaded: {len(existing_files)}\")\n",
        "    \n",
        "    # Check what needs to be downloaded\n",
        "    to_download = []\n",
        "    for _, row in snap_df.iterrows():\n",
        "        timestamp = row['timestamp']\n",
        "        url = row['original']\n",
        "        filename = output_folder / f\"{timestamp}.html\"\n",
        "        \n",
        "        if timestamp not in existing_timestamps and not filename.exists():\n",
        "            to_download.append((timestamp, url, filename))\n",
        "    \n",
        "    print(f\"Files to download: {len(to_download)}\")\n",
        "    \n",
        "    # Initialize log file\n",
        "    with open(run_log_file, \"w\") as log:\n",
        "        log.write(f\"Download run started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        log.write(f\"Total snapshots in list: {len(snap_df)}\\n\")\n",
        "        log.write(f\"Already downloaded: {len(existing_files)}\\n\")\n",
        "        log.write(f\"To download: {len(to_download)}\\n\")\n",
        "        log.write(f\"{'='*60}\\n\\n\")\n",
        "    \n",
        "    if len(to_download) == 0:\n",
        "        print(\"\\n✓ All files already downloaded! Nothing to do.\")\n",
        "        with open(run_log_file, \"a\") as log:\n",
        "            log.write(\"All files already downloaded. No action needed.\\n\")\n",
        "        return\n",
        "    \n",
        "    # Download configuration\n",
        "    BATCH_SIZE = 15\n",
        "    PAUSE_BETWEEN_DOWNLOADS = 3\n",
        "    PAUSE_BETWEEN_BATCHES = 45\n",
        "    PAUSE_AFTER_FAILURE = 60\n",
        "    \n",
        "    # Track statistics\n",
        "    successful_downloads = 0\n",
        "    failed_downloads = []\n",
        "    consecutive_failures = 0\n",
        "    \n",
        "    # Download in batches\n",
        "    for i in range(0, len(to_download), BATCH_SIZE):\n",
        "        batch = to_download[i:i + BATCH_SIZE]\n",
        "        batch_num = (i // BATCH_SIZE) + 1\n",
        "        total_batches = (len(to_download) + BATCH_SIZE - 1) // BATCH_SIZE\n",
        "        \n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Batch {batch_num}/{total_batches} ({len(batch)} files)\")\n",
        "        print(f\"Overall progress: {len(existing_timestamps) + successful_downloads}/{len(snap_df)} total files\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        with open(run_log_file, \"a\") as log:\n",
        "            log.write(f\"\\nBatch {batch_num}/{total_batches} started at {datetime.now().strftime('%H:%M:%S')}\\n\")\n",
        "        \n",
        "        for j, (timestamp, url, filename) in enumerate(batch, 1):\n",
        "            # Double-check file doesn't exist\n",
        "            if filename.exists():\n",
        "                print(f\"\\n[{j}/{len(batch)}] {timestamp} - Already exists, skipping...\")\n",
        "                with open(run_log_file, \"a\") as log:\n",
        "                    log.write(f\"{timestamp}  ⚡ already exists\\n\")\n",
        "                continue\n",
        "                \n",
        "            wayback_url = f\"https://web.archive.org/web/{timestamp}/{url}\"\n",
        "            \n",
        "            print(f\"\\n[{j}/{len(batch)}] Downloading {timestamp}...\", end=\"\")\n",
        "            \n",
        "            response, success = download_with_retry(wayback_url, consecutive_failures=consecutive_failures)\n",
        "            \n",
        "            if response and response.status_code == 200:\n",
        "                try:\n",
        "                    with open(filename, 'w', encoding='utf-8') as f:\n",
        "                        f.write(response.text)\n",
        "                    \n",
        "                    print(\" ✓ Success\")\n",
        "                    successful_downloads += 1\n",
        "                    consecutive_failures = 0\n",
        "                    \n",
        "                    with open(run_log_file, \"a\") as log:\n",
        "                        log.write(f\"{timestamp}  ✓ downloaded\\n\")\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    print(f\" ✗ Error saving file: {e}\")\n",
        "                    failed_downloads.append((timestamp, url, str(e)))\n",
        "                    consecutive_failures += 1\n",
        "                    \n",
        "                    with open(run_log_file, \"a\") as log:\n",
        "                        log.write(f\"{timestamp}  ✗ failed: {str(e)}\\n\")\n",
        "            else:\n",
        "                print(\" ✗ Failed after retries\")\n",
        "                failed_downloads.append((timestamp, url, \"Download failed\"))\n",
        "                consecutive_failures += 1\n",
        "                \n",
        "                with open(run_log_file, \"a\") as log:\n",
        "                    log.write(f\"{timestamp}  ✗ failed: Download failed after retries\\n\")\n",
        "                \n",
        "                if j < len(batch):\n",
        "                    print(f\"    Taking {PAUSE_AFTER_FAILURE}s break after failure...\")\n",
        "                    time.sleep(PAUSE_AFTER_FAILURE)\n",
        "                    continue\n",
        "            \n",
        "            if j < len(batch) and consecutive_failures == 0:\n",
        "                print(f\"    Waiting {PAUSE_BETWEEN_DOWNLOADS} seconds...\")\n",
        "                time.sleep(PAUSE_BETWEEN_DOWNLOADS)\n",
        "        \n",
        "        if i + BATCH_SIZE < len(to_download):\n",
        "            print(f\"\\nBatch complete. Pausing {PAUSE_BETWEEN_BATCHES} seconds...\")\n",
        "            print(f\"This session: {successful_downloads} downloaded, {len(failed_downloads)} failed\")\n",
        "            time.sleep(PAUSE_BETWEEN_BATCHES)\n",
        "    \n",
        "    # Final summary\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Download session complete!\")\n",
        "    print(f\"  Successfully downloaded: {successful_downloads}\")\n",
        "    print(f\"  Failed downloads: {len(failed_downloads)}\")\n",
        "    \n",
        "    # Write final summary to log\n",
        "    with open(run_log_file, \"a\") as log:\n",
        "        log.write(f\"\\n{'='*60}\\n\")\n",
        "        log.write(f\"Download run completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        log.write(f\"Successfully downloaded: {successful_downloads}\\n\")\n",
        "        log.write(f\"Failed downloads: {len(failed_downloads)}\\n\")\n",
        "        \n",
        "        if failed_downloads:\n",
        "            log.write(f\"\\nFailed downloads detail:\\n\")\n",
        "            for timestamp, url, error in failed_downloads:\n",
        "                log.write(f\"  {timestamp}: {error}\\n\")\n",
        "    \n",
        "    if failed_downloads:\n",
        "        print(f\"\\nFailed downloads:\")\n",
        "        for timestamp, url, error in failed_downloads[:10]:\n",
        "            print(f\"  {timestamp}: {error}\")\n",
        "        if len(failed_downloads) > 10:\n",
        "            print(f\"  ... and {len(failed_downloads) - 10} more\")\n",
        "    \n",
        "    print(f\"\\nDownload log saved to: {run_log_file}\")\n",
        "\n",
        "# Download missing files\n",
        "if len(snap_df) > 0:\n",
        "    download_missing_snapshots(snap_df, HTML_DIR)"
      ],
      "id": "ff3d4c6a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Status Check Functions\n"
      ],
      "id": "2bc9f33b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_latest_log():\n",
        "    \"\"\"Find and read the most recent download log\"\"\"\n",
        "    log_files = sorted(META_DIR.glob(\"download_log_*.txt\"))\n",
        "    if not log_files:\n",
        "        print(\"No download logs found.\")\n",
        "        return None\n",
        "    \n",
        "    latest_log = log_files[-1]\n",
        "    print(f\"Latest log: {latest_log.name}\")\n",
        "    return latest_log\n",
        "\n",
        "def analyze_download_status():\n",
        "    \"\"\"Analyze the current download status and latest run results\"\"\"\n",
        "    \n",
        "    # Check what we have\n",
        "    html_files = list(HTML_DIR.glob(\"*.html\"))\n",
        "    downloaded_timestamps = {f.stem for f in html_files}\n",
        "    \n",
        "    # Check what we should have\n",
        "    snapshot_csv = META_DIR / 'snapshots_found.csv'\n",
        "    if snapshot_csv.exists():\n",
        "        snap_df = pd.read_csv(snapshot_csv)\n",
        "        expected_timestamps = set(snap_df['timestamp'].astype(str))\n",
        "    else:\n",
        "        print(\"No snapshot list found. Run search first.\")\n",
        "        return None\n",
        "    \n",
        "    # Calculate missing\n",
        "    missing_timestamps = expected_timestamps - downloaded_timestamps\n",
        "    \n",
        "    # Parse latest log for failures\n",
        "    latest_log = get_latest_log()\n",
        "    failed_in_last_run = []\n",
        "    \n",
        "    if latest_log:\n",
        "        with open(latest_log, 'r') as f:\n",
        "            for line in f:\n",
        "                if '✗ failed:' in line:\n",
        "                    timestamp = line.split()[0]\n",
        "                    if timestamp.isdigit() and len(timestamp) == 14:\n",
        "                        failed_in_last_run.append(timestamp)\n",
        "    \n",
        "    # Create summary\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"DOWNLOAD STATUS SUMMARY\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Expected snapshots: {len(expected_timestamps)}\")\n",
        "    print(f\"Downloaded: {len(downloaded_timestamps)} ({len(downloaded_timestamps)/len(expected_timestamps)*100:.1f}%)\")\n",
        "    print(f\"Missing: {len(missing_timestamps)}\")\n",
        "    \n",
        "    if latest_log:\n",
        "        print(f\"\\nLatest run ({latest_log.name}):\")\n",
        "        print(f\"  Failed downloads: {len(failed_in_last_run)}\")\n",
        "        if failed_in_last_run:\n",
        "            print(f\"  Failed timestamps: {', '.join(failed_in_last_run[:5])}\")\n",
        "            if len(failed_in_last_run) > 5:\n",
        "                print(f\"  ... and {len(failed_in_last_run) - 5} more\")\n",
        "    \n",
        "    print(f\"\\nTotal still needed: {len(missing_timestamps)}\")\n",
        "    \n",
        "    return {\n",
        "        'missing': missing_timestamps,\n",
        "        'failed_last_run': failed_in_last_run,\n",
        "        'downloaded': downloaded_timestamps,\n",
        "        'expected': expected_timestamps\n",
        "    }\n",
        "\n",
        "def create_retry_list(status_info, retry_only_failed=True):\n",
        "    \"\"\"Create a list of files to retry downloading\"\"\"\n",
        "    if not status_info:\n",
        "        return None\n",
        "    \n",
        "    if retry_only_failed and status_info['failed_last_run']:\n",
        "        retry_timestamps = set(status_info['failed_last_run'])\n",
        "        print(f\"\\nWill retry {len(retry_timestamps)} failed downloads from last run\")\n",
        "    else:\n",
        "        retry_timestamps = status_info['missing']\n",
        "        print(f\"\\nWill retry all {len(retry_timestamps)} missing files\")\n",
        "    \n",
        "    snapshot_csv = META_DIR / 'snapshots_found.csv'\n",
        "    snap_df = pd.read_csv(snapshot_csv)\n",
        "    retry_df = snap_df[snap_df['timestamp'].astype(str).isin(retry_timestamps)]\n",
        "    \n",
        "    return retry_df"
      ],
      "id": "af34e9b3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parser Functions\n"
      ],
      "id": "c3e956a5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def clean_jurisdiction_name(name):\n",
        "    \"\"\"Clean up jurisdiction names by removing common prefixes\"\"\"\n",
        "    name = re.sub(r'^.*?Back to top\\s*', '', name)\n",
        "    name = re.sub(r'^.*?Tables by NDIS Participant\\s*', '', name)\n",
        "    name = re.sub(r'^.*?ation\\.\\s*', '', name)\n",
        "    name = name.strip()\n",
        "    return name\n",
        "\n",
        "def standardize_jurisdiction_name(name):\n",
        "    \"\"\"Standardize jurisdiction names to handle variations\"\"\"\n",
        "    name = clean_jurisdiction_name(name)\n",
        "    if name in JURISDICTION_NAME_MAP:\n",
        "        return JURISDICTION_NAME_MAP[name]\n",
        "    return name\n",
        "\n",
        "def extract_data_date(html_content):\n",
        "    \"\"\"Extract the 'Statistics as of' date from HTML content\"\"\"\n",
        "    match = re.search(r'Statistics as of (\\w+ \\d{4})', html_content, re.IGNORECASE)\n",
        "    if match:\n",
        "        date_str = match.group(1)\n",
        "        try:\n",
        "            # Convert \"October 2024\" to datetime\n",
        "            return datetime.strptime(date_str, \"%B %Y\")\n",
        "        except:\n",
        "            pass\n",
        "    return None\n",
        "\n",
        "def parse_ndis_snapshot(html_file):\n",
        "    \"\"\"Parse a single NDIS snapshot file\"\"\"\n",
        "    timestamp = html_file.stem\n",
        "    year = int(timestamp[:4])\n",
        "    \n",
        "    html_content = html_file.read_text('utf-8', errors='ignore')\n",
        "    soup = BeautifulSoup(html_content, 'lxml')\n",
        "    text = soup.get_text(' ', strip=True)\n",
        "    \n",
        "    # Extract the \"as of\" date\n",
        "    data_date = extract_data_date(html_content)\n",
        "    \n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    \n",
        "    records = []\n",
        "    \n",
        "    # Pattern for 2010 (no arrestee data)\n",
        "    if year <= 2010:\n",
        "        pattern = re.compile(\n",
        "            r'([A-Z][a-zA-Z\\s\\.\\-\\'/&()]{2,50}?)\\s+Statistical Information\\s+'\n",
        "            r'.*?Offender Profiles\\s+([\\d,]+)\\s+'\n",
        "            r'.*?Forensic Samples\\s+([\\d,]+)\\s+'\n",
        "            r'.*?NDIS Participating Labs\\s+(\\d+)\\s+'\n",
        "            r'.*?Investigations Aided\\s+([\\d,]+)',\n",
        "            re.I\n",
        "        )\n",
        "        \n",
        "        for match in pattern.finditer(text):\n",
        "            jurisdiction_raw, offender, forensic, labs, investigations = match.groups()\n",
        "            jurisdiction = standardize_jurisdiction_name(jurisdiction_raw)\n",
        "            \n",
        "            records.append({\n",
        "                'timestamp': timestamp,\n",
        "                'jurisdiction': jurisdiction,\n",
        "                'offender_profiles': offender.replace(',', ''),\n",
        "                'arrestee': '0',\n",
        "                'forensic_profiles': forensic.replace(',', ''),\n",
        "                'ndis_labs': labs,\n",
        "                'investigations_aided': investigations.replace(',', ''),\n",
        "                'data_as_of_date': data_date\n",
        "            })\n",
        "    else:\n",
        "        # Pattern for 2011+ (includes arrestee data)\n",
        "        pattern = re.compile(\n",
        "            r'([A-Z][a-zA-Z\\s\\.\\-\\'/&()]{2,50}?)\\s+Statistical Information\\s+'\n",
        "            r'.*?Offender Profiles\\s+([\\d,]+)\\s+'\n",
        "            r'.*?Arrestee\\s+([\\d,]+)\\s+'\n",
        "            r'.*?Forensic Profiles\\s+([\\d,]+)\\s+'\n",
        "            r'.*?NDIS Participating Labs\\s+(\\d+)\\s+'\n",
        "            r'.*?Investigations Aided\\s+([\\d,]+)',\n",
        "            re.I\n",
        "        )\n",
        "        \n",
        "        for match in pattern.finditer(text):\n",
        "            jurisdiction_raw, offender, arrestee, forensic, labs, investigations = match.groups()\n",
        "            jurisdiction = standardize_jurisdiction_name(jurisdiction_raw)\n",
        "            \n",
        "            records.append({\n",
        "                'timestamp': timestamp,\n",
        "                'jurisdiction': jurisdiction,\n",
        "                'offender_profiles': offender.replace(',', ''),\n",
        "                'arrestee': arrestee.replace(',', ''),\n",
        "                'forensic_profiles': forensic.replace(',', ''),\n",
        "                'ndis_labs': labs,\n",
        "                'investigations_aided': investigations.replace(',', ''),\n",
        "                'data_as_of_date': data_date\n",
        "            })\n",
        "    \n",
        "    return records"
      ],
      "id": "75f61a66",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Process All Snapshots\n"
      ],
      "id": "af64efa9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def process_all_snapshots():\n",
        "    \"\"\"Parse all downloaded snapshots and create datasets\"\"\"\n",
        "    print(\"Processing all snapshots...\")\n",
        "    \n",
        "    all_records = []\n",
        "    html_files = sorted(HTML_DIR.glob(\"*.html\"))\n",
        "    \n",
        "    for html_file in tqdm(html_files, desc=\"Parsing HTML files\"):\n",
        "        try:\n",
        "            records = parse_ndis_snapshot(html_file)\n",
        "            all_records.extend(records)\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing {html_file.name}: {e}\")\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(all_records)\n",
        "    \n",
        "    # Convert numeric fields\n",
        "    numeric_fields = ['offender_profiles', 'arrestee', 'forensic_profiles', 'ndis_labs', 'investigations_aided']\n",
        "    for field in numeric_fields:\n",
        "        df[field] = pd.to_numeric(df[field], errors='coerce').fillna(0).astype(int)\n",
        "    \n",
        "    # Add datetime columns\n",
        "    df['capture_datetime'] = pd.to_datetime(df['timestamp'], format='%Y%m%d%H%M%S')\n",
        "    df['capture_date'] = df['capture_datetime'].dt.date\n",
        "    \n",
        "    # Sort by timestamp and jurisdiction\n",
        "    df = df.sort_values(['timestamp', 'jurisdiction'])\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Process all files\n",
        "df_raw = process_all_snapshots()\n",
        "print(f\"\\nProcessed {len(df_raw)} total records\")\n",
        "print(f\"Unique jurisdictions: {df_raw['jurisdiction'].nunique()}\")\n",
        "print(f\"Date range: {df_raw['capture_datetime'].min()} to {df_raw['capture_datetime'].max()}\")"
      ],
      "id": "d01e0545",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check Download Completeness\n"
      ],
      "id": "ad4b7f51"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check if we have all expected files\n",
        "print(\"\\nChecking download completeness...\")\n",
        "status = analyze_download_status()\n",
        "\n",
        "# Automatically retry if there are failures\n",
        "if status and status['failed_last_run'] and len(status['failed_last_run']) > 0:\n",
        "    print(f\"\\n⚠️  Found {len(status['failed_last_run'])} failed downloads from last run. Retrying...\")\n",
        "    retry_df = create_retry_list(status, retry_only_failed=True)\n",
        "    if retry_df is not None and len(retry_df) > 0:\n",
        "        download_missing_snapshots(retry_df, HTML_DIR)\n",
        "        \n",
        "        # Re-check status after retry\n",
        "        print(\"\\nRechecking status after retry...\")\n",
        "        status = analyze_download_status()\n",
        "elif status and status['missing']:\n",
        "    print(f\"\\n⚠️  {len(status['missing'])} files are missing but weren't from a failed run.\")\n",
        "    print(\"These may be new snapshots. Run download cell manually if needed.\")\n",
        "else:\n",
        "    print(\"\\n✅ All files successfully downloaded!\")"
      ],
      "id": "41ceeafd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Apply Typo Fixes\n"
      ],
      "id": "9855cbeb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def apply_typo_fixes(df):\n",
        "    \"\"\"Apply known typo corrections\"\"\"\n",
        "    df_fixed = df.copy()\n",
        "    \n",
        "    for typo in KNOWN_TYPOS:\n",
        "        mask = (\n",
        "            (df_fixed['timestamp'] == typo['timestamp']) & \n",
        "            (df_fixed['jurisdiction'] == typo['jurisdiction'])\n",
        "        )\n",
        "        if mask.any():\n",
        "            df_fixed.loc[mask, typo['field']] = int(typo['correct_value'])\n",
        "            print(f\"Fixed typo: {typo['jurisdiction']} on {typo['timestamp'][:8]} - \"\n",
        "                  f\"{typo['field']} from {typo['wrong_value']} to {typo['correct_value']}\")\n",
        "    \n",
        "    return df_fixed\n",
        "\n",
        "# Apply fixes\n",
        "df_fixed = apply_typo_fixes(df_raw)"
      ],
      "id": "008a2dda",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Datasets\n"
      ],
      "id": "490abfd2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Save datasets\n",
        "df_raw.to_csv(OUTPUT_DIR / 'ndis_data_raw.csv', index=False)\n",
        "df_fixed.to_csv(OUTPUT_DIR / 'ndis_data_fixed.csv', index=False)\n",
        "print(f\"\\nSaved raw data to: {OUTPUT_DIR / 'ndis_data_raw.csv'}\")\n",
        "print(f\"Saved fixed data to: {OUTPUT_DIR / 'ndis_data_fixed.csv'}\")"
      ],
      "id": "58405e97",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary Statistics\n"
      ],
      "id": "bcb913d7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate summary statistics\n",
        "latest_data = df_fixed[df_fixed['capture_datetime'] == df_fixed['capture_datetime'].max()]\n",
        "latest_data = latest_data[latest_data['jurisdiction'] != 'D.C./Metro PD']\n",
        "\n",
        "print(\"\\nLatest Statistics Summary:\")\n",
        "print(f\"  As of: {latest_data['capture_datetime'].iloc[0]}\")\n",
        "print(f\"  Data from: {latest_data['data_as_of_date'].iloc[0] if latest_data['data_as_of_date'].iloc[0] else 'Unknown'}\")\n",
        "print(f\"  Jurisdictions reporting: {len(latest_data)}\")\n",
        "print(f\"  Total offender profiles: {latest_data['offender_profiles'].sum():,}\")\n",
        "print(f\"  Total arrestee profiles: {latest_data['arrestee'].sum():,}\")\n",
        "print(f\"  Total forensic profiles: {latest_data['forensic_profiles'].sum():,}\")\n",
        "print(f\"  Total investigations aided: {latest_data['investigations_aided'].sum():,}\")"
      ],
      "id": "6b4fdacd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizations\n"
      ],
      "id": "70925aaa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 16,
        "fig-height": 18
      },
      "source": [
        "def create_visualizations(df_raw, df_fixed):\n",
        "    \"\"\"Create comprehensive visualizations\"\"\"\n",
        "    # Exclude D.C./Metro PD from visualizations\n",
        "    df_raw_viz = df_raw[df_raw['jurisdiction'] != 'D.C./Metro PD']\n",
        "    df_fixed_viz = df_fixed[df_fixed['jurisdiction'] != 'D.C./Metro PD']\n",
        "    \n",
        "    # Create figure with subplots\n",
        "    fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
        "    \n",
        "    # 1. Jurisdictions reporting over time\n",
        "    for ax, (df, title_suffix) in zip([axes[0,0], axes[0,1]], \n",
        "                                      [(df_raw_viz, 'Raw'), (df_fixed_viz, 'Fixed')]):\n",
        "        jurisdictions_per_date = df.groupby('capture_datetime')['jurisdiction'].nunique()\n",
        "        ax.plot(jurisdictions_per_date.index, jurisdictions_per_date.values, 'b-', linewidth=2)\n",
        "        ax.set_title(f'Jurisdictions Reporting ({title_suffix})')\n",
        "        ax.set_ylabel('Number of Jurisdictions')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
        "    \n",
        "    # 2. Total investigations aided\n",
        "    for ax, (df, title_suffix) in zip([axes[1,0], axes[1,1]], \n",
        "                                      [(df_raw_viz, 'Raw with Typo'), (df_fixed_viz, 'Fixed')]):\n",
        "        total_inv = df.groupby('capture_datetime')['investigations_aided'].sum()\n",
        "        ax.plot(total_inv.index, total_inv.values / 1e3, 'purple', linewidth=2)\n",
        "        ax.set_title(f'Total Investigations Aided ({title_suffix})')\n",
        "        ax.set_ylabel('Thousands of Investigations')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Highlight the typo in raw data\n",
        "        if 'Raw' in title_suffix:\n",
        "            typo_dates = df[(df['jurisdiction'] == 'California') & \n",
        "                          (df['investigations_aided'] > 1000000)]['capture_datetime']\n",
        "            for date in typo_dates:\n",
        "                ax.axvline(x=date, color='red', linestyle='--', alpha=0.5)\n",
        "                ax.text(date, ax.get_ylim()[1]*0.9, 'Typo', rotation=90, \n",
        "                       verticalalignment='bottom', color='red')\n",
        "    \n",
        "    # 3. Data lag analysis\n",
        "    ax = axes[2, 0]\n",
        "    df_with_lag = df_fixed_viz[df_fixed_viz['data_as_of_date'].notna()].copy()\n",
        "    df_with_lag['data_lag_days'] = (df_with_lag['capture_datetime'] - df_with_lag['data_as_of_date']).dt.days\n",
        "    \n",
        "    avg_lag = df_with_lag.groupby('capture_datetime')['data_lag_days'].mean()\n",
        "    ax.plot(avg_lag.index, avg_lag.values, 'orange', linewidth=2)\n",
        "    ax.set_title('Average Data Lag (Capture Date vs \"As Of\" Date)')\n",
        "    ax.set_ylabel('Days')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 4. California investigations over time (showing typo fix)\n",
        "    ax = axes[2, 1]\n",
        "    cal_raw = df_raw_viz[df_raw_viz['jurisdiction'] == 'California']\n",
        "    cal_fixed = df_fixed_viz[df_fixed_viz['jurisdiction'] == 'California']\n",
        "    \n",
        "    ax.plot(cal_raw['capture_datetime'], cal_raw['investigations_aided'], \n",
        "            'r-', label='Raw (with typo)', linewidth=2, alpha=0.7)\n",
        "    ax.plot(cal_fixed['capture_datetime'], cal_fixed['investigations_aided'], \n",
        "            'g-', label='Fixed', linewidth=2)\n",
        "    ax.set_title('California Investigations Aided: Raw vs Fixed')\n",
        "    ax.set_ylabel('Investigations Aided')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(OUTPUT_DIR / 'ndis_analysis_complete.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Create visualizations\n",
        "print(\"\\nCreating visualizations...\")\n",
        "create_visualizations(df_raw, df_fixed)\n",
        "print(\"\\nProcessing complete!\")"
      ],
      "id": "8edeba46",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}