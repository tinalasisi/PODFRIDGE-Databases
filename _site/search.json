[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PODFRIDGE-Databases",
    "section": "",
    "text": "This project analyzes 15+ years of CODIS statistics scraped from the Wayback Machine, revealing growth patterns, data anomalies, and the evolution of DNA databasing in the United States.\n\n\n\n\n\n\n\n\nThe California Typo\n\n\n\nWe discovered a data entry error in California’s October 2024 data where investigations aided was listed as “130,4657” instead of “130,465” - causing an apparent spike of over 1.3 million investigations!\n\n\n\nTotal Growth: Offender profiles grew from 8.5M (2010) to 18.4M (2025)\nData Lag: FBI statistics are typically 2-3 months old when published\nJurisdictional Changes: D.C./Metro PD stopped reporting after 2018\n\nContinue to full analysis →"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "PODFRIDGE-Databases",
    "section": "",
    "text": "This project analyzes 15+ years of CODIS statistics scraped from the Wayback Machine, revealing growth patterns, data anomalies, and the evolution of DNA databasing in the United States.\n\n\n\n\n\n\n\n\nThe California Typo\n\n\n\nWe discovered a data entry error in California’s October 2024 data where investigations aided was listed as “130,4657” instead of “130,465” - causing an apparent spike of over 1.3 million investigations!\n\n\n\nTotal Growth: Offender profiles grew from 8.5M (2010) to 18.4M (2025)\nData Lag: FBI statistics are typically 2-3 months old when published\nJurisdictional Changes: D.C./Metro PD stopped reporting after 2018\n\nContinue to full analysis →"
  },
  {
    "objectID": "index.html#quick-stats",
    "href": "index.html#quick-stats",
    "title": "PODFRIDGE-Databases",
    "section": "Quick Stats",
    "text": "Quick Stats\n#| echo: false #| warning: false import pandas as pd import matplotlib.pyplot as plt"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "ndis_analysis.html",
    "href": "ndis_analysis.html",
    "title": "NDIS Database Analysis",
    "section": "",
    "text": "This analysis processes NDIS (National DNA Index System) statistics from archived FBI web pages. We parse 300+ HTML snapshots from the Wayback Machine to track how the DNA database has grown from 2010 to 2025."
  },
  {
    "objectID": "ndis_analysis.html#introduction",
    "href": "ndis_analysis.html#introduction",
    "title": "NDIS Database Analysis",
    "section": "",
    "text": "This analysis processes NDIS (National DNA Index System) statistics from archived FBI web pages. We parse 300+ HTML snapshots from the Wayback Machine to track how the DNA database has grown from 2010 to 2025."
  },
  {
    "objectID": "ndis_analysis.html#setup",
    "href": "ndis_analysis.html#setup",
    "title": "NDIS Database Analysis",
    "section": "Setup",
    "text": "Setup\n\nimport re\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nfrom datetime import datetime\nfrom bs4 import BeautifulSoup\nfrom tqdm import tqdm\n\n# Set up paths\nOUTPUT_DIR = Path(\"output\")\nHTML_DIR = OUTPUT_DIR / \"wayback_html\"\n\n# Check that we have the HTML files\nhtml_files = list(HTML_DIR.glob(\"*.html\"))\nprint(f\"Found {len(html_files)} HTML files to process\")\n\nFound 317 HTML files to process"
  },
  {
    "objectID": "ndis_analysis.html#configuration",
    "href": "ndis_analysis.html#configuration",
    "title": "NDIS Database Analysis",
    "section": "Configuration",
    "text": "Configuration\nDefine our jurisdiction mappings and known typos:\n\n# Jurisdiction name standardization\nJURISDICTION_NAME_MAP = {\n    'D.C./FBI Lab': 'DC/FBI Lab',\n    'US Army': 'U.S. Army'\n}\n\n# Known data typos to fix\nKNOWN_TYPOS = [\n    {\n        'timestamp': '20250105164014',\n        'jurisdiction': 'California', \n        'field': 'investigations_aided',\n        'wrong_value': '1304657',\n        'correct_value': '130465'\n    },\n    {\n        'timestamp': '20250116205311',\n        'jurisdiction': 'California',\n        'field': 'investigations_aided', \n        'wrong_value': '1304657',\n        'correct_value': '130465'\n    }\n]"
  },
  {
    "objectID": "ndis_analysis.html#parser-functions",
    "href": "ndis_analysis.html#parser-functions",
    "title": "NDIS Database Analysis",
    "section": "Parser Functions",
    "text": "Parser Functions\n\ndef clean_jurisdiction_name(name):\n    \"\"\"Clean up jurisdiction names by removing common prefixes\"\"\"\n    name = re.sub(r'^.*?Back to top\\s*', '', name)\n    name = re.sub(r'^.*?Tables by NDIS Participant\\s*', '', name)\n    name = re.sub(r'^.*?ation\\.\\s*', '', name)\n    name = name.strip()\n    return name\n\ndef standardize_jurisdiction_name(name):\n    \"\"\"Standardize jurisdiction names to handle variations\"\"\"\n    name = clean_jurisdiction_name(name)\n    if name in JURISDICTION_NAME_MAP:\n        return JURISDICTION_NAME_MAP[name]\n    return name\n\ndef extract_data_date(html_content):\n    \"\"\"Extract the 'Statistics as of' date from HTML content\"\"\"\n    match = re.search(r'Statistics as of (\\w+ \\d{4})', html_content, re.IGNORECASE)\n    if match:\n        date_str = match.group(1)\n        try:\n            # Convert \"October 2024\" to datetime\n            return datetime.strptime(date_str, \"%B %Y\")\n        except:\n            pass\n    return None\n\ndef parse_ndis_snapshot(html_file):\n    \"\"\"Parse a single NDIS snapshot file\"\"\"\n    timestamp = html_file.stem\n    year = int(timestamp[:4])\n    \n    html_content = html_file.read_text('utf-8', errors='ignore')\n    soup = BeautifulSoup(html_content, 'lxml')\n    text = soup.get_text(' ', strip=True)\n    \n    # Extract the \"as of\" date\n    data_date = extract_data_date(html_content)\n    \n    # Normalize whitespace\n    text = re.sub(r'\\s+', ' ', text)\n    \n    records = []\n    \n    # Pattern for 2010 (no arrestee data)\n    if year &lt;= 2010:\n        pattern = re.compile(\n            r'([A-Z][a-zA-Z\\s\\.\\-\\'/&()]{2,50}?)\\s+Statistical Information\\s+'\n            r'.*?Offender Profiles\\s+([\\d,]+)\\s+'\n            r'.*?Forensic Samples\\s+([\\d,]+)\\s+'\n            r'.*?NDIS Participating Labs\\s+(\\d+)\\s+'\n            r'.*?Investigations Aided\\s+([\\d,]+)',\n            re.I\n        )\n        \n        for match in pattern.finditer(text):\n            jurisdiction_raw, offender, forensic, labs, investigations = match.groups()\n            jurisdiction = standardize_jurisdiction_name(jurisdiction_raw)\n            \n            records.append({\n                'timestamp': timestamp,\n                'jurisdiction': jurisdiction,\n                'offender_profiles': offender.replace(',', ''),\n                'arrestee': '0',\n                'forensic_profiles': forensic.replace(',', ''),\n                'ndis_labs': labs,\n                'investigations_aided': investigations.replace(',', ''),\n                'data_as_of_date': data_date\n            })\n    else:\n        # Pattern for 2011+ (includes arrestee data)\n        pattern = re.compile(\n            r'([A-Z][a-zA-Z\\s\\.\\-\\'/&()]{2,50}?)\\s+Statistical Information\\s+'\n            r'.*?Offender Profiles\\s+([\\d,]+)\\s+'\n            r'.*?Arrestee\\s+([\\d,]+)\\s+'\n            r'.*?Forensic Profiles\\s+([\\d,]+)\\s+'\n            r'.*?NDIS Participating Labs\\s+(\\d+)\\s+'\n            r'.*?Investigations Aided\\s+([\\d,]+)',\n            re.I\n        )\n        \n        for match in pattern.finditer(text):\n            jurisdiction_raw, offender, arrestee, forensic, labs, investigations = match.groups()\n            jurisdiction = standardize_jurisdiction_name(jurisdiction_raw)\n            \n            records.append({\n                'timestamp': timestamp,\n                'jurisdiction': jurisdiction,\n                'offender_profiles': offender.replace(',', ''),\n                'arrestee': arrestee.replace(',', ''),\n                'forensic_profiles': forensic.replace(',', ''),\n                'ndis_labs': labs,\n                'investigations_aided': investigations.replace(',', ''),\n                'data_as_of_date': data_date\n            })\n    \n    return records"
  },
  {
    "objectID": "ndis_analysis.html#process-all-snapshots",
    "href": "ndis_analysis.html#process-all-snapshots",
    "title": "NDIS Database Analysis",
    "section": "Process All Snapshots",
    "text": "Process All Snapshots\n\ndef process_all_snapshots():\n    \"\"\"Parse all downloaded snapshots and create datasets\"\"\"\n    print(\"Processing all snapshots...\")\n    \n    all_records = []\n    html_files = sorted(HTML_DIR.glob(\"*.html\"))\n    \n    for html_file in tqdm(html_files, desc=\"Parsing HTML files\"):\n        try:\n            records = parse_ndis_snapshot(html_file)\n            all_records.extend(records)\n        except Exception as e:\n            print(f\"Error parsing {html_file.name}: {e}\")\n    \n    # Convert to DataFrame\n    df = pd.DataFrame(all_records)\n    \n    # Convert numeric fields\n    numeric_fields = ['offender_profiles', 'arrestee', 'forensic_profiles', 'ndis_labs', 'investigations_aided']\n    for field in numeric_fields:\n        df[field] = pd.to_numeric(df[field], errors='coerce').fillna(0).astype(int)\n    \n    # Add datetime columns\n    df['capture_datetime'] = pd.to_datetime(df['timestamp'], format='%Y%m%d%H%M%S')\n    df['capture_date'] = df['capture_datetime'].dt.date\n    \n    # Sort by timestamp and jurisdiction\n    df = df.sort_values(['timestamp', 'jurisdiction'])\n    \n    return df\n\n# Process all files\ndf_raw = process_all_snapshots()\nprint(f\"\\nProcessed {len(df_raw)} total records\")\nprint(f\"Unique jurisdictions: {df_raw['jurisdiction'].nunique()}\")\nprint(f\"Date range: {df_raw['capture_datetime'].min()} to {df_raw['capture_datetime'].max()}\")\n\nProcessing all snapshots...\n\n\n\n\n\n\nProcessed 16118 total records\nUnique jurisdictions: 54\nDate range: 2010-10-14 04:38:19 to 2025-06-29 17:15:50"
  },
  {
    "objectID": "ndis_analysis.html#save-raw-data",
    "href": "ndis_analysis.html#save-raw-data",
    "title": "NDIS Database Analysis",
    "section": "Save Raw Data",
    "text": "Save Raw Data\n\n# Save the raw data (with typos)\ndf_raw.to_csv(OUTPUT_DIR / 'ndis_data_raw.csv', index=False)\nprint(f\"Saved raw data: {len(df_raw)} records\")\n\n# Show a sample\ndf_raw.head()\n\nSaved raw data: 16118 records\n\n\n\n\n\n\n\n\n\ntimestamp\njurisdiction\noffender_profiles\narrestee\nforensic_profiles\nndis_labs\ninvestigations_aided\ndata_as_of_date\ncapture_datetime\ncapture_date\n\n\n\n\n0\n20101014043819\nAlabama\n183916\n0\n6243\n4\n3338\nNaT\n2010-10-14 04:38:19\n2010-10-14\n\n\n1\n20101014043819\nAlaska\n20287\n0\n847\n1\n304\nNaT\n2010-10-14 04:38:19\n2010-10-14\n\n\n2\n20101014043819\nArizona\n178774\n0\n9424\n7\n3632\nNaT\n2010-10-14 04:38:19\n2010-10-14\n\n\n3\n20101014043819\nArkansas\n108969\n0\n3955\n1\n1233\nNaT\n2010-10-14 04:38:19\n2010-10-14\n\n\n4\n20101014043819\nCalifornia\n1295199\n0\n28456\n20\n12777\nNaT\n2010-10-14 04:38:19\n2010-10-14"
  },
  {
    "objectID": "ndis_analysis.html#fix-known-typos",
    "href": "ndis_analysis.html#fix-known-typos",
    "title": "NDIS Database Analysis",
    "section": "Fix Known Typos",
    "text": "Fix Known Typos\n\n# Create fixed version\ndf_fixed = df_raw.copy()\n\n# Apply typo fixes\nfor typo in KNOWN_TYPOS:\n    mask = (\n        (df_fixed['timestamp'] == typo['timestamp']) & \n        (df_fixed['jurisdiction'] == typo['jurisdiction'])\n    )\n    if mask.any():\n        df_fixed.loc[mask, typo['field']] = int(typo['correct_value'])\n        print(f\"Fixed typo: {typo['jurisdiction']} on {typo['timestamp'][:8]} - \"\n              f\"{typo['field']} from {typo['wrong_value']} to {typo['correct_value']}\")\n\n# Save fixed data\ndf_fixed.to_csv(OUTPUT_DIR / 'ndis_data_fixed.csv', index=False)\nprint(f\"\\nSaved fixed data: {len(df_fixed)} records\")\n\nFixed typo: California on 20250105 - investigations_aided from 1304657 to 130465\nFixed typo: California on 20250116 - investigations_aided from 1304657 to 130465\n\nSaved fixed data: 16118 records"
  },
  {
    "objectID": "ndis_analysis.html#verify-the-fix",
    "href": "ndis_analysis.html#verify-the-fix",
    "title": "NDIS Database Analysis",
    "section": "Verify the Fix",
    "text": "Verify the Fix\nLet’s check that we correctly fixed the California typo:\n\n# Compare California data before and after fix\ncal_timestamps = ['20250105164014', '20250116205311']\n\nprint(\"California investigations aided:\")\nprint(\"\\nRaw data:\")\nfor ts in cal_timestamps:\n    raw_val = df_raw[(df_raw['timestamp'] == ts) & (df_raw['jurisdiction'] == 'California')]['investigations_aided'].values\n    if len(raw_val) &gt; 0:\n        print(f\"  {ts}: {raw_val[0]:,}\")\n\nprint(\"\\nFixed data:\")\nfor ts in cal_timestamps:\n    fixed_val = df_fixed[(df_fixed['timestamp'] == ts) & (df_fixed['jurisdiction'] == 'California')]['investigations_aided'].values\n    if len(fixed_val) &gt; 0:\n        print(f\"  {ts}: {fixed_val[0]:,}\")\n\nCalifornia investigations aided:\n\nRaw data:\n  20250105164014: 1,304,657\n  20250116205311: 1,304,657\n\nFixed data:\n  20250105164014: 130,465\n  20250116205311: 130,465"
  },
  {
    "objectID": "ndis_analysis.html#summary-statistics",
    "href": "ndis_analysis.html#summary-statistics",
    "title": "NDIS Database Analysis",
    "section": "Summary Statistics",
    "text": "Summary Statistics\n\n# Calculate summary statistics\nlatest_data = df_fixed[df_fixed['capture_datetime'] == df_fixed['capture_datetime'].max()]\nlatest_data = latest_data[latest_data['jurisdiction'] != 'D.C./Metro PD']\n\nprint(\"\\nLatest Statistics Summary:\")\nprint(f\"  As of: {latest_data['capture_datetime'].iloc[0]}\")\nprint(f\"  Data from: {latest_data['data_as_of_date'].iloc[0] if latest_data['data_as_of_date'].iloc[0] else 'Unknown'}\")\nprint(f\"  Jurisdictions reporting: {len(latest_data)}\")\nprint(f\"  Total offender profiles: {latest_data['offender_profiles'].sum():,}\")\nprint(f\"  Total arrestee profiles: {latest_data['arrestee'].sum():,}\")\nprint(f\"  Total forensic profiles: {latest_data['forensic_profiles'].sum():,}\")\nprint(f\"  Total investigations aided: {latest_data['investigations_aided'].sum():,}\")\n\n\nLatest Statistics Summary:\n  As of: 2025-06-29 17:15:50\n  Data from: 2025-04-01 00:00:00\n  Jurisdictions reporting: 53\n  Total offender profiles: 18,431,162\n  Total arrestee profiles: 5,879,537\n  Total forensic profiles: 1,405,917\n  Total investigations aided: 730,426"
  },
  {
    "objectID": "ndis_analysis.html#quick-visualization",
    "href": "ndis_analysis.html#quick-visualization",
    "title": "NDIS Database Analysis",
    "section": "Quick Visualization",
    "text": "Quick Visualization\n\n# Plot total offender profiles over time\ntotals_by_date = df_fixed.groupby('capture_datetime')['offender_profiles'].sum()\n\nplt.figure(figsize=(10, 6))\nplt.plot(totals_by_date.index, totals_by_date.values / 1e6)\nplt.title('Growth of NDIS Offender Profiles Over Time')\nplt.xlabel('Year')\nplt.ylabel('Millions of Profiles')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "ndis_analysis.html#data-files-created",
    "href": "ndis_analysis.html#data-files-created",
    "title": "NDIS Database Analysis",
    "section": "Data Files Created",
    "text": "Data Files Created\nThis analysis creates two CSV files in the output/ directory:\n\nndis_data_raw.csv - Original parsed data including typos\nndis_data_fixed.csv - Corrected data with typos fixed\n\nBoth files contain the following columns: - timestamp: Wayback Machine capture timestamp - jurisdiction: State/territory name (standardized) - offender_profiles: Number of offender DNA profiles - arrestee: Number of arrestee DNA profiles - forensic_profiles: Number of forensic DNA profiles - ndis_labs: Number of participating labs - investigations_aided: Number of investigations aided - data_as_of_date: The date the FBI data is from - capture_datetime: Parsed capture date - capture_date: Date only version ```"
  },
  {
    "objectID": "podfridge-db-env/lib/python3.13/site-packages/numpy/random/LICENSE.html",
    "href": "podfridge-db-env/lib/python3.13/site-packages/numpy/random/LICENSE.html",
    "title": "NCSA Open Source License",
    "section": "",
    "text": "This software is dual-licensed under the The University of Illinois/NCSA Open Source License (NCSA) and The 3-Clause BSD License\n\nNCSA Open Source License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nDeveloped by: Kevin Sheppard (kevin.sheppard@economics.ox.ac.uk, kevin.k.sheppard@gmail.com) http://www.kevinsheppard.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimers.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimers in the documentation and/or other materials provided with the distribution.\nNeither the names of Kevin Sheppard, nor the names of any contributors may be used to endorse or promote products derived from this Software without specific prior written permission.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE.\n\n\n3-Clause BSD License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nComponents\nMany parts of this module have been derived from original sources, often the algorithm’s designer. Component licenses are located with the component code."
  },
  {
    "objectID": "podfridge-db-env/lib/python3.13/site-packages/seaborn-0.13.2.dist-info/LICENSE.html",
    "href": "podfridge-db-env/lib/python3.13/site-packages/seaborn-0.13.2.dist-info/LICENSE.html",
    "title": "PODFRIDGE-Databases",
    "section": "",
    "text": "Copyright (c) 2012-2023, Michael L. Waskom All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the project nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "podfridge-db-env/lib/python3.13/site-packages/soupsieve-2.7.dist-info/licenses/LICENSE.html",
    "href": "podfridge-db-env/lib/python3.13/site-packages/soupsieve-2.7.dist-info/licenses/LICENSE.html",
    "title": "PODFRIDGE-Databases",
    "section": "",
    "text": "MIT License\nCopyright (c) 2018 - 2025 Isaac Muse isaacmuse@gmail.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "podfridge-db-env/lib/python3.13/site-packages/idna-3.10.dist-info/LICENSE.html",
    "href": "podfridge-db-env/lib/python3.13/site-packages/idna-3.10.dist-info/LICENSE.html",
    "title": "PODFRIDGE-Databases",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2013-2024, Kim Davies and contributors. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "podfridge-db-env/lib/python3.13/site-packages/pyzmq-27.0.0.dist-info/licenses/LICENSE.html",
    "href": "podfridge-db-env/lib/python3.13/site-packages/pyzmq-27.0.0.dist-info/licenses/LICENSE.html",
    "title": "PODFRIDGE-Databases",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2009-2012, Brian Granger, Min Ragan-Kelley\nAll rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "podfridge-db-env/lib/python3.13/site-packages/httpcore-1.0.9.dist-info/licenses/LICENSE.html",
    "href": "podfridge-db-env/lib/python3.13/site-packages/httpcore-1.0.9.dist-info/licenses/LICENSE.html",
    "title": "PODFRIDGE-Databases",
    "section": "",
    "text": "Copyright © 2020, Encode OSS Ltd. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "podfridge-db-env/lib/python3.13/site-packages/httpx-0.28.1.dist-info/licenses/LICENSE.html",
    "href": "podfridge-db-env/lib/python3.13/site-packages/httpx-0.28.1.dist-info/licenses/LICENSE.html",
    "title": "PODFRIDGE-Databases",
    "section": "",
    "text": "Copyright © 2019, Encode OSS Ltd. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "ndis_analysis.html#setup-and-configuration",
    "href": "ndis_analysis.html#setup-and-configuration",
    "title": "NDIS Database Analysis",
    "section": "Setup and Configuration",
    "text": "Setup and Configuration\n\n\nInstalling beautifulsoup4...\nRequirement already satisfied: beautifulsoup4 in ./podfridge-db-env/lib/python3.13/site-packages (4.13.4)\nRequirement already satisfied: soupsieve&gt;1.2 in ./podfridge-db-env/lib/python3.13/site-packages (from beautifulsoup4) (2.7)\nRequirement already satisfied: typing-extensions&gt;=4.0.0 in ./podfridge-db-env/lib/python3.13/site-packages (from beautifulsoup4) (4.14.1)\n\n\n\nfrom pathlib import Path\nimport re, json, requests, time\nfrom datetime import datetime\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Configuration\nBASE_DIR = Path(\".\")  # Current directory\nROOT = BASE_DIR\nHTML_DIR = ROOT / \"output\" / \"wayback_html\"\nHTML_DIR.mkdir(parents=True, exist_ok=True)\nCSV_PATH = ROOT / \"output\" / \"ndis_state_metrics.csv\"\n\n# Jurisdiction name standardization mapping\nJURISDICTION_NAME_MAP = {\n    'D.C./FBI Lab': 'DC/FBI Lab',\n    'US Army': 'U.S. Army'\n}\n\n# Known data typos to fix\nKNOWN_TYPOS = [\n    {\n        'timestamp': '20250105164014',\n        'jurisdiction': 'California', \n        'field': 'investigations_aided',\n        'wrong_value': '1304657',  # How it parses\n        'correct_value': '130465'   # What it should be\n    },\n    {\n        'timestamp': '20250116205311',\n        'jurisdiction': 'California',\n        'field': 'investigations_aided', \n        'wrong_value': '1304657',\n        'correct_value': '130465'\n    }\n]\n\nprint(f\"HTML directory: {HTML_DIR}\")\n\nHTML directory: output/wayback_html"
  },
  {
    "objectID": "ndis_analysis.html#wayback-machine-functions",
    "href": "ndis_analysis.html#wayback-machine-functions",
    "title": "NDIS Database Analysis",
    "section": "Wayback Machine Functions",
    "text": "Wayback Machine Functions\n\ndef make_request_with_retry(params, max_retries=3, initial_delay=5):\n    \"\"\"Make a request with exponential backoff retry logic\"\"\"\n    base = \"https://web.archive.org/cdx/search/cdx\"\n    \n    for attempt in range(max_retries):\n        try:\n            r = requests.get(base, params=params, timeout=30)\n            if r.status_code == 200:\n                return r\n            elif r.status_code == 429:  # Rate limited\n                wait_time = initial_delay * (2 ** attempt)\n                print(f\"    Rate limited. Waiting {wait_time} seconds...\")\n                time.sleep(wait_time)\n            else:\n                return r\n        except requests.exceptions.ConnectionError as e:\n            wait_time = initial_delay * (2 ** attempt)\n            print(f\"    Connection error. Waiting {wait_time} seconds before retry {attempt + 1}/{max_retries}\")\n            time.sleep(wait_time)\n        except Exception as e:\n            print(f\"    Unexpected error: {e}\")\n            return None\n    return None"
  },
  {
    "objectID": "ndis_analysis.html#search-for-all-ndis-snapshots",
    "href": "ndis_analysis.html#search-for-all-ndis-snapshots",
    "title": "NDIS Database Analysis",
    "section": "Search for All NDIS Snapshots",
    "text": "Search for All NDIS Snapshots\n\n\nShow search function code\ndef search_all_ndis_snapshots():\n    \"\"\"Search for NDIS snapshots across all known URL variations\"\"\"\n    \n    # Search for both http and https variants\n    protocols = [\"http://\", \"https://\"]\n    subdomains = [\"www\", \"le\", \"*\"]  # Known subdomains plus wildcard\n    \n    all_rows = []\n    seen_timestamps = set()\n    \n    # First, try broad searches with protocol wildcards\n    print(\"Starting wildcard searches...\")\n    for protocol in protocols:\n        for subdomain in subdomains:\n            pattern = f\"{protocol}{subdomain}.fbi.gov/*ndis-statistics*\"\n            print(f\"\\nSearching: {pattern}\")\n            \n            params = {\n                \"url\":         pattern,\n                \"matchType\":   \"wildcard\",\n                \"output\":      \"json\",\n                \"fl\":          \"timestamp,original,mimetype,statuscode\",\n                \"filter\":      [\"statuscode:200\", \"mimetype:text/html\"],\n                \"limit\":       \"10000\",\n            }\n            \n            r = make_request_with_retry(params)\n            if r and r.status_code == 200:\n                data = json.loads(r.text)\n                if len(data) &gt; 1:\n                    new_rows = 0\n                    for row in data[1:]:\n                        if row[0] not in seen_timestamps:\n                            all_rows.append(row)\n                            seen_timestamps.add(row[0])\n                            new_rows += 1\n                    print(f\"  → Found {new_rows} new snapshots\")\n                else:\n                    print(f\"  → No results\")\n            else:\n                print(f\"  → Failed after retries\")\n            \n            # Always wait between requests to avoid rate limiting\n            time.sleep(2)\n    \n    # Also search your specific known URLs with both protocols\n    known_paths = [\n        \"www.fbi.gov/about-us/lab/codis/ndis-statistics\",\n        \"www.fbi.gov/about-us/laboratory/biometric-analysis/codis/ndis-statistics\", \n        \"www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics\",\n        \"le.fbi.gov/science-and-lab/biometrics-and-fingerprints/codis/codis-ndis-statistics\",\n    ]\n    \n    print(\"\\n\\nStarting exact URL searches...\")\n    for path in known_paths:\n        for protocol in protocols:\n            url = f\"{protocol}{path}\"\n            print(f\"\\nSearching: {url}\")\n            \n            params = {\n                \"url\":         url,\n                \"matchType\":   \"exact\",\n                \"output\":      \"json\",\n                \"fl\":          \"timestamp,original,mimetype,statuscode\",\n                \"filter\":      [\"statuscode:200\", \"mimetype:text/html\"],\n                \"limit\":       \"10000\",\n            }\n            \n            r = make_request_with_retry(params)\n            if r and r.status_code == 200:\n                data = json.loads(r.text)\n                if len(data) &gt; 1:\n                    new_rows = 0\n                    for row in data[1:]:\n                        if row[0] not in seen_timestamps:\n                            all_rows.append(row)\n                            seen_timestamps.add(row[0])\n                            new_rows += 1\n                    print(f\"  → Found {new_rows} new snapshots\")\n                else:\n                    print(f\"  → No results\")\n            else:\n                print(f\"  → Failed after retries\")\n            \n            # Always wait between requests\n            time.sleep(2)\n    \n    # Create DataFrame\n    snap_df = (pd.DataFrame(\n                    all_rows,\n                    columns=[\"timestamp\", \"original\", \"mimetype\", \"status\"])\n               .sort_values(\"timestamp\")\n               .reset_index(drop=True))\n    \n    return snap_df\n\n# Check if we already have snapshot data or need to search\nsnapshot_csv = HTML_DIR.parent / 'snapshots_found.csv'\nif snapshot_csv.exists():\n    print(\"Loading existing snapshot list...\")\n    snap_df = pd.read_csv(snapshot_csv)\n    print(f\"Loaded {len(snap_df)} snapshots\")\nelse:\n    print(\"Searching for all NDIS snapshots...\")\n    snap_df = search_all_ndis_snapshots()\n    if len(snap_df) &gt; 0:\n        snap_df.to_csv(snapshot_csv, index=False)\n        print(f\"\\nSaved {len(snap_df)} snapshots to {snapshot_csv}\")\n\nif len(snap_df) &gt; 0:\n    print(f\"\\nTotal unique snapshots found: {len(snap_df):,}\")\n    print(f\"Unique URLs found: {snap_df['original'].nunique()}\")\n    print(\"\\nUnique URL patterns found:\")\n    for url in sorted(snap_df['original'].unique()):\n        print(f\"  {url}\")\n\n\nLoading existing snapshot list...\nLoaded 317 snapshots\n\nTotal unique snapshots found: 317\nUnique URLs found: 8\n\nUnique URL patterns found:\n  http://www.fbi.gov/about-us/lab/codis/ndis-statistics\n  http://www.fbi.gov/about-us/lab/codis/ndis-statistics/\n  http://www.fbi.gov:80/about-us/lab/codis/ndis-statistics\n  http://www.fbi.gov:80/about-us/lab/codis/ndis-statistics/\n  https://le.fbi.gov/science-and-lab/biometrics-and-fingerprints/codis/codis-ndis-statistics\n  https://www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics\n  https://www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics/\n  https://www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics//"
  },
  {
    "objectID": "ndis_analysis.html#download-functions",
    "href": "ndis_analysis.html#download-functions",
    "title": "NDIS Database Analysis",
    "section": "Download Functions",
    "text": "Download Functions\n\ndef download_with_retry(url, max_retries=3, initial_delay=5, consecutive_failures=0):\n    \"\"\"Download with adaptive retry logic based on consecutive failures\"\"\"\n    if consecutive_failures &gt; 0:\n        extra_wait = consecutive_failures * 10\n        print(f\"\\n    Adding {extra_wait}s cooldown due to {consecutive_failures} consecutive failures...\")\n        time.sleep(extra_wait)\n    \n    for attempt in range(max_retries):\n        try:\n            response = requests.get(url, timeout=30)\n            response.raise_for_status()\n            return response, True\n        except requests.exceptions.ConnectionError as e:\n            wait_time = initial_delay * (2 ** attempt)\n            print(f\"\\n    Connection error. Waiting {wait_time} seconds before retry {attempt + 1}/{max_retries}\")\n            time.sleep(wait_time)\n        except requests.exceptions.HTTPError as e:\n            if e.response.status_code == 429:\n                wait_time = initial_delay * (2 ** attempt) * 2\n                print(f\"\\n    Rate limited (429). Waiting {wait_time} seconds...\")\n                time.sleep(wait_time)\n            else:\n                print(f\"\\n    HTTP Error: {e}\")\n                return None, False\n        except Exception as e:\n            print(f\"\\n    Unexpected error: {e}\")\n            return None, False\n    return None, False\n\ndef download_missing_snapshots(snap_df, output_folder):\n    \"\"\"Download HTML snapshots with resume capability\"\"\"\n    \n    # Check what we already have\n    existing_files = list(output_folder.glob(\"*.html\"))\n    existing_timestamps = {f.stem for f in existing_files}\n    print(f\"\\nFiles already downloaded: {len(existing_files)}\")\n    \n    # Check what needs to be downloaded\n    to_download = []\n    for _, row in snap_df.iterrows():\n        timestamp = row['timestamp']\n        url = row['original']\n        filename = output_folder / f\"{timestamp}.html\"\n        \n        if timestamp not in existing_timestamps and not filename.exists():\n            to_download.append((timestamp, url, filename))\n    \n    print(f\"Files to download: {len(to_download)}\")\n    \n    if len(to_download) == 0:\n        print(\"\\n✓ All files already downloaded! Nothing to do.\")\n        return\n    \n    # Download configuration\n    BATCH_SIZE = 15\n    PAUSE_BETWEEN_DOWNLOADS = 3\n    PAUSE_BETWEEN_BATCHES = 45\n    PAUSE_AFTER_FAILURE = 60\n    \n    # Track statistics\n    successful_downloads = 0\n    failed_downloads = []\n    consecutive_failures = 0\n    \n    # Download in batches\n    for i in range(0, len(to_download), BATCH_SIZE):\n        batch = to_download[i:i + BATCH_SIZE]\n        batch_num = (i // BATCH_SIZE) + 1\n        total_batches = (len(to_download) + BATCH_SIZE - 1) // BATCH_SIZE\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"Batch {batch_num}/{total_batches} ({len(batch)} files)\")\n        print(f\"Overall progress: {len(existing_timestamps) + successful_downloads}/{len(snap_df)} total files\")\n        print(f\"{'='*60}\")\n        \n        for j, (timestamp, url, filename) in enumerate(batch, 1):\n            # Double-check file doesn't exist\n            if filename.exists():\n                print(f\"\\n[{j}/{len(batch)}] {timestamp} - Already exists, skipping...\")\n                continue\n                \n            wayback_url = f\"https://web.archive.org/web/{timestamp}/{url}\"\n            \n            print(f\"\\n[{j}/{len(batch)}] Downloading {timestamp}...\", end=\"\")\n            \n            response, success = download_with_retry(wayback_url, consecutive_failures=consecutive_failures)\n            \n            if response and response.status_code == 200:\n                try:\n                    with open(filename, 'w', encoding='utf-8') as f:\n                        f.write(response.text)\n                    \n                    print(\" ✓ Success\")\n                    successful_downloads += 1\n                    consecutive_failures = 0\n                    \n                except Exception as e:\n                    print(f\" ✗ Error saving file: {e}\")\n                    failed_downloads.append((timestamp, url, str(e)))\n                    consecutive_failures += 1\n            else:\n                print(\" ✗ Failed after retries\")\n                failed_downloads.append((timestamp, url, \"Download failed\"))\n                consecutive_failures += 1\n                \n                if j &lt; len(batch):\n                    print(f\"    Taking {PAUSE_AFTER_FAILURE}s break after failure...\")\n                    time.sleep(PAUSE_AFTER_FAILURE)\n                    continue\n            \n            if j &lt; len(batch) and consecutive_failures == 0:\n                print(f\"    Waiting {PAUSE_BETWEEN_DOWNLOADS} seconds...\")\n                time.sleep(PAUSE_BETWEEN_DOWNLOADS)\n        \n        if i + BATCH_SIZE &lt; len(to_download):\n            print(f\"\\nBatch complete. Pausing {PAUSE_BETWEEN_BATCHES} seconds...\")\n            print(f\"This session: {successful_downloads} downloaded, {len(failed_downloads)} failed\")\n            time.sleep(PAUSE_BETWEEN_BATCHES)\n    \n    # Final summary\n    print(f\"\\n{'='*60}\")\n    print(f\"Download session complete!\")\n    print(f\"  Successfully downloaded: {successful_downloads}\")\n    print(f\"  Failed downloads: {len(failed_downloads)}\")\n    \n    if failed_downloads:\n        print(f\"\\nFailed downloads:\")\n        for timestamp, url, error in failed_downloads[:10]:\n            print(f\"  {timestamp}: {error}\")\n        if len(failed_downloads) &gt; 10:\n            print(f\"  ... and {len(failed_downloads) - 10} more\")\n\n# Download missing files\nif len(snap_df) &gt; 0:\n    download_missing_snapshots(snap_df, HTML_DIR)\n\n\nFiles already downloaded: 317\nFiles to download: 0\n\n✓ All files already downloaded! Nothing to do."
  },
  {
    "objectID": "ndis_analysis.html#apply-typo-fixes",
    "href": "ndis_analysis.html#apply-typo-fixes",
    "title": "NDIS Database Analysis",
    "section": "Apply Typo Fixes",
    "text": "Apply Typo Fixes\n\ndef apply_typo_fixes(df):\n    \"\"\"Apply known typo corrections\"\"\"\n    df_fixed = df.copy()\n    \n    for typo in KNOWN_TYPOS:\n        mask = (\n            (df_fixed['timestamp'] == typo['timestamp']) & \n            (df_fixed['jurisdiction'] == typo['jurisdiction'])\n        )\n        if mask.any():\n            df_fixed.loc[mask, typo['field']] = int(typo['correct_value'])\n            print(f\"Fixed typo: {typo['jurisdiction']} on {typo['timestamp'][:8]} - \"\n                  f\"{typo['field']} from {typo['wrong_value']} to {typo['correct_value']}\")\n    \n    return df_fixed\n\n# Apply fixes\ndf_fixed = apply_typo_fixes(df_raw)\n\nFixed typo: California on 20250105 - investigations_aided from 1304657 to 130465\nFixed typo: California on 20250116 - investigations_aided from 1304657 to 130465"
  },
  {
    "objectID": "ndis_analysis.html#save-datasets",
    "href": "ndis_analysis.html#save-datasets",
    "title": "NDIS Database Analysis",
    "section": "Save Datasets",
    "text": "Save Datasets\n\n# Save datasets\nOUTPUT_DIR = HTML_DIR.parent\ndf_raw.to_csv(OUTPUT_DIR / 'ndis_data_raw.csv', index=False)\ndf_fixed.to_csv(OUTPUT_DIR / 'ndis_data_fixed.csv', index=False)\nprint(f\"\\nSaved raw data to: {OUTPUT_DIR / 'ndis_data_raw.csv'}\")\nprint(f\"Saved fixed data to: {OUTPUT_DIR / 'ndis_data_fixed.csv'}\")\n\n\nSaved raw data to: output/ndis_data_raw.csv\nSaved fixed data to: output/ndis_data_fixed.csv"
  },
  {
    "objectID": "ndis_analysis.html#visualizations",
    "href": "ndis_analysis.html#visualizations",
    "title": "NDIS Database Analysis",
    "section": "Visualizations",
    "text": "Visualizations\n\ndef create_visualizations(df_raw, df_fixed):\n    \"\"\"Create comprehensive visualizations\"\"\"\n    # Exclude D.C./Metro PD from visualizations\n    df_raw_viz = df_raw[df_raw['jurisdiction'] != 'D.C./Metro PD']\n    df_fixed_viz = df_fixed[df_fixed['jurisdiction'] != 'D.C./Metro PD']\n    \n    # Create figure with subplots\n    fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n    \n    # 1. Jurisdictions reporting over time\n    for ax, (df, title_suffix) in zip([axes[0,0], axes[0,1]], \n                                      [(df_raw_viz, 'Raw'), (df_fixed_viz, 'Fixed')]):\n        jurisdictions_per_date = df.groupby('capture_datetime')['jurisdiction'].nunique()\n        ax.plot(jurisdictions_per_date.index, jurisdictions_per_date.values, 'b-', linewidth=2)\n        ax.set_title(f'Jurisdictions Reporting ({title_suffix})')\n        ax.set_ylabel('Number of Jurisdictions')\n        ax.grid(True, alpha=0.3)\n        ax.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n    \n    # 2. Total investigations aided\n    for ax, (df, title_suffix) in zip([axes[1,0], axes[1,1]], \n                                      [(df_raw_viz, 'Raw with Typo'), (df_fixed_viz, 'Fixed')]):\n        total_inv = df.groupby('capture_datetime')['investigations_aided'].sum()\n        ax.plot(total_inv.index, total_inv.values / 1e3, 'purple', linewidth=2)\n        ax.set_title(f'Total Investigations Aided ({title_suffix})')\n        ax.set_ylabel('Thousands of Investigations')\n        ax.grid(True, alpha=0.3)\n        \n        # Highlight the typo in raw data\n        if 'Raw' in title_suffix:\n            typo_dates = df[(df['jurisdiction'] == 'California') & \n                          (df['investigations_aided'] &gt; 1000000)]['capture_datetime']\n            for date in typo_dates:\n                ax.axvline(x=date, color='red', linestyle='--', alpha=0.5)\n                ax.text(date, ax.get_ylim()[1]*0.9, 'Typo', rotation=90, \n                       verticalalignment='bottom', color='red')\n    \n    # 3. Data lag analysis\n    ax = axes[2, 0]\n    df_with_lag = df_fixed_viz[df_fixed_viz['data_as_of_date'].notna()].copy()\n    df_with_lag['data_lag_days'] = (df_with_lag['capture_datetime'] - df_with_lag['data_as_of_date']).dt.days\n    \n    avg_lag = df_with_lag.groupby('capture_datetime')['data_lag_days'].mean()\n    ax.plot(avg_lag.index, avg_lag.values, 'orange', linewidth=2)\n    ax.set_title('Average Data Lag (Capture Date vs \"As Of\" Date)')\n    ax.set_ylabel('Days')\n    ax.grid(True, alpha=0.3)\n    \n    # 4. California investigations over time (showing typo fix)\n    ax = axes[2, 1]\n    cal_raw = df_raw_viz[df_raw_viz['jurisdiction'] == 'California']\n    cal_fixed = df_fixed_viz[df_fixed_viz['jurisdiction'] == 'California']\n    \n    ax.plot(cal_raw['capture_datetime'], cal_raw['investigations_aided'], \n            'r-', label='Raw (with typo)', linewidth=2, alpha=0.7)\n    ax.plot(cal_fixed['capture_datetime'], cal_fixed['investigations_aided'], \n            'g-', label='Fixed', linewidth=2)\n    ax.set_title('California Investigations Aided: Raw vs Fixed')\n    ax.set_ylabel('Investigations Aided')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(OUTPUT_DIR / 'ndis_analysis_complete.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n# Create visualizations\nprint(\"\\nCreating visualizations...\")\ncreate_visualizations(df_raw, df_fixed)\nprint(\"\\nProcessing complete!\")\n\n\nCreating visualizations...\n\n\n\n\n\n\n\n\n\n\nProcessing complete!"
  },
  {
    "objectID": "analysis/foia_processing.html",
    "href": "analysis/foia_processing.html",
    "title": "FOIA Document OCR Processing",
    "section": "",
    "text": "This document details the processing of Freedom of Information Act (FOIA) responses from seven U.S. states regarding the demographic composition of their State DNA Index System (SDIS) databases. These responses were obtained by Professor Erin Murphy (NYU Law) in 2018 as part of research on racial disparities in DNA databases."
  },
  {
    "objectID": "analysis/foia_processing.html#overview",
    "href": "analysis/foia_processing.html#overview",
    "title": "FOIA Document OCR Processing",
    "section": "",
    "text": "This document details the processing of Freedom of Information Act (FOIA) responses from seven U.S. states regarding the demographic composition of their State DNA Index System (SDIS) databases. These responses were obtained by Professor Erin Murphy (NYU Law) in 2018 as part of research on racial disparities in DNA databases."
  },
  {
    "objectID": "analysis/foia_processing.html#data-sources",
    "href": "analysis/foia_processing.html#data-sources",
    "title": "FOIA Document OCR Processing",
    "section": "2.1 Data Sources",
    "text": "2.1 Data Sources\n\n2.1.1 Raw FOIA Responses\nThe original FOIA responses are stored in two formats:\n\nPDFs: raw/foia_pdfs/ - Original scanned documents\nHTML: raw/foia_html/ - OCR’d versions for easier extraction\n\nStates included:\n\nCalifornia\nFlorida\n\nIndiana\nMaine\nNevada\nSouth Dakota\nTexas"
  },
  {
    "objectID": "analysis/foia_processing.html#file-structure-and-contents",
    "href": "analysis/foia_processing.html#file-structure-and-contents",
    "title": "FOIA Document OCR Processing",
    "section": "2.2 File Structure and Contents",
    "text": "2.2 File Structure and Contents\n\n2.2.1 State-Specific Files: per_state/[state]_foia_data.csv\nPurpose: Individual files for each state containing only their reported data.\nStructure: Long format with columns:\n\nstate: State name\noffender_type: Category of individuals (Convicted Offender, Arrestee, Combined, etc.)\nvariable_category: Type of data (total, gender, race, gender_race)\nvariable_detailed: Specific value (e.g., Male, Female, African American)\nvalue: The reported number or percentage\nvalue_type: Whether value is a “count” or “percentage”\ndate: Date of data snapshot, if reported\n\n\n\n2.2.2 State Processing\n\nimport pandas as pd\nfrom pathlib import Path\nfrom IPython.display import display, Markdown\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Display options\npd.set_option(\"display.max_columns\", None)\npd.set_option(\"display.width\", None)\n\n# Path to per‑state files (run notebook from analysis/)\nbase_dir   = Path(\"..\")\nper_state  = base_dir / \"output\" / \"foia\" / \"per_state\"\n\n# ------------------------------------------------------------------\n# 1. Discover available per‑state CSV files\n# ------------------------------------------------------------------\nstate_files = sorted(per_state.glob(\"*_foia_data.csv\"))\n\nif not state_files:\n    raise FileNotFoundError(\n        f\"No per‑state FOIA files found in {per_state}. \"\n        \"Check the folder path.\"\n    )\n\ndef stem_to_state(stem: str) -&gt; str:\n    toks = stem.split(\"_\")\n    if \"foia\" in toks:\n        toks = toks[:toks.index(\"foia\")]\n    return \" \".join(t.title() for t in toks)\n\nstates_available = [stem_to_state(f.stem) for f in state_files]\n\n\nprint(f\"✓ Found {len(state_files)} per‑state files:\")\nfor s in states_available:\n    print(\"  •\", s)\n\ndisplay(Markdown(\n    f\"**States with data loaded:** {', '.join(states_available)}\"\n))\n\n# ------------------------------------------------------------------\n# 2. Initialise empty containers for the loop that follows\n# ------------------------------------------------------------------\nfoia_combined       = pd.DataFrame()   # merged tidy data\nfoia_state_metadata = []               # list of dicts, one per state\n\n✓ Found 7 per‑state files:\n  • California\n  • Florida\n  • Indiana\n  • Maine\n  • Nevada\n  • South Dakota\n  • Texas\n\n\nStates with data loaded: California, Florida, Indiana, Maine, Nevada, South Dakota, Texas"
  },
  {
    "objectID": "analysis/foia_processing.html#data-processing-steps",
    "href": "analysis/foia_processing.html#data-processing-steps",
    "title": "FOIA Document OCR Processing",
    "section": "Data Processing Steps",
    "text": "Data Processing Steps\n\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom IPython.display import display, Markdown, HTML\n\n# Set display options for better formatting\npd.options.display.max_columns = None\npd.options.display.width = None\npd.options.display.max_colwidth = None\n\n# Set up paths - assuming we're running from analysis/ directory\nbase_dir = Path(\"..\") \noutput_dir = base_dir / \"output\" / \"foia\"\n\n# Load the raw FOIA data\nfoia_raw = pd.read_csv(output_dir / \"foia_raw.csv\")\nmetadata = pd.read_csv(output_dir / \"state_reporting_metadata.csv\")\n\nprint(f\"Loaded {len(foia_raw)} rows of FOIA data from {foia_raw['state'].nunique()} states\")\nprint(\"\\nStates included:\", ', '.join(foia_raw['state'].unique()))\n\n# Check for any non-numeric values in 'value' column\nnon_numeric = foia_raw[pd.to_numeric(foia_raw['value'], errors='coerce').isna()]['value'].unique()\nif len(non_numeric) &gt; 0:\n    print(f\"\\nNote: Found non-numeric value(s): {non_numeric}\")\n\n# Convert value column, handling special cases\nfoia_raw['value_numeric'] = foia_raw['value'].apply(\n    lambda x: 0.5 if str(x) == '&lt;1' else pd.to_numeric(x, errors='coerce')\n)\n\n# Show metadata summary\ndisplay(Markdown(\"### State Reporting Summary\"))\ndisplay(HTML(metadata[['state', 'separates_offender_types', 'reports_counts', 'reports_percentages']].to_html(index=False)))\n\nLoaded 143 rows of FOIA data from 7 states\n\nStates included: California, Florida, Indiana, Maine, Nevada, South Dakota, Texas\n\nNote: Found non-numeric value(s): ['&lt;1']\n\n\n\nState Reporting Summary\n\n\n\n\n\n\nstate\nseparates_offender_types\nreports_counts\nreports_percentages\n\n\n\n\nCalifornia\nYes\nYes\nNo\n\n\nFlorida\nNo\nYes\nYes\n\n\nIndiana\nTotals only\nTotals only\nDemographics only\n\n\nMaine\nNo\nYes\nYes\n\n\nNevada\nYes\nYes\nSome percentages\n\n\nSouth Dakota\nNo\nYes\nYes\n\n\nTexas\nYes\nYes (partial)\nNo\n\n\n\n\n\n\nStep 1: States with Both Counts AND Percentages\nThese states provided both counts and percentages, allowing us to verify their calculations.\n\nFlorida: Complete Count and Percentage Data\n\n# Florida provides both counts and percentages\nflorida_data = foia_raw[foia_raw['state'] == 'Florida'].copy()\n\n# Pivot to get counts and percentages side by side\nflorida_wide = florida_data.pivot_table(\n    index=['offender_type', 'variable_category', 'variable_detailed'],\n    columns='value_type',\n    values='value_numeric',\n    aggfunc='first'\n).reset_index()\n\n# Calculate percentages from counts and compare\nif 'count' in florida_wide.columns and 'percentage' in florida_wide.columns:\n    florida_check = florida_wide[florida_wide['count'].notna()].copy()\n    total_profiles = 1175391\n\n    florida_check['calc_percentage'] = np.where(\n        florida_check['variable_detailed'] == 'total_profiles',\n        100.0,\n        round(florida_check['count'] / total_profiles * 100, 2)\n    )\n    florida_check['diff'] = florida_check['calc_percentage'] - florida_check['percentage']\n\n    display(Markdown(\"**Florida: Comparing reported vs calculated percentages**\"))\n    \n    # Create summary table\n    summary_df = florida_check[['variable_category', 'variable_detailed', 'count', \n                               'percentage', 'calc_percentage', 'diff']]\n    summary_df.columns = ['Category', 'Detail', 'Count', 'Reported %', 'Calculated %', 'Difference']\n    \n    # Format numbers nicely\n    summary_df['Count'] = summary_df['Count'].apply(lambda x: f\"{x:,.0f}\")\n    summary_df['Reported %'] = summary_df['Reported %'].apply(lambda x: f\"{x:.2f}%\")\n    summary_df['Calculated %'] = summary_df['Calculated %'].apply(lambda x: f\"{x:.2f}%\")\n    summary_df['Difference'] = summary_df['Difference'].apply(lambda x: f\"{x:+.2f}\" if pd.notna(x) else \"\")\n    \n    display(HTML(summary_df.to_html(index=False, classes='table table-striped table-hover')))\n\n    # Check if percentages sum to 100\n    florida_pct_sum = florida_check.groupby('variable_category')['percentage'].sum()\n    display(Markdown(\"\\n**Florida percentage totals by category:**\"))\n    totals_df = pd.DataFrame({\n        'Category': florida_pct_sum[florida_pct_sum.index != 'total'].index,\n        'Total %': [f\"{x:.2f}%\" for x in florida_pct_sum[florida_pct_sum.index != 'total'].values]\n    })\n    display(HTML(totals_df.to_html(index=False, classes='table table-striped')))\n\nFlorida: Comparing reported vs calculated percentages\n\n\n\n\n\nCategory\nDetail\nCount\nReported %\nCalculated %\nDifference\n\n\n\n\ngender\nFemale\n260,885\n22.20%\n22.20%\n+0.00\n\n\ngender\nMale\n901,126\n76.67%\n76.67%\n+0.00\n\n\ngender\nUnknown\n13,380\n1.14%\n1.14%\n+0.00\n\n\nrace\nAfrican American\n413,733\n35.20%\n35.20%\n+0.00\n\n\nrace\nAsian\n2,659\n0.23%\n0.23%\n+0.00\n\n\nrace\nCaucasian\n721,485\n61.38%\n61.38%\n+0.00\n\n\nrace\nHispanic\n28,452\n2.42%\n2.42%\n+0.00\n\n\nrace\nNative American\n667\n0.06%\n0.06%\n+0.00\n\n\nrace\nOther\n1,176\n0.10%\n0.10%\n+0.00\n\n\nrace\nUnknown\n7,219\n0.61%\n0.61%\n+0.00\n\n\ntotal\ntotal_profiles\n1,175,391\n100.00%\n100.00%\n+0.00\n\n\n\n\n\nFlorida percentage totals by category:\n\n\n\n\n\nCategory\nTotal %\n\n\n\n\ngender\n100.01%\n\n\nrace\n100.00%\n\n\n\n\n\n\n\nMaine: Count and Percentage Verification\n\n# Maine provides both counts and percentages\nmaine_data = foia_raw[foia_raw['state'] == 'Maine'].copy()\n\nmaine_wide = maine_data.pivot_table(\n    index=['offender_type', 'variable_category', 'variable_detailed'],\n    columns='value_type',\n    values='value_numeric',\n    aggfunc='first'\n).reset_index()\n\nif 'count' in maine_wide.columns and 'percentage' in maine_wide.columns:\n    maine_check = maine_wide[maine_wide['count'].notna()].copy()\n    total_maine = 33711\n\n    maine_check['calc_percentage'] = round(maine_check['count'] / total_maine * 100, 1)\n    maine_check['diff'] = maine_check['calc_percentage'] - maine_check['percentage']\n\n    display(Markdown(\"**Maine: Comparing reported vs calculated percentages**\"))\n    \n    summary_df = maine_check[maine_check['variable_category'] != 'total'][\n        ['variable_category', 'variable_detailed', 'count', 'percentage', 'calc_percentage', 'diff']\n    ].copy()\n    summary_df.columns = ['Category', 'Detail', 'Count', 'Reported %', 'Calculated %', 'Difference']\n    \n    # Format for display\n    summary_df['Count'] = summary_df['Count'].apply(lambda x: f\"{x:,.0f}\")\n    summary_df['Reported %'] = summary_df['Reported %'].apply(lambda x: f\"{x:.1f}%\")\n    summary_df['Calculated %'] = summary_df['Calculated %'].apply(lambda x: f\"{x:.1f}%\")\n    summary_df['Difference'] = summary_df['Difference'].apply(lambda x: f\"{x:+.1f}\")\n    \n    display(HTML(summary_df.to_html(index=False, classes='table table-striped table-hover')))\n\n    # Check sums\n    maine_pct_sum = maine_check.groupby('variable_category')['percentage'].sum()\n    display(Markdown(\"\\n**Maine percentage totals by category:**\"))\n    totals_df = pd.DataFrame({\n        'Category': maine_pct_sum[maine_pct_sum.index != 'total'].index,\n        'Total %': [f\"{x:.1f}%\" for x in maine_pct_sum[maine_pct_sum.index != 'total'].values]\n    })\n    display(HTML(totals_df.to_html(index=False, classes='table table-striped')))\n\nMaine: Comparing reported vs calculated percentages\n\n\n\n\n\nCategory\nDetail\nCount\nReported %\nCalculated %\nDifference\n\n\n\n\ngender\nFemale\n5,734\n17.0%\n17.0%\n+0.0\n\n\ngender\nMale\n27,694\n82.7%\n82.2%\n-0.5\n\n\ngender\nUnknown\n83\n0.2%\n0.2%\n+0.0\n\n\nrace\nAsian\n128\n0.4%\n0.4%\n+0.0\n\n\nrace\nBlack\n1,299\n3.9%\n3.9%\n+0.0\n\n\nrace\nHispanic\n171\n0.5%\n0.5%\n+0.0\n\n\nrace\nNative American\n345\n1.0%\n1.0%\n+0.0\n\n\nrace\nUnknown\n470\n1.4%\n1.4%\n+0.0\n\n\nrace\nWhite\n31,298\n92.8%\n92.8%\n+0.0\n\n\n\n\n\nMaine percentage totals by category:\n\n\n\n\n\nCategory\nTotal %\n\n\n\n\ngender\n99.9%\n\n\nrace\n100.0%\n\n\n\n\n\n\n\nSouth Dakota: Count and Percentage Verification\n\n# South Dakota - check main categories (not intersections)\nsd_data = foia_raw[\n    (foia_raw['state'] == 'South Dakota') & \n    (foia_raw['variable_category'].isin(['total', 'gender', 'race']))\n].copy()\n\nsd_wide = sd_data.pivot_table(\n    index=['offender_type', 'variable_category', 'variable_detailed'],\n    columns='value_type',\n    values='value_numeric',\n    aggfunc='first'\n).reset_index()\n\nsd_check = sd_wide[sd_wide['count'].notna()].copy()\ntotal_sd = 67753\n\nsd_check['calc_percentage'] = np.where(\n    sd_check['variable_detailed'] == 'total_profiles',\n    100.0,\n    round(sd_check['count'] / total_sd * 100, 2)\n)\nsd_check['diff'] = sd_check['calc_percentage'] - sd_check['percentage']\n\ndisplay(Markdown(\"**South Dakota: Comparing reported vs calculated percentages**\"))\n\nsummary_df = sd_check[sd_check['variable_category'] != 'total'][\n    ['variable_category', 'variable_detailed', 'count', 'percentage', 'calc_percentage', 'diff']\n].copy()\nsummary_df.columns = ['Category', 'Detail', 'Count', 'Reported %', 'Calculated %', 'Difference']\n\n# Format for display\nsummary_df['Count'] = summary_df['Count'].apply(lambda x: f\"{x:,.0f}\")\nsummary_df['Reported %'] = summary_df['Reported %'].apply(lambda x: f\"{x:.2f}%\")\nsummary_df['Calculated %'] = summary_df['Calculated %'].apply(lambda x: f\"{x:.2f}%\")\nsummary_df['Difference'] = summary_df['Difference'].apply(lambda x: f\"{x:+.2f}\")\n\ndisplay(HTML(summary_df.to_html(index=False, classes='table table-striped table-hover')))\n\n# Check sums\nsd_pct_sum = sd_check.groupby('variable_category')['percentage'].sum()\ndisplay(Markdown(\"\\n**South Dakota percentage totals by category:**\"))\ntotals_df = pd.DataFrame({\n    'Category': sd_pct_sum[sd_pct_sum.index != 'total'].index,\n    'Total %': [f\"{x:.2f}%\" for x in sd_pct_sum[sd_pct_sum.index != 'total'].values]\n})\ndisplay(HTML(totals_df.to_html(index=False, classes='table table-striped')))\n\ndisplay(Markdown(\"\\n*Note: South Dakota uniquely provides race×gender intersection data (24 additional rows not shown here)*\"))\n\nSouth Dakota: Comparing reported vs calculated percentages\n\n\n\n\n\nCategory\nDetail\nCount\nReported %\nCalculated %\nDifference\n\n\n\n\ngender\nFemale\n16,556\n24.44%\n24.44%\n+0.00\n\n\ngender\nMale\n51,197\n75.56%\n75.56%\n+0.00\n\n\nrace\nAsian\n5\n0.08%\n0.01%\n-0.07\n\n\nrace\nBlack\n4,041\n5.96%\n5.96%\n+0.00\n\n\nrace\nHispanic\n2,949\n4.35%\n4.35%\n+0.00\n\n\nrace\nNative American\n14,593\n21.54%\n21.54%\n+0.00\n\n\nrace\nOther/Unknown\n891\n1.32%\n1.32%\n+0.00\n\n\nrace\nWhite/Caucasian\n45,223\n66.75%\n66.75%\n+0.00\n\n\n\n\n\nSouth Dakota percentage totals by category:\n\n\n\n\n\nCategory\nTotal %\n\n\n\n\ngender\n100.00%\n\n\nrace\n100.00%\n\n\n\n\n\nNote: South Dakota uniquely provides race×gender intersection data (24 additional rows not shown here)\n\n\n\n\n\nStep 2: States with Only Counts\nThese states need percentages calculated.\n\nCalifornia: Calculate Percentages\n\n# California only provided counts\ncalifornia_data = foia_raw[foia_raw['state'] == 'California'].copy()\n\n# Get totals for each offender type\nca_totals = california_data[california_data['variable_category'] == 'total'].set_index('offender_type')['value_numeric'].to_dict()\n\n# Calculate percentages\nca_with_pct = california_data[california_data['value_type'] == 'count'].copy()\n\ndef get_total(row):\n    if row['variable_category'] == 'total':\n        return row['value_numeric']\n    else:\n        return ca_totals.get(row['offender_type'], 0)\n\nca_with_pct['total'] = ca_with_pct.apply(get_total, axis=1)\nca_with_pct['percentage'] = round(ca_with_pct['value_numeric'] / ca_with_pct['total'] * 100, 2)\n\ndisplay(Markdown(\"**California: Calculated percentages**\"))\n\n# Show sample by offender type\nfor offender_type in ['Convicted Offender', 'Arrestee']:\n    subset = ca_with_pct[\n        (ca_with_pct['offender_type'] == offender_type) & \n        (ca_with_pct['variable_category'] != 'total')\n    ].copy()\n    \n    display(Markdown(f\"\\n*{offender_type}:*\"))\n    summary_df = subset[['variable_category', 'variable_detailed', 'value_numeric', 'percentage']]\n    summary_df.columns = ['Category', 'Detail', 'Count', 'Calculated %']\n    summary_df['Count'] = summary_df['Count'].apply(lambda x: f\"{x:,.0f}\")\n    summary_df['Calculated %'] = summary_df['Calculated %'].apply(lambda x: f\"{x:.2f}%\")\n    display(HTML(summary_df.to_html(index=False, classes='table table-striped table-hover')))\n\n# Check if percentages sum to 100%\nca_pct_check = ca_with_pct[ca_with_pct['variable_category'] != 'total'].groupby(\n    ['offender_type', 'variable_category']\n)['percentage'].sum().reset_index()\n\ndisplay(Markdown(\"\\n**California: Percentage totals by category**\"))\nca_pct_check.columns = ['Offender Type', 'Category', 'Total %']\nca_pct_check['Total %'] = ca_pct_check['Total %'].apply(lambda x: f\"{x:.2f}%\")\ndisplay(HTML(ca_pct_check.to_html(index=False, classes='table table-striped')))\n\ndisplay(Markdown(\"\\n⚠️ **Warning**: California race percentages don't sum to 100%, indicating missing categories\"))\n\nCalifornia: Calculated percentages\n\n\nConvicted Offender:\n\n\n\n\n\nCategory\nDetail\nCount\nCalculated %\n\n\n\n\ngender\nFemale\n309,827\n15.34%\n\n\ngender\nMale\n1,603,222\n79.37%\n\n\ngender\nUnknown\n106,850\n5.29%\n\n\nrace\nAfrican American\n368,952\n18.27%\n\n\nrace\nCaucasian\n588,555\n29.14%\n\n\nrace\nHispanic\n652,121\n32.28%\n\n\nrace\nAsian\n16,384\n0.81%\n\n\n\n\n\nArrestee:\n\n\n\n\n\nCategory\nDetail\nCount\nCalculated %\n\n\n\n\ngender\nFemale\n208,225\n27.70%\n\n\ngender\nMale\n524,231\n69.73%\n\n\ngender\nUnknown\n19,366\n2.58%\n\n\nrace\nAfrican American\n104,741\n13.93%\n\n\nrace\nCaucasian\n231,313\n30.77%\n\n\nrace\nHispanic\n308,450\n41.03%\n\n\nrace\nAsian\n11,191\n1.49%\n\n\n\n\n\nCalifornia: Percentage totals by category\n\n\n\n\n\nOffender Type\nCategory\nTotal %\n\n\n\n\nArrestee\ngender\n100.01%\n\n\nArrestee\nrace\n87.22%\n\n\nConvicted Offender\ngender\n100.00%\n\n\nConvicted Offender\nrace\n80.50%\n\n\n\n\n\n⚠️ Warning: California race percentages don’t sum to 100%, indicating missing categories\n\n\n\n\nTexas: Calculate Percentages and Identify Missing Data\n\n# Texas only provided counts (and only female counts for gender)\ntexas_data = foia_raw[foia_raw['state'] == 'Texas'].copy()\n\n# First, check if race counts sum to totals\ntexas_totals = texas_data[texas_data['variable_category'] == 'total'].set_index('offender_type')['value_numeric'].to_dict()\ntexas_race_sums = texas_data[texas_data['variable_category'] == 'race'].groupby('offender_type')['value_numeric'].sum()\n\ndisplay(Markdown(\"**Texas: Missing race data (Unknown category)**\"))\nmissing_data = []\nfor offender_type, total in texas_totals.items():\n    if offender_type in texas_race_sums.index:\n        race_sum = texas_race_sums[offender_type]\n        missing = total - race_sum\n        missing_data.append({\n            'Offender Type': offender_type,\n            'Total': f\"{int(total):,}\",\n            'Race Sum': f\"{int(race_sum):,}\",\n            'Missing': f\"{int(missing):,}\",\n            'Missing %': f\"{missing/total * 100:.2f}%\"\n        })\n\nmissing_df = pd.DataFrame(missing_data)\ndisplay(HTML(missing_df.to_html(index=False, classes='table table-striped table-hover')))\n\n# Calculate percentages for available data\ntexas_with_pct = texas_data[\n    (texas_data['value_type'] == 'count') & \n    (texas_data['variable_category'] != 'total')\n].copy()\n\ntexas_with_pct['total'] = texas_with_pct['offender_type'].map(texas_totals)\ntexas_with_pct['percentage'] = round(texas_with_pct['value_numeric'] / texas_with_pct['total'] * 100, 2)\n\ndisplay(Markdown(\"\\n**Texas: Gender data (only female counts provided)**\"))\ngender_df = texas_with_pct[texas_with_pct['variable_category'] == 'gender'][\n    ['offender_type', 'variable_detailed', 'value_numeric', 'percentage']\n].copy()\ngender_df.columns = ['Offender Type', 'Gender', 'Count', 'Percentage']\ngender_df['Count'] = gender_df['Count'].apply(lambda x: f\"{x:,.0f}\")\ngender_df['Percentage'] = gender_df['Percentage'].apply(lambda x: f\"{x:.2f}%\")\ndisplay(HTML(gender_df.to_html(index=False, classes='table table-striped table-hover')))\n\n# Calculate male counts\ndisplay(Markdown(\"\\n**Calculated Male/Other counts:**\"))\nmale_data = []\nfor offender_type in ['Offenders', 'Arrestee']:\n    total = texas_totals[offender_type]\n    female = texas_with_pct[(texas_with_pct['offender_type'] == offender_type) & \n                           (texas_with_pct['variable_category'] == 'gender')]['value_numeric'].iloc[0]\n    male = total - female\n    male_data.append({\n        'Offender Type': offender_type,\n        'Male/Other Count': f\"{int(male):,}\",\n        'Male/Other %': f\"{male/total * 100:.2f}%\"\n    })\nmale_df = pd.DataFrame(male_data)\ndisplay(HTML(male_df.to_html(index=False, classes='table table-striped')))\n\ndisplay(Markdown(\"\\n⚠️ **Note**: Texas only reported female counts. Male/Other counts must be calculated by subtraction.\"))\n\nTexas: Missing race data (Unknown category)\n\n\n\n\n\nOffender Type\nTotal\nRace Sum\nMissing\nMissing %\n\n\n\n\nOffenders\n845,322\n845,293\n29\n0.00%\n\n\nArrestee\n73,631\n71,470\n2,161\n2.93%\n\n\n\n\n\nTexas: Gender data (only female counts provided)\n\n\n\n\n\nOffender Type\nGender\nCount\nPercentage\n\n\n\n\nOffenders\nFemale\n121,434\n14.37%\n\n\nArrestee\nFemale\n18,721\n25.43%\n\n\n\n\n\nCalculated Male/Other counts:\n\n\n\n\n\nOffender Type\nMale/Other Count\nMale/Other %\n\n\n\n\nOffenders\n723,888\n85.63%\n\n\nArrestee\n54,910\n74.57%\n\n\n\n\n\n⚠️ Note: Texas only reported female counts. Male/Other counts must be calculated by subtraction.\n\n\n\n\n\nStep 3: States with Only Percentages\nIndiana requires special handling as they only provided percentages for demographics.\n\nIndiana: Verify Percentages Sum to 100%\n\n# Indiana only provided percentages for demographics\nindiana_pct = foia_raw[\n    (foia_raw['state'] == 'Indiana') & \n    (foia_raw['value_type'] == 'percentage')\n].copy()\n\n# Check if percentages sum to 100%\nindiana_pct_check = indiana_pct.groupby('variable_category').agg({\n    'value_numeric': 'sum',\n    'variable_detailed': lambda x: ', '.join(x)\n}).rename(columns={'value_numeric': 'total_percentage'})\n\ndisplay(Markdown(\"**Indiana: Checking if percentages sum to 100%**\"))\ncheck_df = indiana_pct_check.reset_index()\ncheck_df.columns = ['Category', 'Total %', 'Values']\ncheck_df['Total %'] = check_df['Total %'].apply(lambda x: f\"{x:.1f}%\")\ndisplay(HTML(check_df.to_html(index=False, classes='table table-striped table-hover')))\n\n# Get totals for calculating counts\nindiana_totals = foia_raw[\n    (foia_raw['state'] == 'Indiana') & \n    (foia_raw['variable_category'] == 'total')\n]\n\nconvicted = indiana_totals[indiana_totals['offender_type'] == 'Convicted Offender']['value_numeric'].iloc[0]\narrestee = indiana_totals[indiana_totals['offender_type'] == 'Arrestee']['value_numeric'].iloc[0]\ncombined = convicted + arrestee\n\ndisplay(Markdown(\"\\n**Indiana totals for calculating counts:**\"))\ntotals_data = [\n    ['Convicted', f\"{int(convicted):,}\"],\n    ['Arrestee', f\"{int(arrestee):,}\"],\n    ['Combined', f\"{int(combined):,}\"]\n]\ntotals_df = pd.DataFrame(totals_data, columns=['Type', 'Count'])\ndisplay(HTML(totals_df.to_html(index=False, classes='table table-striped')))\n\n# Calculate counts from percentages\ndisplay(Markdown(\"\\n**Calculated counts from percentages:**\"))\ncalculated_counts = []\nfor _, row in indiana_pct.iterrows():\n    count = round(combined * row['value_numeric'] / 100)\n    calculated_counts.append({\n        'Category': row['variable_category'],\n        'Detail': row['variable_detailed'],\n        'Percentage': f\"{row['value_numeric']}%\",\n        'Calculated Count': f\"{int(count):,}\"\n    })\ncalc_df = pd.DataFrame(calculated_counts)\ndisplay(HTML(calc_df.to_html(index=False, classes='table table-striped table-hover')))\n\ndisplay(Markdown(\"\\n⚠️ **Note**: Indiana's race percentages sum to 100.5% due to '&lt;1%' being interpreted as 0.5%\"))\n\nIndiana: Checking if percentages sum to 100%\n\n\n\n\n\nCategory\nTotal %\nValues\n\n\n\n\ngender\n100.0%\nFemale, Male\n\n\nrace\n100.5%\nCaucasian, Black, Hispanic, Other\n\n\n\n\n\nIndiana totals for calculating counts:\n\n\n\n\n\nType\nCount\n\n\n\n\nConvicted\n279,654\n\n\nArrestee\n21,087\n\n\nCombined\n300,741\n\n\n\n\n\nCalculated counts from percentages:\n\n\n\n\n\nCategory\nDetail\nPercentage\nCalculated Count\n\n\n\n\ngender\nFemale\n20.0%\n60,148\n\n\ngender\nMale\n80.0%\n240,593\n\n\nrace\nCaucasian\n70.0%\n210,519\n\n\nrace\nBlack\n26.0%\n78,193\n\n\nrace\nHispanic\n4.0%\n12,030\n\n\nrace\nOther\n0.5%\n1,504\n\n\n\n\n\n⚠️ Note: Indiana’s race percentages sum to 100.5% due to ‘&lt;1%’ being interpreted as 0.5%\n\n\n\n\n\nStep 4: Nevada Special Case\nNevada provides counts for all data but only some percentages.\n\n# Nevada has counts for everything, percentages for some\nnevada_data = foia_raw[foia_raw['state'] == 'Nevada'].copy()\n\nnevada_wide = nevada_data.pivot_table(\n    index=['offender_type', 'variable_category', 'variable_detailed'],\n    columns='value_type',\n    values='value_numeric',\n    aggfunc='first'\n).reset_index()\n\ndisplay(Markdown(\"**Nevada: Data with provided percentages**\"))\nwith_pct = nevada_wide[nevada_wide['percentage'].notna()].copy()\nsummary_df = with_pct[['offender_type', 'variable_category', 'variable_detailed', 'count', 'percentage']]\nsummary_df.columns = ['Offender Type', 'Category', 'Detail', 'Count', 'Percentage']\nsummary_df['Count'] = summary_df['Count'].apply(lambda x: f\"{x:,.0f}\")\nsummary_df['Percentage'] = summary_df['Percentage'].apply(lambda x: f\"{x:.4f}%\")\ndisplay(HTML(summary_df.to_html(index=False, classes='table table-striped table-hover')))\n\n# Calculate missing percentages\ntotal_flags = 344097\nnevada_calc = nevada_wide[nevada_wide['percentage'].isna() & nevada_wide['count'].notna()].copy()\nnevada_calc['calc_percentage'] = round(nevada_calc['count'] / total_flags * 100, 3)\n\ndisplay(Markdown(\"\\n**Nevada: Calculated percentages for missing data**\"))\ncalc_df = nevada_calc[['offender_type', 'variable_category', 'variable_detailed', 'count', 'calc_percentage']].copy()\ncalc_df.columns = ['Offender Type', 'Category', 'Detail', 'Count', 'Calculated %']\ncalc_df['Count'] = calc_df['Count'].apply(lambda x: f\"{x:,.0f}\")\ncalc_df['Calculated %'] = calc_df['Calculated %'].apply(lambda x: f\"{x:.3f}%\")\ndisplay(HTML(calc_df.to_html(index=False, classes='table table-striped table-hover')))\n\ndisplay(Markdown(\"\\n⚠️ **Note**: Nevada uses 'flags' instead of 'profiles' terminology\"))\n\nNevada: Data with provided percentages\n\n\n\n\n\nOffender Type\nCategory\nDetail\nCount\nPercentage\n\n\n\n\nArrested offender\ntotal\ntotal_profiles\n185,074\n53.7850%\n\n\nCombined\ngender\nFemale\n63,287\n18.3920%\n\n\nCombined\ngender\nMale\n280,738\n81.5870%\n\n\nCombined\ngender\nUnknown\n72\n0.0209%\n\n\nCombined\nrace\nAmerican Indian\n5,710\n1.6590%\n\n\nCombined\nrace\nAsian\n7,999\n2.3460%\n\n\nCombined\nrace\nBlack\n88,174\n25.6250%\n\n\nCombined\nrace\nUnknown\n3,491\n1.0150%\n\n\nCombined\nrace\nWhite\n238,723\n69.3770%\n\n\nConvicted offenders\ntotal\ntotal_profiles\n159,023\n46.2150%\n\n\n\n\n\nNevada: Calculated percentages for missing data\n\n\n\n\n\nOffender Type\nCategory\nDetail\nCount\nCalculated %\n\n\n\n\nAll\ntotal\ntotal_flags\n344,097\n100.000%\n\n\n\n\n\n⚠️ Note: Nevada uses ‘flags’ instead of ‘profiles’ terminology"
  },
  {
    "objectID": "analysis/foia_processing.html#summary-of-calculations-needed",
    "href": "analysis/foia_processing.html#summary-of-calculations-needed",
    "title": "FOIA Document OCR Processing",
    "section": "Summary of Calculations Needed",
    "text": "Summary of Calculations Needed\n\n1. Verification Results (States with Both)\n✅ All states with both counts and percentages show excellent agreement:\n\nFlorida: Perfect match - all differences are 0.00\nMaine: Only 0.5% difference for Male due to rounding\n\nSouth Dakota: Only 0.07% difference for Asian due to rounding\n\n\n\n2. States Requiring Percentage Calculations\n⚠️ Critical issues found:\n\nCalifornia:\n\nAll percentages need calculation\nRace percentages don’t sum to 100% (Convicted: 80.50%, Arrestee: 87.22%)\n\nTexas:\n\nAll percentages need calculation\nOnly female gender counts provided\nRace counts missing Unknown category\n\n\n\n\n3. States Requiring Count Calculations\n\nIndiana:\n\nGender percentages sum to 100% ✓\nRace percentages sum to 100.5% (due to “&lt;1%” → 0.5%)\nUse combined total of 300,741 for calculations\n\n\n\n\n4. Mixed Reporting\n\nNevada:\n\nHas counts for all data\nMissing some percentages (need calculation)\nUses “flags” terminology instead of “profiles”"
  },
  {
    "objectID": "analysis/foia_processing.html#data-quality-issues-identified",
    "href": "analysis/foia_processing.html#data-quality-issues-identified",
    "title": "FOIA Document OCR Processing",
    "section": "Data Quality Issues Identified",
    "text": "Data Quality Issues Identified\n\nTexas Gender Data: Only female counts provided\nTexas Race Data: Totals don’t match sum of races (missing Unknown)\nIndiana “Other” Race: Reported as “&lt;1%” - needs interpretation\nFlorida Rounding: Total percentages sum to 100.01%\nNevada Terminology: Uses “flags” instead of “profiles”\nSouth Dakota Complexity: Provides race×gender intersections others don’t"
  },
  {
    "objectID": "analysis/ndis_scraping.html",
    "href": "analysis/ndis_scraping.html",
    "title": "NDIS Database Analysis",
    "section": "",
    "text": "This analysis processes NDIS (National DNA Index System) statistics from archived FBI web pages. We parse 300+ HTML snapshots from the Wayback Machine to track how the DNA database has grown from 2010 to 2025."
  },
  {
    "objectID": "analysis/ndis_scraping.html#introduction",
    "href": "analysis/ndis_scraping.html#introduction",
    "title": "NDIS Database Analysis",
    "section": "",
    "text": "This analysis processes NDIS (National DNA Index System) statistics from archived FBI web pages. We parse 300+ HTML snapshots from the Wayback Machine to track how the DNA database has grown from 2010 to 2025."
  },
  {
    "objectID": "analysis/ndis_scraping.html#setup-and-configuration",
    "href": "analysis/ndis_scraping.html#setup-and-configuration",
    "title": "NDIS Database Analysis",
    "section": "Setup and Configuration",
    "text": "Setup and Configuration\n\n\nInstalling beautifulsoup4...\nRequirement already satisfied: beautifulsoup4 in /Users/tlasisi/GitHub/PODFRIDGE-Databases/podfridge-db-env/lib/python3.13/site-packages (4.13.4)\nRequirement already satisfied: soupsieve&gt;1.2 in /Users/tlasisi/GitHub/PODFRIDGE-Databases/podfridge-db-env/lib/python3.13/site-packages (from beautifulsoup4) (2.7)\nRequirement already satisfied: typing-extensions&gt;=4.0.0 in /Users/tlasisi/GitHub/PODFRIDGE-Databases/podfridge-db-env/lib/python3.13/site-packages (from beautifulsoup4) (4.14.1)\n\n\n\nfrom pathlib import Path\nimport re, json, requests, time\nfrom datetime import datetime\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Configuration\n# Path setup - using current directory as base\n# This assumes you run the notebook from the project root (PODFRIDGE-Databases)\nBASE_DIR = Path(\"..\")  # Current working directory\nHTML_DIR = BASE_DIR / \"raw\" / \"wayback_html\"\nMETA_DIR = BASE_DIR / \"raw\" / \"wayback_meta\"\nOUTPUT_DIR = BASE_DIR / \"output\" / \"ndis\"\n\n# Create directories if they don't exist\nHTML_DIR.mkdir(parents=True, exist_ok=True)\nMETA_DIR.mkdir(parents=True, exist_ok=True)\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# Jurisdiction name standardization mapping\nJURISDICTION_NAME_MAP = {\n    'D.C./FBI Lab': 'DC/FBI Lab',\n    'US Army': 'U.S. Army'\n}\n\n# Known data typos to fix\nKNOWN_TYPOS = [\n    {\n        'timestamp': '20250105164014',\n        'jurisdiction': 'California', \n        'field': 'investigations_aided',\n        'wrong_value': '1304657',  # How it parses\n        'correct_value': '130465'   # What it should be\n    },\n    {\n        'timestamp': '20250116205311',\n        'jurisdiction': 'California',\n        'field': 'investigations_aided', \n        'wrong_value': '1304657',\n        'correct_value': '130465'\n    }\n]\n\nprint(f\"Working directory: {BASE_DIR.resolve()}\")\nprint(f\"HTML directory: {HTML_DIR}\")\nprint(f\"Meta directory: {META_DIR}\")\nprint(f\"Output directory: {OUTPUT_DIR}\")\n\nWorking directory: /Users/tlasisi/GitHub/PODFRIDGE-Databases\nHTML directory: ../raw/wayback_html\nMeta directory: ../raw/wayback_meta\nOutput directory: ../output/ndis"
  },
  {
    "objectID": "analysis/ndis_scraping.html#wayback-machine-functions",
    "href": "analysis/ndis_scraping.html#wayback-machine-functions",
    "title": "NDIS Database Analysis",
    "section": "Wayback Machine Functions",
    "text": "Wayback Machine Functions\n\ndef make_request_with_retry(params, max_retries=3, initial_delay=5):\n    \"\"\"Make a request with exponential backoff retry logic\"\"\"\n    base = \"https://web.archive.org/cdx/search/cdx\"\n    \n    for attempt in range(max_retries):\n        try:\n            r = requests.get(base, params=params, timeout=30)\n            if r.status_code == 200:\n                return r\n            elif r.status_code == 429:  # Rate limited\n                wait_time = initial_delay * (2 ** attempt)\n                print(f\"    Rate limited. Waiting {wait_time} seconds...\")\n                time.sleep(wait_time)\n            else:\n                return r\n        except requests.exceptions.ConnectionError as e:\n            wait_time = initial_delay * (2 ** attempt)\n            print(f\"    Connection error. Waiting {wait_time} seconds before retry {attempt + 1}/{max_retries}\")\n            time.sleep(wait_time)\n        except Exception as e:\n            print(f\"    Unexpected error: {e}\")\n            return None\n    return None"
  },
  {
    "objectID": "analysis/ndis_scraping.html#search-for-all-ndis-snapshots",
    "href": "analysis/ndis_scraping.html#search-for-all-ndis-snapshots",
    "title": "NDIS Database Analysis",
    "section": "Search for All NDIS Snapshots",
    "text": "Search for All NDIS Snapshots\n\n\nShow search function code\ndef search_all_ndis_snapshots():\n    \"\"\"Search for NDIS snapshots across all known URL variations\"\"\"\n    \n    # Search for both http and https variants\n    protocols = [\"http://\", \"https://\"]\n    subdomains = [\"www\", \"le\", \"*\"]  # Known subdomains plus wildcard\n    \n    all_rows = []\n    seen_timestamps = set()\n    \n    # First, try broad searches with protocol wildcards\n    print(\"Starting wildcard searches...\")\n    for protocol in protocols:\n        for subdomain in subdomains:\n            pattern = f\"{protocol}{subdomain}.fbi.gov/*ndis-statistics*\"\n            print(f\"\\nSearching: {pattern}\")\n            \n            params = {\n                \"url\":         pattern,\n                \"matchType\":   \"wildcard\",\n                \"output\":      \"json\",\n                \"fl\":          \"timestamp,original,mimetype,statuscode\",\n                \"filter\":      [\"statuscode:200\", \"mimetype:text/html\"],\n                \"limit\":       \"10000\",\n            }\n            \n            r = make_request_with_retry(params)\n            if r and r.status_code == 200:\n                data = json.loads(r.text)\n                if len(data) &gt; 1:\n                    new_rows = 0\n                    for row in data[1:]:\n                        if row[0] not in seen_timestamps:\n                            all_rows.append(row)\n                            seen_timestamps.add(row[0])\n                            new_rows += 1\n                    print(f\"  → Found {new_rows} new snapshots\")\n                else:\n                    print(f\"  → No results\")\n            else:\n                print(f\"  → Failed after retries\")\n            \n            # Always wait between requests to avoid rate limiting\n            time.sleep(2)\n    \n    # Also search your specific known URLs with both protocols\n    known_paths = [\n        \"www.fbi.gov/about-us/lab/codis/ndis-statistics\",\n        \"www.fbi.gov/about-us/laboratory/biometric-analysis/codis/ndis-statistics\", \n        \"www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics\",\n        \"le.fbi.gov/science-and-lab/biometrics-and-fingerprints/codis/codis-ndis-statistics\",\n    ]\n    \n    print(\"\\n\\nStarting exact URL searches...\")\n    for path in known_paths:\n        for protocol in protocols:\n            url = f\"{protocol}{path}\"\n            print(f\"\\nSearching: {url}\")\n            \n            params = {\n                \"url\":         url,\n                \"matchType\":   \"exact\",\n                \"output\":      \"json\",\n                \"fl\":          \"timestamp,original,mimetype,statuscode\",\n                \"filter\":      [\"statuscode:200\", \"mimetype:text/html\"],\n                \"limit\":       \"10000\",\n            }\n            \n            r = make_request_with_retry(params)\n            if r and r.status_code == 200:\n                data = json.loads(r.text)\n                if len(data) &gt; 1:\n                    new_rows = 0\n                    for row in data[1:]:\n                        if row[0] not in seen_timestamps:\n                            all_rows.append(row)\n                            seen_timestamps.add(row[0])\n                            new_rows += 1\n                    print(f\"  → Found {new_rows} new snapshots\")\n                else:\n                    print(f\"  → No results\")\n            else:\n                print(f\"  → Failed after retries\")\n            \n            # Always wait between requests\n            time.sleep(2)\n    \n    # Create DataFrame\n    snap_df = (pd.DataFrame(\n                    all_rows,\n                    columns=[\"timestamp\", \"original\", \"mimetype\", \"status\"])\n               .sort_values(\"timestamp\")\n               .reset_index(drop=True))\n    \n    return snap_df\n\n# Check if we already have snapshot data or need to search\nsnapshot_csv = META_DIR / 'snapshots_found.csv'\nif snapshot_csv.exists():\n    print(\"Loading existing snapshot list...\")\n    snap_df = pd.read_csv(snapshot_csv)\n    print(f\"Loaded {len(snap_df)} snapshots\")\nelse:\n    print(\"Searching for all NDIS snapshots...\")\n    snap_df = search_all_ndis_snapshots()\n    if len(snap_df) &gt; 0:\n        snap_df.to_csv(snapshot_csv, index=False)\n        print(f\"\\nSaved {len(snap_df)} snapshots to {snapshot_csv}\")\n\nif len(snap_df) &gt; 0:\n    print(f\"\\nTotal unique snapshots found: {len(snap_df):,}\")\n    print(f\"Unique URLs found: {snap_df['original'].nunique()}\")\n    print(\"\\nUnique URL patterns found:\")\n    for url in sorted(snap_df['original'].unique()):\n        print(f\"  {url}\")\n\n\nLoading existing snapshot list...\nLoaded 317 snapshots\n\nTotal unique snapshots found: 317\nUnique URLs found: 8\n\nUnique URL patterns found:\n  http://www.fbi.gov/about-us/lab/codis/ndis-statistics\n  http://www.fbi.gov/about-us/lab/codis/ndis-statistics/\n  http://www.fbi.gov:80/about-us/lab/codis/ndis-statistics\n  http://www.fbi.gov:80/about-us/lab/codis/ndis-statistics/\n  https://le.fbi.gov/science-and-lab/biometrics-and-fingerprints/codis/codis-ndis-statistics\n  https://www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics\n  https://www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics/\n  https://www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics//"
  },
  {
    "objectID": "analysis/ndis_scraping.html#download-functions",
    "href": "analysis/ndis_scraping.html#download-functions",
    "title": "NDIS Database Analysis",
    "section": "Download Functions",
    "text": "Download Functions\n\ndef download_with_retry(url, max_retries=3, initial_delay=5, consecutive_failures=0):\n    \"\"\"Download with adaptive retry logic based on consecutive failures\"\"\"\n    if consecutive_failures &gt; 0:\n        extra_wait = consecutive_failures * 10\n        print(f\"\\n    Adding {extra_wait}s cooldown due to {consecutive_failures} consecutive failures...\")\n        time.sleep(extra_wait)\n    \n    for attempt in range(max_retries):\n        try:\n            response = requests.get(url, timeout=30)\n            response.raise_for_status()\n            return response, True\n        except requests.exceptions.ConnectionError as e:\n            wait_time = initial_delay * (2 ** attempt)\n            print(f\"\\n    Connection error. Waiting {wait_time} seconds before retry {attempt + 1}/{max_retries}\")\n            time.sleep(wait_time)\n        except requests.exceptions.HTTPError as e:\n            if e.response.status_code == 429:\n                wait_time = initial_delay * (2 ** attempt) * 2\n                print(f\"\\n    Rate limited (429). Waiting {wait_time} seconds...\")\n                time.sleep(wait_time)\n            else:\n                print(f\"\\n    HTTP Error: {e}\")\n                return None, False\n        except Exception as e:\n            print(f\"\\n    Unexpected error: {e}\")\n            return None, False\n    return None, False\n\ndef download_missing_snapshots(snap_df, output_folder):\n    \"\"\"Download HTML snapshots with resume capability and detailed logging\"\"\"\n    \n    # Create run-specific log file\n    run_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    run_log_file = META_DIR / f\"download_log_{run_timestamp}.txt\"\n    \n    # Check what we already have\n    existing_files = list(output_folder.glob(\"*.html\"))\n    existing_timestamps = {f.stem for f in existing_files}\n    print(f\"\\nFiles already downloaded: {len(existing_files)}\")\n    \n    # Check what needs to be downloaded\n    to_download = []\n    for _, row in snap_df.iterrows():\n        timestamp = row['timestamp']\n        url = row['original']\n        filename = output_folder / f\"{timestamp}.html\"\n        \n        if timestamp not in existing_timestamps and not filename.exists():\n            to_download.append((timestamp, url, filename))\n    \n    print(f\"Files to download: {len(to_download)}\")\n    \n    # Initialize log file\n    with open(run_log_file, \"w\") as log:\n        log.write(f\"Download run started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n        log.write(f\"Total snapshots in list: {len(snap_df)}\\n\")\n        log.write(f\"Already downloaded: {len(existing_files)}\\n\")\n        log.write(f\"To download: {len(to_download)}\\n\")\n        log.write(f\"{'='*60}\\n\\n\")\n    \n    if len(to_download) == 0:\n        print(\"\\n✓ All files already downloaded! Nothing to do.\")\n        with open(run_log_file, \"a\") as log:\n            log.write(\"All files already downloaded. No action needed.\\n\")\n        return\n    \n    # Download configuration\n    BATCH_SIZE = 15\n    PAUSE_BETWEEN_DOWNLOADS = 3\n    PAUSE_BETWEEN_BATCHES = 45\n    PAUSE_AFTER_FAILURE = 60\n    \n    # Track statistics\n    successful_downloads = 0\n    failed_downloads = []\n    consecutive_failures = 0\n    \n    # Download in batches\n    for i in range(0, len(to_download), BATCH_SIZE):\n        batch = to_download[i:i + BATCH_SIZE]\n        batch_num = (i // BATCH_SIZE) + 1\n        total_batches = (len(to_download) + BATCH_SIZE - 1) // BATCH_SIZE\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"Batch {batch_num}/{total_batches} ({len(batch)} files)\")\n        print(f\"Overall progress: {len(existing_timestamps) + successful_downloads}/{len(snap_df)} total files\")\n        print(f\"{'='*60}\")\n        \n        with open(run_log_file, \"a\") as log:\n            log.write(f\"\\nBatch {batch_num}/{total_batches} started at {datetime.now().strftime('%H:%M:%S')}\\n\")\n        \n        for j, (timestamp, url, filename) in enumerate(batch, 1):\n            # Double-check file doesn't exist\n            if filename.exists():\n                print(f\"\\n[{j}/{len(batch)}] {timestamp} - Already exists, skipping...\")\n                with open(run_log_file, \"a\") as log:\n                    log.write(f\"{timestamp}  ⚡ already exists\\n\")\n                continue\n                \n            wayback_url = f\"https://web.archive.org/web/{timestamp}/{url}\"\n            \n            print(f\"\\n[{j}/{len(batch)}] Downloading {timestamp}...\", end=\"\")\n            \n            response, success = download_with_retry(wayback_url, consecutive_failures=consecutive_failures)\n            \n            if response and response.status_code == 200:\n                try:\n                    with open(filename, 'w', encoding='utf-8') as f:\n                        f.write(response.text)\n                    \n                    print(\" ✓ Success\")\n                    successful_downloads += 1\n                    consecutive_failures = 0\n                    \n                    with open(run_log_file, \"a\") as log:\n                        log.write(f\"{timestamp}  ✓ downloaded\\n\")\n                    \n                except Exception as e:\n                    print(f\" ✗ Error saving file: {e}\")\n                    failed_downloads.append((timestamp, url, str(e)))\n                    consecutive_failures += 1\n                    \n                    with open(run_log_file, \"a\") as log:\n                        log.write(f\"{timestamp}  ✗ failed: {str(e)}\\n\")\n            else:\n                print(\" ✗ Failed after retries\")\n                failed_downloads.append((timestamp, url, \"Download failed\"))\n                consecutive_failures += 1\n                \n                with open(run_log_file, \"a\") as log:\n                    log.write(f\"{timestamp}  ✗ failed: Download failed after retries\\n\")\n                \n                if j &lt; len(batch):\n                    print(f\"    Taking {PAUSE_AFTER_FAILURE}s break after failure...\")\n                    time.sleep(PAUSE_AFTER_FAILURE)\n                    continue\n            \n            if j &lt; len(batch) and consecutive_failures == 0:\n                print(f\"    Waiting {PAUSE_BETWEEN_DOWNLOADS} seconds...\")\n                time.sleep(PAUSE_BETWEEN_DOWNLOADS)\n        \n        if i + BATCH_SIZE &lt; len(to_download):\n            print(f\"\\nBatch complete. Pausing {PAUSE_BETWEEN_BATCHES} seconds...\")\n            print(f\"This session: {successful_downloads} downloaded, {len(failed_downloads)} failed\")\n            time.sleep(PAUSE_BETWEEN_BATCHES)\n    \n    # Final summary\n    print(f\"\\n{'='*60}\")\n    print(f\"Download session complete!\")\n    print(f\"  Successfully downloaded: {successful_downloads}\")\n    print(f\"  Failed downloads: {len(failed_downloads)}\")\n    \n    # Write final summary to log\n    with open(run_log_file, \"a\") as log:\n        log.write(f\"\\n{'='*60}\\n\")\n        log.write(f\"Download run completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n        log.write(f\"Successfully downloaded: {successful_downloads}\\n\")\n        log.write(f\"Failed downloads: {len(failed_downloads)}\\n\")\n        \n        if failed_downloads:\n            log.write(f\"\\nFailed downloads detail:\\n\")\n            for timestamp, url, error in failed_downloads:\n                log.write(f\"  {timestamp}: {error}\\n\")\n    \n    if failed_downloads:\n        print(f\"\\nFailed downloads:\")\n        for timestamp, url, error in failed_downloads[:10]:\n            print(f\"  {timestamp}: {error}\")\n        if len(failed_downloads) &gt; 10:\n            print(f\"  ... and {len(failed_downloads) - 10} more\")\n    \n    print(f\"\\nDownload log saved to: {run_log_file}\")\n\n# Download missing files\nif len(snap_df) &gt; 0:\n    download_missing_snapshots(snap_df, HTML_DIR)\n\n\nFiles already downloaded: 0\nFiles to download: 317\n\n============================================================\nBatch 1/22 (15 files)\nOverall progress: 0/317 total files\n============================================================\n\n[1/15] Downloading 20101014043819... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20110111093835... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20110127075531... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20110410202836... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20110706142451... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20110903022829... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20111001205511... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20111022093959... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20111026175039... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20111101090945... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20111115114507... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20111202050758... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20120101203804... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20120111115907... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20120508173804... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 15 downloaded, 0 failed\n\n============================================================\nBatch 2/22 (15 files)\nOverall progress: 15/317 total files\n============================================================\n\n[1/15] Downloading 20120701052745... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20120815160113... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20120911092525... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20120913074430... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20120913074444... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20120915124215...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[7/15] Downloading 20120921122419...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20120927161952... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20120927162304... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20120928031850... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20121006223803... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20121014054004... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20121014145914... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20121017230003... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20121018124013... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 29 downloaded, 1 failed\n\n============================================================\nBatch 3/22 (15 files)\nOverall progress: 29/317 total files\n============================================================\n\n[1/15] Downloading 20121021061338... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20121022162330... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20121023200759... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20121025072144... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20121027093545... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20121028031145... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20121030024545... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20121031002501... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20121103063643... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20121105112218... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20121114072458... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20160730053232...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[13/15] Downloading 20160826152101...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20161014043057... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20170106073100... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 43 downloaded, 2 failed\n\n============================================================\nBatch 4/22 (15 files)\nOverall progress: 43/317 total files\n============================================================\n\n[1/15] Downloading 20170127174455... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20170129013226... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20170227173146... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20170409202627... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20170505054222... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20170513040512... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20170516144007... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20170516201425... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20170518003707... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20170520050254... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20170621200054... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20170710141223... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20170925144132... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20170930134422... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20171014140750... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 58 downloaded, 2 failed\n\n============================================================\nBatch 5/22 (15 files)\nOverall progress: 58/317 total files\n============================================================\n\n[1/15] Downloading 20171025074255... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20171206151623... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20171214175911...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[4/15] Downloading 20180211052927...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20180314184913... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20180425125459... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20180428000233... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20180517024114... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20180608053326... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20180707231136... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20180710171340... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20180827122745... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20180911202332... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20180913062030... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20181016070747... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 72 downloaded, 3 failed\n\n============================================================\nBatch 6/22 (15 files)\nOverall progress: 72/317 total files\n============================================================\n\n[1/15] Downloading 20181017073224... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20181018155249... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20181019162711... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20181020170549... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20181021184026... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20181022182033... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20181024094434... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20181027194438... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20181104153147...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[10/15] Downloading 20181116183506...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20181130044044... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20181204032834... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20181220214643... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20181230180748... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20190103203642... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 86 downloaded, 4 failed\n\n============================================================\nBatch 7/22 (15 files)\nOverall progress: 86/317 total files\n============================================================\n\n[1/15] Downloading 20190129222014... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20190215164131... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20190302060036... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20190302211500... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20190305145401... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20190307110347... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20190320233159... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20190322204155... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20190323202517... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20190324201718... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20190326161430... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20190401203737... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20190412150804... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20190502135053... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20190502231236...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n\nBatch complete. Pausing 45 seconds...\nThis session: 100 downloaded, 5 failed\n\n============================================================\nBatch 8/22 (15 files)\nOverall progress: 100/317 total files\n============================================================\n\n[1/15] Downloading 20190510235359...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20190510235400... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20190511000325... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20190523123747... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20190611144717... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20190612165511... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20190613161343...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[8/15] Downloading 20190614180425...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20190620210900... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20190626135247... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20190702201315... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20190703224220... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20190810091618... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20190816134908... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20190824202250... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 114 downloaded, 6 failed\n\n============================================================\nBatch 9/22 (15 files)\nOverall progress: 114/317 total files\n============================================================\n\n[1/15] Downloading 20190825210357... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20190929200543... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20190929200638... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20191004193052... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20191005212555... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20191006085602... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20191011161318... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20191017000804... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20191019061318... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20191026043249... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20191027055331... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20191030211212... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20191102010133...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[14/15] Downloading 20191104050315...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20191107032513... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 128 downloaded, 7 failed\n\n============================================================\nBatch 10/22 (15 files)\nOverall progress: 128/317 total files\n============================================================\n\n[1/15] Downloading 20191108023243... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20191117165200... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20191124095229... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20191217191331... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20191226185928... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20200117180042... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20200301051858... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20200307031008... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20200326154938... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20200401175110... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20200401211339... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20200405080000... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20200414121931... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20200415182137... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20200429225129... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 143 downloaded, 7 failed\n\n============================================================\nBatch 11/22 (15 files)\nOverall progress: 143/317 total files\n============================================================\n\n[1/15] Downloading 20200510221207... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20200511224952... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20200519201942... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20200607224349...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[5/15] Downloading 20200620221947...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20200719134420... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20200728162952... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20200802095215... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20200803110741... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20200806102056... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20200806191421... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20200809030342... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20200814161013... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20200816121500... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20200818151622... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 157 downloaded, 8 failed\n\n============================================================\nBatch 12/22 (15 files)\nOverall progress: 157/317 total files\n============================================================\n\n[1/15] Downloading 20200820052431... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20200826044232... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20200914021204... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20200916105338... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20200916211529... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20200917051636... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20200918184343... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20200920212212... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20200926224215... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20201002213332...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[11/15] Downloading 20201016205200...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20201018092433... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20201020010327... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20201028215925... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20201029194830... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 171 downloaded, 9 failed\n\n============================================================\nBatch 13/22 (15 files)\nOverall progress: 171/317 total files\n============================================================\n\n[1/15] Downloading 20201111223430... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20201112011539... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20201125102405... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20201126071015... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20201203022058... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20201203072310... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20201204071640... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20201209084343... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20201218220958... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20201224155501... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20201225185038... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20201226161923... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20201227181400... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20201228193055... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20201229213443... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 186 downloaded, 9 failed\n\n============================================================\nBatch 14/22 (15 files)\nOverall progress: 186/317 total files\n============================================================\n\n[1/15] Downloading 20201230185607...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[2/15] Downloading 20210101071729...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20210103113801... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20210104124131... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20210117123428... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20210117154805... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20210118215353... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20210119080323... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20210124090728... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20210125004521... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20210126040048... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20210129170244... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20210131205415... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20210206102132... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20210212004658... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 200 downloaded, 10 failed\n\n============================================================\nBatch 15/22 (15 files)\nOverall progress: 200/317 total files\n============================================================\n\n[1/15] Downloading 20210214004627... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20210303055250... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20210305012011... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20210308040136... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20210308111758... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20210309005158... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20210311023951...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[8/15] Downloading 20210318000609...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20210324050729... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20210409020505... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20210414043038... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20210428153014... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20210507090954... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20210507090955... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20210514075457... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 214 downloaded, 11 failed\n\n============================================================\nBatch 16/22 (15 files)\nOverall progress: 214/317 total files\n============================================================\n\n[1/15] Downloading 20210527100801... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20210619090441... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20210703080605... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20210703105804... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20210706213905... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20210711064442... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20210713143621... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20210718105628... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20210808190600... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20210809035809... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20210905221144... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20210908064238... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20210924184245...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[14/15] Downloading 20210926213344...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20211010060654... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 228 downloaded, 12 failed\n\n============================================================\nBatch 17/22 (15 files)\nOverall progress: 228/317 total files\n============================================================\n\n[1/15] Downloading 20211010060655... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20211010203923... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20211019180134... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20211021235339... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20211025021035... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20211104193210... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20211114092857... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20211121045733... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20211128164432... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20211205045644... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20211205200142... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20211217120358... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20211224185611... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20220125103659... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20220130221713... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 243 downloaded, 12 failed\n\n============================================================\nBatch 18/22 (15 files)\nOverall progress: 243/317 total files\n============================================================\n\n[1/15] Downloading 20220209033036... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20220210054625... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20220214082051... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20220226123842...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[5/15] Downloading 20220305152131...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20220307213357... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20220311083835... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20220321052126... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20220326170646... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20220328083342... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20220329024143... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20220331230634... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20220402203124... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20220416015545... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20220423192010... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 257 downloaded, 13 failed\n\n============================================================\nBatch 19/22 (15 files)\nOverall progress: 257/317 total files\n============================================================\n\n[1/15] Downloading 20220519223937... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20220601085211... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20220629123237... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20230411172744... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20230515144637... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20230528103842... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20230713202131... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20230713202137... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20230829204608... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20230829213846...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[11/15] Downloading 20230831093549...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20230901125409... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20230902061832... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20230913085200... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20231025142202... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 271 downloaded, 14 failed\n\n============================================================\nBatch 20/22 (15 files)\nOverall progress: 271/317 total files\n============================================================\n\n[1/15] Downloading 20231025183800... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20231031115239... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20231102072222... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20231113033840... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20231127144926... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20231130100758... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20240111202156... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20240116143636... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20240203212847... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20240317130138... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20240413070959... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20240415150311... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20240713143931... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20240714204728... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20240822224345... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 286 downloaded, 14 failed\n\n============================================================\nBatch 21/22 (15 files)\nOverall progress: 286/317 total files\n============================================================\n\n[1/15] Downloading 20240824025139...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[2/15] Downloading 20240826235428...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20240927093855... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20241014183409... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20241128025327... ✓ Success\n    Waiting 3 seconds...\n\n[6/15] Downloading 20250105164014... ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20250116205311... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20250117084527... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20250201152305... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20250305200402... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20250307115942... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20250329050408... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20250523182825... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20250603075541... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20250606041947... ✓ Success\n\nBatch complete. Pausing 45 seconds...\nThis session: 300 downloaded, 15 failed\n\n============================================================\nBatch 22/22 (2 files)\nOverall progress: 300/317 total files\n============================================================\n\n[1/2] Downloading 20250606050213... ✓ Success\n    Waiting 3 seconds...\n\n[2/2] Downloading 20250629171550... ✓ Success\n\n============================================================\nDownload session complete!\n  Successfully downloaded: 302\n  Failed downloads: 15\n\nFailed downloads:\n  20120915124215: Download failed\n  20160730053232: Download failed\n  20171214175911: Download failed\n  20181104153147: Download failed\n  20190502231236: Download failed\n  20190613161343: Download failed\n  20191102010133: Download failed\n  20200607224349: Download failed\n  20201002213332: Download failed\n  20201230185607: Download failed\n  ... and 5 more\n\nDownload log saved to: ../raw/wayback_meta/download_log_20250721_190902.txt"
  },
  {
    "objectID": "analysis/ndis_scraping.html#download-status-check-functions",
    "href": "analysis/ndis_scraping.html#download-status-check-functions",
    "title": "NDIS Database Analysis",
    "section": "Download Status Check Functions",
    "text": "Download Status Check Functions\n\ndef get_latest_log():\n    \"\"\"Find and read the most recent download log\"\"\"\n    log_files = sorted(META_DIR.glob(\"download_log_*.txt\"))\n    if not log_files:\n        print(\"No download logs found.\")\n        return None\n    \n    latest_log = log_files[-1]\n    print(f\"Latest log: {latest_log.name}\")\n    return latest_log\n\ndef analyze_download_status():\n    \"\"\"Analyze the current download status and latest run results\"\"\"\n    \n    # Check what we have\n    html_files = list(HTML_DIR.glob(\"*.html\"))\n    downloaded_timestamps = {f.stem for f in html_files}\n    \n    # Check what we should have\n    snapshot_csv = META_DIR / 'snapshots_found.csv'\n    if snapshot_csv.exists():\n        snap_df = pd.read_csv(snapshot_csv)\n        expected_timestamps = set(snap_df['timestamp'].astype(str))\n    else:\n        print(\"No snapshot list found. Run search first.\")\n        return None\n    \n    # Calculate missing\n    missing_timestamps = expected_timestamps - downloaded_timestamps\n    \n    # Parse latest log for failures\n    latest_log = get_latest_log()\n    failed_in_last_run = []\n    \n    if latest_log:\n        with open(latest_log, 'r') as f:\n            for line in f:\n                if '✗ failed:' in line:\n                    timestamp = line.split()[0]\n                    if timestamp.isdigit() and len(timestamp) == 14:\n                        failed_in_last_run.append(timestamp)\n    \n    # Create summary\n    print(f\"\\n{'='*60}\")\n    print(\"DOWNLOAD STATUS SUMMARY\")\n    print(f\"{'='*60}\")\n    print(f\"Expected snapshots: {len(expected_timestamps)}\")\n    print(f\"Downloaded: {len(downloaded_timestamps)} ({len(downloaded_timestamps)/len(expected_timestamps)*100:.1f}%)\")\n    print(f\"Missing: {len(missing_timestamps)}\")\n    \n    if latest_log:\n        print(f\"\\nLatest run ({latest_log.name}):\")\n        print(f\"  Failed downloads: {len(failed_in_last_run)}\")\n        if failed_in_last_run:\n            print(f\"  Failed timestamps: {', '.join(failed_in_last_run[:5])}\")\n            if len(failed_in_last_run) &gt; 5:\n                print(f\"  ... and {len(failed_in_last_run) - 5} more\")\n    \n    print(f\"\\nTotal still needed: {len(missing_timestamps)}\")\n    \n    return {\n        'missing': missing_timestamps,\n        'failed_last_run': failed_in_last_run,\n        'downloaded': downloaded_timestamps,\n        'expected': expected_timestamps\n    }\n\ndef create_retry_list(status_info, retry_only_failed=True):\n    \"\"\"Create a list of files to retry downloading\"\"\"\n    if not status_info:\n        return None\n    \n    if retry_only_failed and status_info['failed_last_run']:\n        retry_timestamps = set(status_info['failed_last_run'])\n        print(f\"\\nWill retry {len(retry_timestamps)} failed downloads from last run\")\n    else:\n        retry_timestamps = status_info['missing']\n        print(f\"\\nWill retry all {len(retry_timestamps)} missing files\")\n    \n    snapshot_csv = META_DIR / 'snapshots_found.csv'\n    snap_df = pd.read_csv(snapshot_csv)\n    retry_df = snap_df[snap_df['timestamp'].astype(str).isin(retry_timestamps)]\n    \n    return retry_df"
  },
  {
    "objectID": "analysis/ndis_scraping.html#parser-functions",
    "href": "analysis/ndis_scraping.html#parser-functions",
    "title": "NDIS Database Analysis",
    "section": "Parser Functions",
    "text": "Parser Functions\n\ndef clean_jurisdiction_name(name):\n    \"\"\"Clean up jurisdiction names by removing common prefixes\"\"\"\n    name = re.sub(r'^.*?Back to top\\s*', '', name)\n    name = re.sub(r'^.*?Tables by NDIS Participant\\s*', '', name)\n    name = re.sub(r'^.*?ation\\.\\s*', '', name)\n    name = name.strip()\n    return name\n\ndef standardize_jurisdiction_name(name):\n    \"\"\"Standardize jurisdiction names to handle variations\"\"\"\n    name = clean_jurisdiction_name(name)\n    if name in JURISDICTION_NAME_MAP:\n        return JURISDICTION_NAME_MAP[name]\n    return name\n\ndef extract_data_date(html_content):\n    \"\"\"Extract the 'Statistics as of' date from HTML content\"\"\"\n    match = re.search(r'Statistics as of (\\w+ \\d{4})', html_content, re.IGNORECASE)\n    if match:\n        date_str = match.group(1)\n        try:\n            # Convert \"October 2024\" to datetime\n            return datetime.strptime(date_str, \"%B %Y\")\n        except:\n            pass\n    return None\n\ndef parse_ndis_snapshot(html_file):\n    \"\"\"Parse a single NDIS snapshot file\"\"\"\n    timestamp = html_file.stem\n    year = int(timestamp[:4])\n    \n    html_content = html_file.read_text('utf-8', errors='ignore')\n    soup = BeautifulSoup(html_content, 'lxml')\n    text = soup.get_text(' ', strip=True)\n    \n    # Extract the \"as of\" date\n    data_date = extract_data_date(html_content)\n    \n    # Normalize whitespace\n    text = re.sub(r'\\s+', ' ', text)\n    \n    records = []\n    \n    # Pattern for 2010 (no arrestee data)\n    if year &lt;= 2010:\n        pattern = re.compile(\n            r'([A-Z][a-zA-Z\\s\\.\\-\\'/&()]{2,50}?)\\s+Statistical Information\\s+'\n            r'.*?Offender Profiles\\s+([\\d,]+)\\s+'\n            r'.*?Forensic Samples\\s+([\\d,]+)\\s+'\n            r'.*?NDIS Participating Labs\\s+(\\d+)\\s+'\n            r'.*?Investigations Aided\\s+([\\d,]+)',\n            re.I\n        )\n        \n        for match in pattern.finditer(text):\n            jurisdiction_raw, offender, forensic, labs, investigations = match.groups()\n            jurisdiction = standardize_jurisdiction_name(jurisdiction_raw)\n            \n            records.append({\n                'timestamp': timestamp,\n                'jurisdiction': jurisdiction,\n                'offender_profiles': offender.replace(',', ''),\n                'arrestee': '0',\n                'forensic_profiles': forensic.replace(',', ''),\n                'ndis_labs': labs,\n                'investigations_aided': investigations.replace(',', ''),\n                'data_as_of_date': data_date\n            })\n    else:\n        # Pattern for 2011+ (includes arrestee data)\n        pattern = re.compile(\n            r'([A-Z][a-zA-Z\\s\\.\\-\\'/&()]{2,50}?)\\s+Statistical Information\\s+'\n            r'.*?Offender Profiles\\s+([\\d,]+)\\s+'\n            r'.*?Arrestee\\s+([\\d,]+)\\s+'\n            r'.*?Forensic Profiles\\s+([\\d,]+)\\s+'\n            r'.*?NDIS Participating Labs\\s+(\\d+)\\s+'\n            r'.*?Investigations Aided\\s+([\\d,]+)',\n            re.I\n        )\n        \n        for match in pattern.finditer(text):\n            jurisdiction_raw, offender, arrestee, forensic, labs, investigations = match.groups()\n            jurisdiction = standardize_jurisdiction_name(jurisdiction_raw)\n            \n            records.append({\n                'timestamp': timestamp,\n                'jurisdiction': jurisdiction,\n                'offender_profiles': offender.replace(',', ''),\n                'arrestee': arrestee.replace(',', ''),\n                'forensic_profiles': forensic.replace(',', ''),\n                'ndis_labs': labs,\n                'investigations_aided': investigations.replace(',', ''),\n                'data_as_of_date': data_date\n            })\n    \n    return records"
  },
  {
    "objectID": "analysis/ndis_scraping.html#process-all-snapshots",
    "href": "analysis/ndis_scraping.html#process-all-snapshots",
    "title": "NDIS Database Analysis",
    "section": "Process All Snapshots",
    "text": "Process All Snapshots\n\ndef process_all_snapshots():\n    \"\"\"Parse all downloaded snapshots and create datasets\"\"\"\n    print(\"Processing all snapshots...\")\n    \n    all_records = []\n    html_files = sorted(HTML_DIR.glob(\"*.html\"))\n    \n    for html_file in tqdm(html_files, desc=\"Parsing HTML files\"):\n        try:\n            records = parse_ndis_snapshot(html_file)\n            all_records.extend(records)\n        except Exception as e:\n            print(f\"Error parsing {html_file.name}: {e}\")\n    \n    # Convert to DataFrame\n    df = pd.DataFrame(all_records)\n    \n    # Convert numeric fields\n    numeric_fields = ['offender_profiles', 'arrestee', 'forensic_profiles', 'ndis_labs', 'investigations_aided']\n    for field in numeric_fields:\n        df[field] = pd.to_numeric(df[field], errors='coerce').fillna(0).astype(int)\n    \n    # Add datetime columns\n    df['capture_datetime'] = pd.to_datetime(df['timestamp'], format='%Y%m%d%H%M%S')\n    df['capture_date'] = df['capture_datetime'].dt.date\n    \n    # Sort by timestamp and jurisdiction\n    df = df.sort_values(['timestamp', 'jurisdiction'])\n    \n    return df\n\n# Process all files\ndf_raw = process_all_snapshots()\nprint(f\"\\nProcessed {len(df_raw)} total records\")\nprint(f\"Unique jurisdictions: {df_raw['jurisdiction'].nunique()}\")\nprint(f\"Date range: {df_raw['capture_datetime'].min()} to {df_raw['capture_datetime'].max()}\")\n\nProcessing all snapshots...\n\n\n\n\n\n\nProcessed 15320 total records\nUnique jurisdictions: 54\nDate range: 2010-10-14 04:38:19 to 2025-06-29 17:15:50"
  },
  {
    "objectID": "analysis/ndis_scraping.html#check-download-completeness",
    "href": "analysis/ndis_scraping.html#check-download-completeness",
    "title": "NDIS Database Analysis",
    "section": "Check Download Completeness",
    "text": "Check Download Completeness\n\n# Check if we have all expected files\nprint(\"\\nChecking download completeness...\")\nstatus = analyze_download_status()\n\n# Automatically retry if there are failures\nif status and status['failed_last_run'] and len(status['failed_last_run']) &gt; 0:\n    print(f\"\\n⚠️  Found {len(status['failed_last_run'])} failed downloads from last run. Retrying...\")\n    retry_df = create_retry_list(status, retry_only_failed=True)\n    if retry_df is not None and len(retry_df) &gt; 0:\n        download_missing_snapshots(retry_df, HTML_DIR)\n        \n        # Re-check status after retry\n        print(\"\\nRechecking status after retry...\")\n        status = analyze_download_status()\nelif status and status['missing']:\n    print(f\"\\n⚠️  {len(status['missing'])} files are missing but weren't from a failed run.\")\n    print(\"These may be new snapshots. Run download cell manually if needed.\")\nelse:\n    print(\"\\n✅ All files successfully downloaded!\")\n\n\nChecking download completeness...\nLatest log: download_log_20250721_190902.txt\n\n============================================================\nDOWNLOAD STATUS SUMMARY\n============================================================\nExpected snapshots: 317\nDownloaded: 302 (95.3%)\nMissing: 15\n\nLatest run (download_log_20250721_190902.txt):\n  Failed downloads: 15\n  Failed timestamps: 20120915124215, 20160730053232, 20171214175911, 20181104153147, 20190502231236\n  ... and 10 more\n\nTotal still needed: 15\n\n⚠️  Found 15 failed downloads from last run. Retrying...\n\nWill retry 15 failed downloads from last run\n\nFiles already downloaded: 302\nFiles to download: 15\n\n============================================================\nBatch 1/1 (15 files)\nOverall progress: 302/15 total files\n============================================================\n\n[1/15] Downloading 20120915124215... ✓ Success\n    Waiting 3 seconds...\n\n[2/15] Downloading 20160730053232... ✓ Success\n    Waiting 3 seconds...\n\n[3/15] Downloading 20171214175911... ✓ Success\n    Waiting 3 seconds...\n\n[4/15] Downloading 20181104153147... ✓ Success\n    Waiting 3 seconds...\n\n[5/15] Downloading 20190502231236...\n    Connection error. Waiting 5 seconds before retry 1/3\n\n    Connection error. Waiting 10 seconds before retry 2/3\n\n    Connection error. Waiting 20 seconds before retry 3/3\n ✗ Failed after retries\n    Taking 60s break after failure...\n\n[6/15] Downloading 20190613161343...\n    Adding 10s cooldown due to 1 consecutive failures...\n ✓ Success\n    Waiting 3 seconds...\n\n[7/15] Downloading 20191102010133... ✓ Success\n    Waiting 3 seconds...\n\n[8/15] Downloading 20200607224349... ✓ Success\n    Waiting 3 seconds...\n\n[9/15] Downloading 20201002213332... ✓ Success\n    Waiting 3 seconds...\n\n[10/15] Downloading 20201230185607... ✓ Success\n    Waiting 3 seconds...\n\n[11/15] Downloading 20210311023951... ✓ Success\n    Waiting 3 seconds...\n\n[12/15] Downloading 20210924184245... ✓ Success\n    Waiting 3 seconds...\n\n[13/15] Downloading 20220226123842... ✓ Success\n    Waiting 3 seconds...\n\n[14/15] Downloading 20230829213846... ✓ Success\n    Waiting 3 seconds...\n\n[15/15] Downloading 20240824025139... ✓ Success\n\n============================================================\nDownload session complete!\n  Successfully downloaded: 14\n  Failed downloads: 1\n\nFailed downloads:\n  20190502231236: Download failed\n\nDownload log saved to: ../raw/wayback_meta/download_log_20250721_200952.txt\n\nRechecking status after retry...\nLatest log: download_log_20250721_200952.txt\n\n============================================================\nDOWNLOAD STATUS SUMMARY\n============================================================\nExpected snapshots: 317\nDownloaded: 316 (99.7%)\nMissing: 1\n\nLatest run (download_log_20250721_200952.txt):\n  Failed downloads: 1\n  Failed timestamps: 20190502231236\n\nTotal still needed: 1"
  },
  {
    "objectID": "analysis/ndis_scraping.html#apply-typo-fixes",
    "href": "analysis/ndis_scraping.html#apply-typo-fixes",
    "title": "NDIS Database Analysis",
    "section": "Apply Typo Fixes",
    "text": "Apply Typo Fixes\n\ndef apply_typo_fixes(df):\n    \"\"\"Apply known typo corrections\"\"\"\n    df_fixed = df.copy()\n    \n    for typo in KNOWN_TYPOS:\n        mask = (\n            (df_fixed['timestamp'] == typo['timestamp']) & \n            (df_fixed['jurisdiction'] == typo['jurisdiction'])\n        )\n        if mask.any():\n            df_fixed.loc[mask, typo['field']] = int(typo['correct_value'])\n            print(f\"Fixed typo: {typo['jurisdiction']} on {typo['timestamp'][:8]} - \"\n                  f\"{typo['field']} from {typo['wrong_value']} to {typo['correct_value']}\")\n    \n    return df_fixed\n\n# Apply fixes\ndf_fixed = apply_typo_fixes(df_raw)\n\nFixed typo: California on 20250105 - investigations_aided from 1304657 to 130465\nFixed typo: California on 20250116 - investigations_aided from 1304657 to 130465"
  },
  {
    "objectID": "analysis/ndis_scraping.html#save-datasets",
    "href": "analysis/ndis_scraping.html#save-datasets",
    "title": "NDIS Database Analysis",
    "section": "Save Datasets",
    "text": "Save Datasets\n\n# Save datasets\ndf_raw.to_csv(OUTPUT_DIR / 'ndis_data_raw.csv', index=False)\ndf_fixed.to_csv(OUTPUT_DIR / 'ndis_data_fixed.csv', index=False)\nprint(f\"\\nSaved raw data to: {OUTPUT_DIR / 'ndis_data_raw.csv'}\")\nprint(f\"Saved fixed data to: {OUTPUT_DIR / 'ndis_data_fixed.csv'}\")\n\n\nSaved raw data to: ../output/ndis/ndis_data_raw.csv\nSaved fixed data to: ../output/ndis/ndis_data_fixed.csv"
  },
  {
    "objectID": "analysis/ndis_scraping.html#summary-statistics",
    "href": "analysis/ndis_scraping.html#summary-statistics",
    "title": "NDIS Database Analysis",
    "section": "Summary Statistics",
    "text": "Summary Statistics\n\n# Calculate summary statistics\nlatest_data = df_fixed[df_fixed['capture_datetime'] == df_fixed['capture_datetime'].max()]\nlatest_data = latest_data[latest_data['jurisdiction'] != 'D.C./Metro PD']\n\nprint(\"\\nLatest Statistics Summary:\")\nprint(f\"  As of: {latest_data['capture_datetime'].iloc[0]}\")\nprint(f\"  Data from: {latest_data['data_as_of_date'].iloc[0] if latest_data['data_as_of_date'].iloc[0] else 'Unknown'}\")\nprint(f\"  Jurisdictions reporting: {len(latest_data)}\")\nprint(f\"  Total offender profiles: {latest_data['offender_profiles'].sum():,}\")\nprint(f\"  Total arrestee profiles: {latest_data['arrestee'].sum():,}\")\nprint(f\"  Total forensic profiles: {latest_data['forensic_profiles'].sum():,}\")\nprint(f\"  Total investigations aided: {latest_data['investigations_aided'].sum():,}\")\n\n\nLatest Statistics Summary:\n  As of: 2025-06-29 17:15:50\n  Data from: 2025-04-01 00:00:00\n  Jurisdictions reporting: 53\n  Total offender profiles: 18,431,162\n  Total arrestee profiles: 5,879,537\n  Total forensic profiles: 1,405,917\n  Total investigations aided: 730,426"
  },
  {
    "objectID": "analysis/ndis_scraping.html#visualizations",
    "href": "analysis/ndis_scraping.html#visualizations",
    "title": "NDIS Database Analysis",
    "section": "Visualizations",
    "text": "Visualizations\n\ndef create_visualizations(df_raw, df_fixed):\n    \"\"\"Create comprehensive visualizations\"\"\"\n    # Exclude D.C./Metro PD from visualizations\n    df_raw_viz = df_raw[df_raw['jurisdiction'] != 'D.C./Metro PD']\n    df_fixed_viz = df_fixed[df_fixed['jurisdiction'] != 'D.C./Metro PD']\n    \n    # Create figure with subplots\n    fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n    \n    # 1. Jurisdictions reporting over time\n    for ax, (df, title_suffix) in zip([axes[0,0], axes[0,1]], \n                                      [(df_raw_viz, 'Raw'), (df_fixed_viz, 'Fixed')]):\n        jurisdictions_per_date = df.groupby('capture_datetime')['jurisdiction'].nunique()\n        ax.plot(jurisdictions_per_date.index, jurisdictions_per_date.values, 'b-', linewidth=2)\n        ax.set_title(f'Jurisdictions Reporting ({title_suffix})')\n        ax.set_ylabel('Number of Jurisdictions')\n        ax.grid(True, alpha=0.3)\n        ax.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n    \n    # 2. Total investigations aided\n    for ax, (df, title_suffix) in zip([axes[1,0], axes[1,1]], \n                                      [(df_raw_viz, 'Raw with Typo'), (df_fixed_viz, 'Fixed')]):\n        total_inv = df.groupby('capture_datetime')['investigations_aided'].sum()\n        ax.plot(total_inv.index, total_inv.values / 1e3, 'purple', linewidth=2)\n        ax.set_title(f'Total Investigations Aided ({title_suffix})')\n        ax.set_ylabel('Thousands of Investigations')\n        ax.grid(True, alpha=0.3)\n        \n        # Highlight the typo in raw data\n        if 'Raw' in title_suffix:\n            typo_dates = df[(df['jurisdiction'] == 'California') & \n                          (df['investigations_aided'] &gt; 1000000)]['capture_datetime']\n            for date in typo_dates:\n                ax.axvline(x=date, color='red', linestyle='--', alpha=0.5)\n                ax.text(date, ax.get_ylim()[1]*0.9, 'Typo', rotation=90, \n                       verticalalignment='bottom', color='red')\n    \n    # 3. Data lag analysis\n    ax = axes[2, 0]\n    df_with_lag = df_fixed_viz[df_fixed_viz['data_as_of_date'].notna()].copy()\n    df_with_lag['data_lag_days'] = (df_with_lag['capture_datetime'] - df_with_lag['data_as_of_date']).dt.days\n    \n    avg_lag = df_with_lag.groupby('capture_datetime')['data_lag_days'].mean()\n    ax.plot(avg_lag.index, avg_lag.values, 'orange', linewidth=2)\n    ax.set_title('Average Data Lag (Capture Date vs \"As Of\" Date)')\n    ax.set_ylabel('Days')\n    ax.grid(True, alpha=0.3)\n    \n    # 4. California investigations over time (showing typo fix)\n    ax = axes[2, 1]\n    cal_raw = df_raw_viz[df_raw_viz['jurisdiction'] == 'California']\n    cal_fixed = df_fixed_viz[df_fixed_viz['jurisdiction'] == 'California']\n    \n    ax.plot(cal_raw['capture_datetime'], cal_raw['investigations_aided'], \n            'r-', label='Raw (with typo)', linewidth=2, alpha=0.7)\n    ax.plot(cal_fixed['capture_datetime'], cal_fixed['investigations_aided'], \n            'g-', label='Fixed', linewidth=2)\n    ax.set_title('California Investigations Aided: Raw vs Fixed')\n    ax.set_ylabel('Investigations Aided')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(OUTPUT_DIR / 'ndis_analysis_complete.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n# Create visualizations\nprint(\"\\nCreating visualizations...\")\ncreate_visualizations(df_raw, df_fixed)\nprint(\"\\nProcessing complete!\")\n\n\nCreating visualizations...\n\n\n\n\n\n\n\n\n\n\nProcessing complete!"
  },
  {
    "objectID": "analysis/foia_processing.html#data-quality-issues-summary",
    "href": "analysis/foia_processing.html#data-quality-issues-summary",
    "title": "FOIA Document OCR Processing",
    "section": "Data Quality Issues Summary",
    "text": "Data Quality Issues Summary\n\n\n\n\n\n\n\n \nState\nIssue Type\nSeverity\nDescription\n\n\n\n\n0\nTexas\nGender\nCritical\nOnly female counts provided; male counts must be calculated\n\n\n1\nTexas\nRace\nCritical\nTotals don't match sum; 29 Offenders and 2,161 Arrestees missing\n\n\n2\nCalifornia\nRace\nMajor\nConvicted: 80.50% total; Arrestee: 87.22% total\n\n\n3\nIndiana\nRace\nMinor\n'&lt;1%' for Other race requires interpretation\n\n\n4\nFlorida\nGender\nMinor\nPercentages sum to 100.01% (rounding)\n\n\n5\nNevada\nTerminology\nNote\nUses 'flags' instead of 'profiles'\n\n\n6\nSouth Dakota\nComplexity\nNote\nProvides race×gender intersections (unique among states)"
  },
  {
    "objectID": "analysis/foia_processing.html#processing-workflow",
    "href": "analysis/foia_processing.html#processing-workflow",
    "title": "FOIA Document OCR Processing",
    "section": "2.3 Processing workflow",
    "text": "2.3 Processing workflow\nFor transparency, each state file is processed independently then merged into a single combined long‑format table (foia_combined):\n\nLoad one file per state from output/foia/per_state/.\nAppend its rows to foia_combined. A parallel dataframe, foia_state_metadata, records what each state reported (counts, percentages, which categories) and any state-specific characteristics (e.g. Nevada’s “flags” terminology).\nQuality‑check each state:\n\nverify that race and gender percentages sum to ≈ 100 % when provided,\nconfirm that demographic counts sum to the state’s reported total profiles,\ncalculate any missing counts or percentages and tag those rows value_source = \"calculated\".\n\nSave outputs\n\noutput/foia/foia_data_clean.csv — the fully combined tidy table with both reported and calculated values,\noutput/foia/foia_state_metadata.csv — one row per state summarising coverage and caveats. After QC passes, I will freeze foia_data_clean.csv to data/v1.0/foia_state_race_v1.0.csv."
  },
  {
    "objectID": "analysis/foia_processing.html#state-processing",
    "href": "analysis/foia_processing.html#state-processing",
    "title": "FOIA Document OCR Processing",
    "section": "State Processing",
    "text": "State Processing\n\nimport pandas as pd\nfrom pathlib import Path\nfrom IPython.display import display, Markdown\nimport matplotlib.pyplot as plt\n\n# Display options\npd.set_option(\"display.max_columns\", None)\npd.set_option(\"display.width\", None)\n\n# Path to per‑state files (run notebook from analysis/)\nbase_dir   = Path(\"..\")\nper_state  = base_dir / \"output\" / \"foia\" / \"per_state\"\n\n# ------------------------------------------------------------------\n# 1. Discover available per‑state CSV files\n# ------------------------------------------------------------------\nstate_files = sorted(per_state.glob(\"*_foia_data.csv\"))\n\nif not state_files:\n    raise FileNotFoundError(\n        f\"No per‑state FOIA files found in {per_state}. \"\n        \"Check the folder path.\"\n    )\n\ndef stem_to_state(stem: str) -&gt; str:\n    toks = stem.split(\"_\")\n    if \"foia\" in toks:\n        toks = toks[:toks.index(\"foia\")]\n    return \" \".join(t.title() for t in toks)\n\nstates_available = [stem_to_state(f.stem) for f in state_files]\n\n\nprint(f\"✓ Found {len(state_files)} per‑state files:\")\nfor s in states_available:\n    print(\"  •\", s)\n\ndisplay(Markdown(\n    f\"**States with data loaded:** {', '.join(states_available)}\"\n))\n\n# ------------------------------------------------------------------\n# 2. Initialise empty containers for the loop that follows\n# ------------------------------------------------------------------\nfoia_combined       = pd.DataFrame()   # merged tidy data\nfoia_state_metadata = []               # list of dicts, one per state\n\n✓ Found 7 per‑state files:\n  • California\n  • Florida\n  • Indiana\n  • Maine\n  • Nevada\n  • South Dakota\n  • Texas\n\n\nStates with data loaded: California, Florida, Indiana, Maine, Nevada, South Dakota, Texas"
  },
  {
    "objectID": "analysis/foia_processing.html#helper-functions",
    "href": "analysis/foia_processing.html#helper-functions",
    "title": "FOIA Document OCR Processing",
    "section": "2.4 Helper Functions",
    "text": "2.4 Helper Functions\n\n# Define columns needed for foia_combined\nCOLUMNS_NEEDED = ['state', 'offender_type', 'variable_category', \n                  'variable_detailed', 'value', 'value_type']\n\ndef report_status(df, category):\n    values = df.loc[df[\"variable_category\"] == category, \"value_type\"].unique()\n    if {\"count\", \"percentage\"}.issubset(values):\n        return \"both\"\n    elif \"count\" in values:\n        return \"counts\"\n    elif \"percentage\" in values:\n        return \"percentages\"\n    else:\n        return \"neither\"\n\ndef verify_category_totals(df):\n    \"\"\"\n    For every offender_type × variable_category pair:\n      • find the reported total_profiles,\n      • sum all count rows in that category,\n      • return the difference.\n    \"\"\"\n    # 1  pull total_profiles per offender_type\n    total_map = (df[(df[\"variable_category\"] == \"total\") &\n                    (df[\"variable_detailed\"] == \"total_profiles\")]\n                 .set_index(\"offender_type\")[\"value\"]\n                 .to_dict())\n\n    # 2  sum counts by offender_type and variable_category\n    demo_sum = (df[(df[\"value_type\"] == \"count\") &\n                   (df[\"variable_category\"] != \"total\")]\n                .groupby([\"offender_type\", \"variable_category\"])[\"value\"]\n                .sum()\n                .rename(\"sum_demo_counts\")\n                .reset_index())\n\n    # 3  attach total_profiles and compute difference\n    demo_sum[\"total_profiles\"] = demo_sum[\"offender_type\"].map(total_map)\n    demo_sum[\"difference\"] = demo_sum[\"total_profiles\"] - demo_sum[\"sum_demo_counts\"]\n\n    # tidy columns order\n    return demo_sum[[\"offender_type\",\n                     \"variable_category\",\n                     \"total_profiles\",\n                     \"sum_demo_counts\",\n                     \"difference\"]]\n\ndef calculate_combined_totals(df, state_name):\n    \"\"\"\n    Calculate Combined totals by summing across offender types.\n    Returns a dataframe of Combined rows to add.\n    \"\"\"\n    # Get all counts\n    counts_df = df[(df['value_type'] == 'count')].copy()\n    \n    # Group by variable_category and variable_detailed, sum values\n    combined_sums = (counts_df.groupby(['variable_category', 'variable_detailed'])['value']\n                     .sum()\n                     .reset_index())\n    \n    # Create Combined rows\n    combined_rows = []\n    for _, row in combined_sums.iterrows():\n        combined_rows.append({\n            'state': state_name,\n            'offender_type': 'Combined',\n            'variable_category': row['variable_category'],\n            'variable_detailed': row['variable_detailed'],\n            'value': row['value'],\n            'value_type': 'count',\n            'value_source': 'calculated'\n        })\n    \n    return pd.DataFrame(combined_rows)\n\n\ndef calculate_percentages(df_combined, state_name):\n    \"\"\"\n    Calculate percentages for all demographic categories.\n    Returns a dataframe of percentage rows to add.\n    \"\"\"\n    # Get total profiles for each offender type\n    totals_map = (df_combined[\n        (df_combined['state'] == state_name) &\n        (df_combined['variable_category'] == 'total') &\n        (df_combined['variable_detailed'] == 'total_profiles')\n    ].set_index('offender_type')['value'].to_dict())\n    \n    percentage_rows = []\n    \n    for offender_type, total in totals_map.items():\n        # Get all demographic counts\n        demo_data = df_combined[\n            (df_combined['state'] == state_name) &\n            (df_combined['offender_type'] == offender_type) &\n            (df_combined['variable_category'].isin(['gender', 'race'])) &\n            (df_combined['value_type'] == 'count')\n        ]\n        \n        # Calculate percentage for each\n        for _, row in demo_data.iterrows():\n            percentage = (row['value'] / total) * 100\n            percentage_rows.append({\n                'state': state_name,\n                'offender_type': offender_type,\n                'variable_category': row['variable_category'],\n                'variable_detailed': row['variable_detailed'],\n                'value': round(percentage, 2),\n                'value_type': 'percentage',\n                'value_source': 'calculated'\n            })\n    \n    return pd.DataFrame(percentage_rows)\n\n\ndef calculate_counts_from_percentages(df_combined, state_name):\n    \"\"\"\n    Calculate counts from percentages when only percentages are provided.\n    Returns a dataframe of count rows to add.\n    \"\"\"\n    # Get total profiles for each offender type\n    totals_map = (df_combined[\n        (df_combined['state'] == state_name) &\n        (df_combined['variable_category'] == 'total') &\n        (df_combined['variable_detailed'] == 'total_profiles')\n    ].set_index('offender_type')['value'].to_dict())\n    \n    count_rows = []\n    \n    for offender_type, total in totals_map.items():\n        # Get all demographic percentages\n        demo_data = df_combined[\n            (df_combined['state'] == state_name) &\n            (df_combined['offender_type'] == offender_type) &\n            (df_combined['variable_category'].isin(['gender', 'race'])) &\n            (df_combined['value_type'] == 'percentage')\n        ]\n        \n        # Calculate count for each\n        for _, row in demo_data.iterrows():\n            count = round(total * (row['value'] / 100))\n            count_rows.append({\n                'state': state_name,\n                'offender_type': offender_type,\n                'variable_category': row['variable_category'],\n                'variable_detailed': row['variable_detailed'],\n                'value': int(count),\n                'value_type': 'count',\n                'value_source': 'calculated'\n            })\n    \n    return pd.DataFrame(count_rows)\n\n\ndef standardize_offender_types(df):\n    \"\"\"\n    Standardize offender type terminology across states.\n    \"\"\"\n    replacements = {\n        'Offenders': 'Convicted Offender',\n        'Convicted offenders': 'Convicted Offender',\n        'Arrested offender': 'Arrestee',\n        'All': 'Combined'\n    }\n    \n    df = df.copy()\n    df['offender_type'] = df['offender_type'].replace(replacements)\n    return df\n\n\ndef prepare_state_for_combined(df, state_name):\n    \"\"\"\n    Prepare state data for appending to foia_combined.\n    Selects only needed columns and adds value_source if missing.\n    \"\"\"\n    df_prepared = df[COLUMNS_NEEDED].copy()\n    \n    # Add value_source if not present\n    if 'value_source' not in df_prepared.columns:\n        df_prepared['value_source'] = 'reported'\n    else:\n        df_prepared['value_source'] = df_prepared['value_source'].fillna('reported')\n    \n    return df_prepared\n\n\ndef format_compact(x, p=None):\n    \"\"\"\n    Format numbers in compact notation (k for thousands, M for millions).\n    \n    Args:\n        x: The number to format\n        p: Position parameter (not used, but kept for compatibility with matplotlib)\n    \n    Returns:\n        Formatted string with k/M suffix\n    \"\"\"\n    if x &gt;= 1000000:\n        # For millions, show one decimal place if not a whole million\n        return f'{x/1000000:.1f}M' if x/1000000 != int(x/1000000) else f'{int(x/1000000)}M'\n    elif x &gt;= 1000:\n        # For thousands, always show as integer\n        return f'{int(x/1000)}k'\n    else:\n        # For values under 1000, show as integer\n        return f'{int(x)}'\n\n\ndef create_state_visualizations(df_combined, state_name):\n    \"\"\"\n    Create count and percentage pie charts for a state.\n    \"\"\"\n    state_data = df_combined[df_combined['state'] == state_name]\n    \n    # Determine offender types for this state\n    offender_types = sorted(state_data['offender_type'].unique())\n    n_types = len(offender_types)\n    \n    # Create figure\n    fig, axes = plt.subplots(4, n_types, figsize=(5*n_types, 16))\n    if n_types == 1:\n        axes = axes.reshape(-1, 1)\n    \n    fig.suptitle(f'{state_name} DNA Database Demographics - Counts and Percentages', fontsize=18)\n    \n    # Create pie charts for each metric\n    for i, offender_type in enumerate(offender_types):\n        # Gender counts\n        create_pie_chart(axes[0, i], state_data, offender_type, 'gender', 'count', \n                        f'{offender_type}\\nGender Counts', show_values=True)\n        \n        # Gender percentages\n        create_pie_chart(axes[1, i], state_data, offender_type, 'gender', 'percentage',\n                        f'{offender_type}\\nGender Percentages')\n        \n        # Race counts\n        create_pie_chart(axes[2, i], state_data, offender_type, 'race', 'count',\n                        f'{offender_type}\\nRace Counts', show_values=True)\n        \n        # Race percentages\n        create_pie_chart(axes[3, i], state_data, offender_type, 'race', 'percentage',\n                        f'{offender_type}\\nRace Percentages')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Display count summary\n    print(f\"\\n{state_name} profile counts by offender type:\")\n    count_summary = state_data[\n        (state_data['variable_category'] == 'total') &\n        (state_data['variable_detailed'] == 'total_profiles') &\n        (state_data['value_type'] == 'count')\n    ][['offender_type', 'value']].set_index('offender_type')\n    count_summary.columns = ['Total Profiles']\n    count_summary['Total Profiles'] = count_summary['Total Profiles'].apply(lambda x: f\"{x:,}\")\n    display(count_summary)\n\n\ndef create_pie_chart(ax, data, offender_type, category, value_type, title, show_values=False):\n    \"\"\"\n    Helper to create a single pie chart.\n    \"\"\"\n    chart_data = data[\n        (data['offender_type'] == offender_type) &\n        (data['variable_category'] == category) &\n        (data['value_type'] == value_type)\n    ].sort_values('value', ascending=False)\n    \n    if len(chart_data) &gt; 0:\n        if show_values and value_type == 'count':\n            labels = [f\"{row['variable_detailed']}\\n({row['value']:,.0f})\" \n                     for _, row in chart_data.iterrows()]\n        else:\n            labels = chart_data['variable_detailed']\n        \n        colors = plt.cm.Set3(range(len(chart_data)))\n        wedges, texts, autotexts = ax.pie(chart_data['value'], \n                                          labels=labels,\n                                          autopct='%1.1f%%',\n                                          startangle=90,\n                                          colors=colors)\n        \n        # Adjust text size for readability\n        for text in texts:\n            text.set_fontsize(10 if category == 'race' else 11)\n        for autotext in autotexts:\n            autotext.set_fontsize(10)\n            \n        ax.set_title(title)\n    else:\n        ax.text(0.5, 0.5, 'No data', ha='center', va='center', transform=ax.transAxes)\n        ax.set_title(title)\n\n\ndef verify_percentage_consistency(df_combined, state_name):\n    \"\"\"\n    Verify that reported percentages match calculated percentages (within tolerance).\n    Returns True if consistent, False otherwise.\n    \"\"\"\n    state_data = df_combined[df_combined['state'] == state_name]\n    \n    # Get all offender types that have both counts and percentages\n    offender_types = state_data['offender_type'].unique()\n    \n    consistency_results = []\n    \n    for offender_type in offender_types:\n        offender_data = state_data[state_data['offender_type'] == offender_type]\n        \n        # Check if we have both reported and calculated percentages\n        for category in ['gender', 'race']:\n            reported_pcts = offender_data[\n                (offender_data['variable_category'] == category) &\n                (offender_data['value_type'] == 'percentage') &\n                (offender_data['value_source'] == 'reported')\n            ]\n            \n            calculated_pcts = offender_data[\n                (offender_data['variable_category'] == category) &\n                (offender_data['value_type'] == 'percentage') &\n                (offender_data['value_source'] == 'calculated')\n            ]\n            \n            if len(reported_pcts) &gt; 0 and len(calculated_pcts) &gt; 0:\n                # Compare each demographic value\n                for _, rep_row in reported_pcts.iterrows():\n                    calc_match = calculated_pcts[\n                        calculated_pcts['variable_detailed'] == rep_row['variable_detailed']\n                    ]\n                    if len(calc_match) &gt; 0:\n                        diff = abs(rep_row['value'] - calc_match.iloc[0]['value'])\n                        consistency_results.append({\n                            'offender_type': offender_type,\n                            'category': category,\n                            'variable': rep_row['variable_detailed'],\n                            'reported': rep_row['value'],\n                            'calculated': calc_match.iloc[0]['value'],\n                            'difference': diff,\n                            'consistent': diff &lt; 0.5  # 0.5% tolerance\n                        })\n    \n    if consistency_results:\n        consistency_df = pd.DataFrame(consistency_results)\n        print(f\"\\nPercentage consistency check for {state_name}:\")\n        print(f\"All values consistent: {consistency_df['consistent'].all()}\")\n        \n        if not consistency_df['consistent'].all():\n            print(\"\\nInconsistent values:\")\n            display(consistency_df[~consistency_df['consistent']])\n        \n        return consistency_df['consistent'].all()\n    else:\n        # No comparison possible - state only has one type of data\n        return True\n\n\ndef create_demographic_bar_charts(df_combined, state_name):\n    \"\"\"\n    Create clean bar charts showing demographics by offender type.\n    Uses counts for exact values and includes percentage labels.\n    \"\"\"\n    state_data = df_combined[df_combined['state'] == state_name]\n    \n    # Get offender types and ensure Combined is last\n    offender_types = sorted(state_data[state_data['value_type'] == 'count']['offender_type'].unique())\n    if 'Combined' in offender_types:\n        offender_types.remove('Combined')\n        offender_types.append('Combined')\n    \n    # Create figure with side-by-side subplots for gender and race\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 10))\n    fig.suptitle(f'{state_name} DNA Database Demographics', fontsize=20, fontweight='bold', y=1.01)\n    \n    # Color palettes - using a modern, visually appealing palette\n    gender_colors = {'Male': '#4E79A7', 'Female': '#E15759'}\n    race_colors = {\n        'White': '#4E79A7',\n        'African American': '#F28E2B', \n        'Hispanic': '#E15759',\n        'Asian': '#76B7B2',\n        'Native American': '#59A14F',\n        'Unknown': '#AF7AA1',\n        'Other': '#9C755F'\n    }\n    \n    # Gender subplot - horizontal bars\n    ax1.set_title('Gender Distribution', fontsize=18, pad=15)\n    gender_data = state_data[\n        (state_data['variable_category'] == 'gender') &\n        (state_data['value_type'] == 'count')\n    ]\n    \n    # Prepare data for grouped horizontal bars\n    y_positions = np.arange(len(offender_types))\n    bar_height = 0.35\n    \n    # Process each gender\n    for i, gender in enumerate(['Female', 'Male']):  # Female first to be on top\n        values = []\n        percentages = []\n        \n        for offender_type in offender_types:\n            # Get count\n            count_val = gender_data[\n                (gender_data['offender_type'] == offender_type) &\n                (gender_data['variable_detailed'] == gender)\n            ]['value'].values\n            \n            # Get percentage\n            pct_val = state_data[\n                (state_data['offender_type'] == offender_type) &\n                (state_data['variable_detailed'] == gender) &\n                (state_data['variable_category'] == 'gender') &\n                (state_data['value_type'] == 'percentage')\n            ]['value'].values\n            \n            values.append(count_val[0] if len(count_val) &gt; 0 else 0)\n            percentages.append(pct_val[0] if len(pct_val) &gt; 0 else 0)\n        \n        # Plot horizontal bars\n        positions = y_positions + bar_height * (i - 0.5)\n        bars = ax1.barh(positions, values, bar_height, label=gender, \n                        color=gender_colors.get(gender, '#333333'))\n        \n        # Add count and percentage labels to the right of bars\n        for bar, pct, val in zip(bars, percentages, values):\n            width = bar.get_width()\n            if width &gt; 0:\n                # Put percentage and count to the right of the bar\n                ax1.text(width, bar.get_y() + bar.get_height()/2,\n                        f'  {pct:.1f}% ({int(val):,})', ha='left', va='center', fontsize=14)\n    \n    ax1.set_xlabel('Number of Profiles', fontsize=16)\n    ax1.set_yticks(y_positions)\n    ax1.set_yticklabels(offender_types, fontsize=14)\n    ax1.legend(loc='upper right', fontsize=12)\n    # Remove grid lines and spines for cleaner look\n    ax1.spines['top'].set_visible(False)\n    ax1.spines['right'].set_visible(False)\n    # Add some padding to x-axis for labels\n    ax1.set_xlim(0, max([v for sublist in [gender_data[\n        (gender_data['offender_type'] == ot) &\n        (gender_data['variable_detailed'].isin(['Male', 'Female']))\n    ]['value'].values for ot in offender_types] for v in sublist]) * 1.25)\n    \n    # Format x-axis with compact notation (k for thousands, M for millions)\n    ax1.xaxis.set_major_formatter(plt.FuncFormatter(format_compact))\n    ax1.tick_params(axis='x', labelsize=12)\n    ax1.invert_yaxis()  # Invert y-axis to have first offender type at top\n    \n    # Race subplot - horizontal bars\n    ax2.set_title('Race Distribution', fontsize=18, pad=15)\n    race_data = state_data[\n        (state_data['variable_category'] == 'race') &\n        (state_data['value_type'] == 'count')\n    ]\n    \n    # Debug: Check what races we have and their values\n    if state_name == \"California\":\n        print(f\"\\nDEBUG: Race data for California in visualization:\")\n        ca_race_debug = state_data[\n            (state_data['variable_category'] == 'race') & \n            (state_data['variable_detailed'] == 'Unknown')\n        ][['offender_type', 'value_type', 'value']]\n        print(ca_race_debug)\n    \n    # Get all unique races for this state\n    races = sorted(race_data['variable_detailed'].unique())\n    \n    # Adjust bar height based on number of races\n    race_height = min(0.8 / len(races), 0.15)  # Cap at 0.15 to prevent too thick bars\n    \n    # Create race positions for grouping\n    for i, race in enumerate(races):\n        values = []\n        percentages = []\n        \n        for offender_type in offender_types:\n            # Get count\n            count_val = race_data[\n                (race_data['offender_type'] == offender_type) &\n                (race_data['variable_detailed'] == race)\n            ]['value'].values\n            \n            # Get percentage\n            pct_val = state_data[\n                (state_data['offender_type'] == offender_type) &\n                (state_data['variable_detailed'] == race) &\n                (state_data['variable_category'] == 'race') &\n                (state_data['value_type'] == 'percentage')\n            ]['value'].values\n            \n            values.append(count_val[0] if len(count_val) &gt; 0 else 0)\n            percentages.append(pct_val[0] if len(pct_val) &gt; 0 else 0)\n        \n        # Plot horizontal bars\n        positions = y_positions + race_height * (i - len(races)/2 + 0.5)\n        bars = ax2.barh(positions, values, race_height, label=race,\n                        color=race_colors.get(race, '#333333'))\n        \n        # Add count and percentage labels to the right of bars\n        for bar, pct, val in zip(bars, percentages, values):\n            width = bar.get_width()\n            if width &gt; 0:\n                # Put percentage and count to the right of the bar\n                ax2.text(width, bar.get_y() + bar.get_height()/2,\n                        f'  {pct:.1f}% ({int(val):,})', ha='left', va='center', fontsize=12)\n    \n    ax2.set_xlabel('Number of Profiles', fontsize=16)\n    ax2.set_yticks(y_positions)\n    ax2.set_yticklabels(offender_types, fontsize=14)\n    ax2.legend(loc='upper right', ncol=1, fontsize=11)\n    # Remove grid lines and spines for cleaner look\n    ax2.spines['top'].set_visible(False)\n    ax2.spines['right'].set_visible(False)\n    # Add some padding to x-axis for labels\n    ax2.set_xlim(0, max([v for sublist in [race_data[\n        race_data['offender_type'] == ot\n    ]['value'].values for ot in offender_types] for v in sublist if v &gt; 0]) * 1.25)\n    \n    # Format x-axis with compact notation (k for thousands, M for millions)\n    ax2.xaxis.set_major_formatter(plt.FuncFormatter(format_compact))\n    ax2.tick_params(axis='x', labelsize=12)\n    ax2.invert_yaxis()  # Invert y-axis to have first offender type at top\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Also display a summary table with totals\n    print(f\"\\n{state_name} profile totals by offender type:\")\n    totals_data = state_data[\n        (state_data['variable_category'] == 'total') &\n        (state_data['variable_detailed'] == 'total_profiles') &\n        (state_data['value_type'] == 'count')\n    ][['offender_type', 'value']].copy()\n    totals_data['value'] = totals_data['value'].apply(lambda x: f\"{int(x):,}\")\n    totals_data.columns = ['Offender Type', 'Total Profiles']\n    display(totals_data.set_index('Offender Type'))"
  },
  {
    "objectID": "analysis/foia_processing.html#final-processing-summary",
    "href": "analysis/foia_processing.html#final-processing-summary",
    "title": "FOIA Document OCR Processing",
    "section": "4.1 Final Processing Summary",
    "text": "4.1 Final Processing Summary\nAll seven states have been successfully processed and combined into a unified dataset.\n\n# Display final metadata summary\nprint(\"State Processing Metadata:\")\nprint(\"=\" * 80)\nmetadata_df = pd.DataFrame(foia_state_metadata)\ndisplay(metadata_df)\n\n# Summary statistics\nprint(f\"\\n✓ Total rows in foia_combined: {len(foia_combined)}\")\nprint(f\"✓ States processed: {sorted(foia_combined['state'].unique())}\")\nprint(f\"✓ Offender types: {sorted(foia_combined['offender_type'].unique())}\")\nprint(f\"✓ Variable categories: {sorted(foia_combined['variable_category'].unique())}\")\n\n# Value source breakdown\nprint(\"\\nValue source breakdown:\")\nsource_counts = foia_combined['value_source'].value_counts()\nfor source, count in source_counts.items():\n    pct = (count / len(foia_combined)) * 100\n    print(f\"  • {source}: {count} rows ({pct:.1f}%)\")\n\n# Show sample of final combined data\nprint(\"\\nSample of final combined dataset:\")\ndisplay(foia_combined.sample(10, random_state=42))\n\nState Processing Metadata:\n================================================================================\n\n\n\n\n\n\n\n\n\nstate\nrace_report_values\ngender_report_values\nnotes\n\n\n\n\n0\nCalifornia\ncounts\ncounts\nNaN\n\n\n1\nFlorida\nboth\nboth\nNaN\n\n\n2\nIndiana\npercentages\npercentages\nProvides percentages for demographics, counts ...\n\n\n3\nMaine\nboth\nboth\nNaN\n\n\n4\nNevada\nboth\nboth\nUses 'flags' terminology instead of 'profiles'\n\n\n5\nSouth Dakota\nboth\nboth\nIncludes gender_race cross-tabulation for inte...\n\n\n6\nTexas\ncounts\ncounts\nNaN\n\n\n\n\n\n\n\n\n✓ Total rows in foia_combined: 214\n✓ States processed: ['California', 'Florida', 'Indiana', 'Maine', 'Nevada', 'South Dakota', 'Texas']\n✓ Offender types: ['Arrestee', 'Combined', 'Convicted Offender']\n✓ Variable categories: ['gender', 'gender_race', 'race', 'total']\n\nValue source breakdown:\n  • reported: 143 rows (66.8%)\n  • calculated: 71 rows (33.2%)\n\nSample of final combined dataset:\n\n\n\n\n\n\n\n\n\nstate\noffender_type\nvariable_category\nvariable_detailed\nvalue\nvalue_type\nvalue_source\n\n\n\n\n9\nCalifornia\nConvicted Offender\nrace\nCaucasian\n588555.0000\ncount\nreported\n\n\n197\nTexas\nConvicted Offender\nrace\nHispanic\n32.6800\npercentage\ncalculated\n\n\n66\nFlorida\nCombined\nrace\nHispanic\n2.4200\npercentage\nreported\n\n\n191\nTexas\nCombined\nrace\nOther\n2531.0000\ncount\ncalculated\n\n\n117\nNevada\nCombined\ngender\nUnknown\n0.0209\npercentage\nreported\n\n\n111\nNevada\nConvicted Offender\ntotal\ntotal_profiles\n46.2150\npercentage\nreported\n\n\n15\nCalifornia\nArrestee\nrace\nAsian\n11191.0000\ncount\nreported\n\n\n86\nIndiana\nCombined\nrace\nHispanic\n12030.0000\ncount\ncalculated\n\n\n75\nIndiana\nCombined\ngender\nFemale\n20.0000\npercentage\nreported\n\n\n144\nSouth Dakota\nCombined\nrace\nWhite/Caucasian\n66.7500\npercentage\nreported"
  },
  {
    "objectID": "analysis/foia_processing.html#saving-processed-data",
    "href": "analysis/foia_processing.html#saving-processed-data",
    "title": "FOIA Document OCR Processing",
    "section": "4.2 Saving Processed Data",
    "text": "4.2 Saving Processed Data\n\n# Define output paths\noutput_dir = base_dir / \"output\" / \"foia\"\noutput_dir.mkdir(parents=True, exist_ok=True)\n\n# Save the combined dataset\nfoia_output_path = output_dir / \"foia_data_clean.csv\"\nfoia_combined.to_csv(foia_output_path, index=False)\nprint(f\"✓ Saved combined FOIA data to: {foia_output_path}\")\n\n# Save the metadata\nmetadata_output_path = output_dir / \"foia_state_metadata.csv\"\nmetadata_df.to_csv(metadata_output_path, index=False)\nprint(f\"✓ Saved state metadata to: {metadata_output_path}\")\n\n# Create final frozen version\nfrozen_dir = base_dir / \"data\" / \"v1.0\"\nfrozen_dir.mkdir(parents=True, exist_ok=True)\n\nfrozen_path = frozen_dir / \"foia_state_race_v1.0.csv\"\nfoia_combined.to_csv(frozen_path, index=False)\nprint(f\"✓ Created frozen version at: {frozen_path}\")\n\nprint(\"\\n✅ All processing complete!\")\n\n✓ Saved combined FOIA data to: ../output/foia/foia_data_clean.csv\n✓ Saved state metadata to: ../output/foia/foia_state_metadata.csv\n✓ Created frozen version at: ../data/v1.0/foia_state_race_v1.0.csv\n\n✅ All processing complete!"
  },
  {
    "objectID": "analysis/foia_processing.html#key-findings-and-notes",
    "href": "analysis/foia_processing.html#key-findings-and-notes",
    "title": "FOIA Document OCR Processing",
    "section": "4.3 Key Findings and Notes",
    "text": "4.3 Key Findings and Notes\n\nData Completeness: All states provided total profile counts and demographic breakdowns, though reporting formats varied significantly.\nCalculated Values:\n\nCalifornia and Texas required Combined totals and percentage calculations\nIndiana required count calculations from percentages\nAll calculated values are clearly marked with value_source = \"calculated\"\n\nTerminology Standardization:\n\nNevada: “All” → “Combined”, “Arrested offender” → “Arrestee”\nTexas: “Offenders” → “Convicted Offender”\n\nUnique Features:\n\nSouth Dakota provided intersectional gender×race data unavailable from other states\nNevada uses “flags” terminology instead of “profiles”\nCalifornia acknowledged missing race data, requiring “Unknown” category calculation\n\nData Quality: After processing, all states show internally consistent data with demographic counts summing to reported totals and percentages summing to approximately 100% (accounting for rounding).\n\nThis standardized dataset enables direct state-to-state comparisons of DNA database demographics while maintaining full transparency about data sources and calculations.\nThis processing pipeline has successfully transformed heterogeneous FOIA responses from seven states into a standardized, analysis-ready dataset. By maintaining clear attribution of reported versus calculated values and documenting all processing decisions, this work supports reproducible research on racial disparities in DNA databases.\nThe unified dataset (foia_state_race_v1.0.csv) and accompanying metadata provide researchers with accessible, transparent data to examine critical questions about equity and representation in forensic DNA databases. As noted in the introduction, making this data more accessible represents an important contribution to research ethics and transparency in criminal justice studies."
  },
  {
    "objectID": "analysis/foia_processing.html#conclusion",
    "href": "analysis/foia_processing.html#conclusion",
    "title": "FOIA Document OCR Processing",
    "section": "Conclusion",
    "text": "Conclusion\nThis processing pipeline has successfully transformed heterogeneous FOIA responses from seven states into a standardized, analysis-ready dataset. By maintaining clear attribution of reported versus calculated values and documenting all processing decisions, this work supports reproducible research on racial disparities in DNA databases.\nThe unified dataset (foia_state_race_2016.csv) and accompanying metadata provide researchers with accessible, transparent data to examine critical questions about equity and representation in forensic DNA databases. As noted in the introduction, making this data more accessible represents an important contribution to research ethics and transparency in criminal justice studies.\n\nIndiana\n\nin_path = per_state / \"indiana_foia_data.csv\"\nindiana = pd.read_csv(in_path)\n\ndisplay(indiana)\n\n\n\n\n\n\n\n\nstate\noffender_type\nvariable_category\nvariable_detailed\nvalue\nvalue_type\ndate\n\n\n\n\n0\nIndiana\nConvicted Offender\ntotal\ntotal_profiles\n279654\ncount\n2018-07-24\n\n\n1\nIndiana\nArrestee\ntotal\ntotal_profiles\n21087\ncount\n2018-07-24\n\n\n2\nIndiana\nCombined\ngender\nFemale\n20\npercentage\n2018-07-24\n\n\n3\nIndiana\nCombined\ngender\nMale\n80\npercentage\n2018-07-24\n\n\n4\nIndiana\nCombined\nrace\nCaucasian\n70\npercentage\n2018-07-24\n\n\n5\nIndiana\nCombined\nrace\nBlack\n26\npercentage\n2018-07-24\n\n\n6\nIndiana\nCombined\nrace\nHispanic\n4\npercentage\n2018-07-24\n\n\n7\nIndiana\nCombined\nrace\nOther\n&lt;1\npercentage\n2018-07-24\n\n\n\n\n\n\n\nIndiana presents a unique reporting pattern: total counts are provided, but demographic breakdowns are given only as percentages. Additionally, the value column contains String data that requires conversion.\n\n# First, let's examine the data to see what values we have\nprint(\"Unique values in Indiana data before conversion:\")\nprint(indiana['value'].unique())\n\n# Convert string values to numeric, handling \"&lt;1\" as 0.5\nindiana['value'] = indiana['value'].apply(lambda x: 0.5 if x == '&lt;1' else pd.to_numeric(x))\n\nprint(\"\\n✓ Converted Indiana values from String to numeric\")\nprint(f\"Value data type after conversion: {indiana['value'].dtype}\")\nprint(f\"Values after conversion: {indiana['value'].unique()}\")\n\nUnique values in Indiana data before conversion:\n['279654' '21087' '20' '80' '70' '26' '4' '&lt;1']\n\n✓ Converted Indiana values from String to numeric\nValue data type after conversion: float64\nValues after conversion: [2.79654e+05 2.10870e+04 2.00000e+01 8.00000e+01 7.00000e+01 2.60000e+01\n 4.00000e+00 5.00000e-01]\n\n\nRecording Indiana’s reporting structure:\n\nfoia_state_metadata.append({\n    \"state\": \"Indiana\",\n    \"race_report_values\": report_status(indiana, \"race\"),\n    \"gender_report_values\": report_status(indiana, \"gender\"),\n    \"notes\": \"Provides percentages for demographics, counts for totals only\"\n})\n\nprint(\"✓ Added metadata row:\", foia_state_metadata[-1])\n\n✓ Added metadata row: {'state': 'Indiana', 'race_report_values': 'percentages', 'gender_report_values': 'percentages', 'notes': 'Provides percentages for demographics, counts for totals only'}\n\n\n\nUnderstanding Indiana’s unique reporting structure\nIndiana provides an unusual reporting pattern: - Total profile counts for Convicted Offender and Arrestee separately - Demographic percentages only for Combined totals (not broken down by offender type)\nFirst, let’s calculate the Combined total:\n\n# Calculate Combined total from the separate offender type totals\nconv_total = indiana[(indiana['offender_type'] == 'Convicted Offender') & \n                     (indiana['variable_detailed'] == 'total_profiles')]['value'].values[0]\narr_total = indiana[(indiana['offender_type'] == 'Arrestee') & \n                    (indiana['variable_detailed'] == 'total_profiles')]['value'].values[0]\ncombined_total = conv_total + arr_total\n\nprint(f\"Convicted Offender total: {conv_total:,.0f}\")\nprint(f\"Arrestee total: {arr_total:,.0f}\")\nprint(f\"Combined total: {combined_total:,.0f}\")\n\n# Add Combined total to Indiana data\ncombined_total_row = pd.DataFrame([{\n    'state': 'Indiana',\n    'offender_type': 'Combined',\n    'variable_category': 'total',\n    'variable_detailed': 'total_profiles',\n    'value': combined_total,\n    'value_type': 'count',\n    'value_source': 'calculated'\n}])\n\nindiana = pd.concat([indiana, combined_total_row], ignore_index=True)\nprint(\"\\n✓ Added Combined total to Indiana data\")\n\nConvicted Offender total: 279,654\nArrestee total: 21,087\nCombined total: 300,741\n\n✓ Added Combined total to Indiana data\n\n\n\n\nCalculating demographic counts from percentages\nNow we can calculate the actual counts from the Combined percentages:\n\n# First, prepare and append the reported data\nin_for_combined = prepare_state_for_combined(indiana, \"Indiana\")\n\n# Update value_source for the calculated Combined total\nin_for_combined.loc[\n    (in_for_combined['offender_type'] == 'Combined') & \n    (in_for_combined['variable_detailed'] == 'total_profiles'),\n    'value_source'\n] = 'calculated'\n\nfoia_combined = pd.concat([foia_combined, in_for_combined], ignore_index=True)\n\nprint(f\"✓ Appended {len(in_for_combined)} Indiana reported rows to foia_combined\")\n\n# Now calculate counts from the percentages\nin_counts = calculate_counts_from_percentages(foia_combined, \"Indiana\")\n\n# Append calculated counts\nif len(in_counts) &gt; 0:\n    foia_combined = pd.concat([foia_combined, in_counts], ignore_index=True)\n    print(f\"✓ Calculated and added {len(in_counts)} count rows for Indiana\")\n    \n    # Display sample of calculated counts\n    print(\"\\nSample of calculated Indiana counts:\")\n    display(in_counts[in_counts['variable_category'] == 'race'].head())\nelse:\n    print(\"✗ No counts could be calculated (this shouldn't happen)\")\n\nprint(f\"✓ Total rows in foia_combined: {len(foia_combined)}\")\n\n✓ Appended 9 Indiana reported rows to foia_combined\n✓ Calculated and added 6 count rows for Indiana\n\nSample of calculated Indiana counts:\n\n\n\n\n\n\n\n\n\nstate\noffender_type\nvariable_category\nvariable_detailed\nvalue\nvalue_type\nvalue_source\n\n\n\n\n2\nIndiana\nCombined\nrace\nCaucasian\n210519\ncount\ncalculated\n\n\n3\nIndiana\nCombined\nrace\nBlack\n78193\ncount\ncalculated\n\n\n4\nIndiana\nCombined\nrace\nHispanic\n12030\ncount\ncalculated\n\n\n5\nIndiana\nCombined\nrace\nOther\n1504\ncount\ncalculated\n\n\n\n\n\n\n\n✓ Total rows in foia_combined: 195\n\n\n\n\nVerifying calculated totals\n\n# Create temporary dataframe with Indiana's complete data for verification\nin_complete = pd.concat([indiana, in_counts], ignore_index=True)\nin_category_qc = verify_category_totals(in_complete)\ndisplay(in_category_qc)\n\n# Check if percentages sum to ~100%\nprint(\"\\nVerifying Indiana percentages sum to ~100%:\")\nin_pct_check = (indiana[indiana['value_type'] == 'percentage']\n                .groupby(['offender_type', 'variable_category'])['value']\n                .sum()\n                .round(2))\ndisplay(in_pct_check)\n\n\n\n\n\n\n\n\noffender_type\nvariable_category\ntotal_profiles\nsum_demo_counts\ndifference\n\n\n\n\n0\nCombined\ngender\n300741.0\n300741.0\n0.0\n\n\n1\nCombined\nrace\n300741.0\n302246.0\n-1505.0\n\n\n\n\n\n\n\n\nVerifying Indiana percentages sum to ~100%:\n\n\noffender_type  variable_category\nCombined       gender               100.0\n               race                 100.5\nName: value, dtype: float64\n\n\n\n\nVisualizing Indiana demographics\n\ncreate_state_visualizations(foia_combined, \"Indiana\")\n\n\n\n\n\n\n\n\n\nIndiana profile counts by offender type:\n\n\n\n\n\n\n\n\n\nTotal Profiles\n\n\noffender_type\n\n\n\n\n\nConvicted Offender\n279,654.0\n\n\nArrestee\n21,087.0\n\n\nCombined\n300,741.0\n\n\n\n\n\n\n\n\n\nSummary of Indiana processing\nIndiana data processing complete. The state’s unique reporting required: - Conversion of String values to numeric format - Calculation of demographic counts from reported percentages - All demographic values now available in both count and percentage formats\nMixed value_source attribution: percentages are “reported”, counts are “calculated”.\n\n\n\nMaine\n\nme_path = per_state / \"maine_foia_data.csv\"\nmaine = pd.read_csv(me_path)\n\ndisplay(maine)\n\n\n\n\n\n\n\n\nstate\noffender_type\nvariable_category\nvariable_detailed\nvalue\nvalue_type\ndate\n\n\n\n\n0\nMaine\nCombined\ntotal\ntotal_profiles\n33711.0\ncount\n2018-07-10\n\n\n1\nMaine\nCombined\ngender\nMale\n27694.0\ncount\n2018-07-10\n\n\n2\nMaine\nCombined\ngender\nMale\n82.7\npercentage\n2018-07-10\n\n\n3\nMaine\nCombined\ngender\nFemale\n5734.0\ncount\n2018-07-10\n\n\n4\nMaine\nCombined\ngender\nFemale\n17.0\npercentage\n2018-07-10\n\n\n5\nMaine\nCombined\ngender\nUnknown\n83.0\ncount\n2018-07-10\n\n\n6\nMaine\nCombined\ngender\nUnknown\n0.2\npercentage\n2018-07-10\n\n\n7\nMaine\nCombined\nrace\nWhite\n31298.0\ncount\n2018-07-10\n\n\n8\nMaine\nCombined\nrace\nWhite\n92.8\npercentage\n2018-07-10\n\n\n9\nMaine\nCombined\nrace\nBlack\n1299.0\ncount\n2018-07-10\n\n\n10\nMaine\nCombined\nrace\nBlack\n3.9\npercentage\n2018-07-10\n\n\n11\nMaine\nCombined\nrace\nUnknown\n470.0\ncount\n2018-07-10\n\n\n12\nMaine\nCombined\nrace\nUnknown\n1.4\npercentage\n2018-07-10\n\n\n13\nMaine\nCombined\nrace\nNative American\n345.0\ncount\n2018-07-10\n\n\n14\nMaine\nCombined\nrace\nNative American\n1.0\npercentage\n2018-07-10\n\n\n15\nMaine\nCombined\nrace\nHispanic\n171.0\ncount\n2018-07-10\n\n\n16\nMaine\nCombined\nrace\nHispanic\n0.5\npercentage\n2018-07-10\n\n\n17\nMaine\nCombined\nrace\nAsian\n128.0\ncount\n2018-07-10\n\n\n18\nMaine\nCombined\nrace\nAsian\n0.4\npercentage\n2018-07-10\n\n\n\n\n\n\n\nMaine provides comprehensive reporting with both counts and percentages for all demographic categories, including Combined totals.\nRecording Maine’s reporting structure:\n\nfoia_state_metadata.append({\n    \"state\": \"Maine\",\n    \"race_report_values\": report_status(maine, \"race\"),\n    \"gender_report_values\": report_status(maine, \"gender\")\n})\n\nprint(\"✓ Added metadata row:\", foia_state_metadata[-1])\n\n✓ Added metadata row: {'state': 'Maine', 'race_report_values': 'both', 'gender_report_values': 'both'}\n\n\n\nVerifying demographic consistency\n\nme_category_qc = verify_category_totals(maine)\ndisplay(me_category_qc)\n\n# Also verify that reported percentages sum to ~100%\nprint(\"\\nVerifying Maine percentages sum to ~100%:\")\nme_pct_check = (maine[maine['value_type'] == 'percentage']\n                .groupby(['variable_category'])['value']\n                .sum()\n                .round(2))\ndisplay(me_pct_check)\n\n\n\n\n\n\n\n\noffender_type\nvariable_category\ntotal_profiles\nsum_demo_counts\ndifference\n\n\n\n\n0\nCombined\ngender\n33711.0\n33511.0\n200.0\n\n\n1\nCombined\nrace\n33711.0\n33711.0\n0.0\n\n\n\n\n\n\n\n\nVerifying Maine percentages sum to ~100%:\n\n\nvariable_category\ngender     99.9\nrace      100.0\nName: value, dtype: float64\n\n\nMaine’s data shows excellent internal consistency with both counts and percentages properly aligned.\n\n\nPreparing Maine data for the combined dataset\n\n# Prepare Maine data with only needed columns\nme_for_combined = prepare_state_for_combined(maine, \"Maine\")\n\n# Display structure verification\nprint(\"Maine data structure verification:\")\nprint(f\"Unique offender types: {me_for_combined['offender_type'].unique()}\")\nprint(f\"Unique variable categories: {me_for_combined['variable_category'].unique()}\")\nprint(f\"Value types present: {me_for_combined['value_type'].unique()}\")\n\n# Append to the combined dataframe\nfoia_combined = pd.concat([foia_combined, me_for_combined], ignore_index=True)\n\nprint(f\"\\n✓ Appended {len(me_for_combined)} Maine rows to foia_combined\")\nprint(f\"✓ Total rows in foia_combined: {len(foia_combined)}\")\n\nMaine data structure verification:\nUnique offender types: ['Combined']\nUnique variable categories: ['total' 'gender' 'race']\nValue types present: ['count' 'percentage']\n\n✓ Appended 19 Maine rows to foia_combined\n✓ Total rows in foia_combined: 214\n\n\n\n\nVisualizing Maine demographics\n\ncreate_state_visualizations(foia_combined, \"Maine\")\n\n\n\n\n\n\n\n\n\nMaine profile counts by offender type:\n\n\n\n\n\n\n\n\n\nTotal Profiles\n\n\noffender_type\n\n\n\n\n\nCombined\n33,711.0\n\n\n\n\n\n\n\n\n\nSummary of Maine processing\nMaine data processing complete. The state provided: - Complete reporting with both counts and percentages - Combined totals already calculated - Internally consistent data with percentages summing to 100%\nAll values maintain value_source = \"reported\" as no calculations were necessary."
  },
  {
    "objectID": "analysis/foia_processing.html#california",
    "href": "analysis/foia_processing.html#california",
    "title": "FOIA Document OCR Processing",
    "section": "3.1 California",
    "text": "3.1 California\n\nca_path = per_state / \"california_foia_data.csv\"\nca      = pd.read_csv(ca_path)\n\ndisplay(ca)\n\n\n\n\n\n\n\n\nstate\noffender_type\nvariable_category\nvariable_detailed\nvalue\nvalue_type\ndate\n\n\n\n\n0\nCalifornia\nConvicted Offender\ntotal\ntotal_profiles\n2019899\ncount\n2018-07-05\n\n\n1\nCalifornia\nArrestee\ntotal\ntotal_profiles\n751822\ncount\n2018-07-05\n\n\n2\nCalifornia\nConvicted Offender\ngender\nFemale\n309827\ncount\n2018-07-05\n\n\n3\nCalifornia\nConvicted Offender\ngender\nMale\n1603222\ncount\n2018-07-05\n\n\n4\nCalifornia\nConvicted Offender\ngender\nUnknown\n106850\ncount\n2018-07-05\n\n\n5\nCalifornia\nArrestee\ngender\nFemale\n208225\ncount\n2018-07-05\n\n\n6\nCalifornia\nArrestee\ngender\nMale\n524231\ncount\n2018-07-05\n\n\n7\nCalifornia\nArrestee\ngender\nUnknown\n19366\ncount\n2018-07-05\n\n\n8\nCalifornia\nConvicted Offender\nrace\nAfrican American\n368952\ncount\n2018-07-05\n\n\n9\nCalifornia\nConvicted Offender\nrace\nCaucasian\n588555\ncount\n2018-07-05\n\n\n10\nCalifornia\nConvicted Offender\nrace\nHispanic\n652121\ncount\n2018-07-05\n\n\n11\nCalifornia\nConvicted Offender\nrace\nAsian\n16384\ncount\n2018-07-05\n\n\n12\nCalifornia\nArrestee\nrace\nAfrican American\n104741\ncount\n2018-07-05\n\n\n13\nCalifornia\nArrestee\nrace\nCaucasian\n231313\ncount\n2018-07-05\n\n\n14\nCalifornia\nArrestee\nrace\nHispanic\n308450\ncount\n2018-07-05\n\n\n15\nCalifornia\nArrestee\nrace\nAsian\n11191\ncount\n2018-07-05\n\n\n\n\n\n\n\nWe begin by loading California’s file. California supplies counts only for gender and race plus a separate total for each offender type; no percentages are reported.\nRecording California’s reporting structure:\n\nfoia_state_metadata.append({\n    \"state\": \"California\",\n    \"race_report_values\":   report_status(ca, \"race\"),\n    \"gender_report_values\": report_status(ca, \"gender\")\n})\n\nprint(\"✓ Added metadata row:\", foia_state_metadata[-1])\n\n✓ Added metadata row: {'state': 'California', 'race_report_values': 'counts', 'gender_report_values': 'counts'}\n\n\n\n3.1.1 Verifying that demographic counts match reported totals\nUsing the verify_category_totals helper function:\n\nca_category_qc = verify_category_totals(ca)\nca_category_qc\n\n\n\n\n\n\n\n\noffender_type\nvariable_category\ntotal_profiles\nsum_demo_counts\ndifference\n\n\n\n\n0\nArrestee\ngender\n751822\n751822\n0\n\n\n1\nArrestee\nrace\n751822\n655695\n96127\n\n\n2\nConvicted Offender\ngender\n2019899\n2019899\n0\n\n\n3\nConvicted Offender\nrace\n2019899\n1626012\n393887\n\n\n\n\n\n\n\n\n\n3.1.2 Interpreting the race shortfall\n\n“Racial classification is not considered a required field on the collection card; thus, an unknown number of offenders may have no racial classification listed.” — California DOJ FOIA letter, July 10 2018 (raw/foia_pdfs/FOIA_RacialComp_California.pdf)\n\nThe 393 887 Convicted‑Offender profiles and 96 127 Arrestee profiles that do not appear in any of the four reported race categories must therefore belong to an unreported “Unknown/Other” bucket. To keep row totals consistent and enable percentage calculations later, we add one new race row per offender type:\n\nvariable_category = \"race\"\nvariable_detailed = \"Unknown\"\nvalue_type = \"count\"\nvalue_source = \"calculated\"\n\n\n# ------------------------------------------------------------------\n# 1.  Remove any prior Unknown‑race rows \n# ------------------------------------------------------------------\nca = ca[~(\n    (ca[\"variable_category\"] == \"race\") &\n    (ca[\"variable_detailed\"] == \"Unknown\")\n)].copy()\n\n# ------------------------------------------------------------------\n# 2.  Add one Unknown‑race row per offender type\n# ------------------------------------------------------------------\nunknown_rows = pd.DataFrame([\n    {\n        \"state\": \"California\",\n        \"offender_type\": \"Convicted Offender\",\n        \"variable_category\": \"race\",\n        \"variable_detailed\": \"Unknown\",\n        \"value\": 393887,           \n        \"value_type\": \"count\",\n        \"value_source\": \"calculated\",\n    },\n    {\n        \"state\": \"California\",\n        \"offender_type\": \"Arrestee\",\n        \"variable_category\": \"race\",\n        \"variable_detailed\": \"Unknown\",\n        \"value\": 96127,\n        \"value_type\": \"count\",\n        \"value_source\": \"calculated\",\n    },\n])\n\nca = pd.concat([ca, unknown_rows], ignore_index=True)\n\n# ------------------------------------------------------------------\n# 3.  Confirm race and gender now reconcile to totals\n# ------------------------------------------------------------------\nprint(\"Category‑level check after adding Unknown rows:\")\ndisplay(verify_category_totals(ca))\n\nCategory‑level check after adding Unknown rows:\n\n\n\n\n\n\n\n\n\noffender_type\nvariable_category\ntotal_profiles\nsum_demo_counts\ndifference\n\n\n\n\n0\nArrestee\ngender\n751822\n751822\n0\n\n\n1\nArrestee\nrace\n751822\n751822\n0\n\n\n2\nConvicted Offender\ngender\n2019899\n2019899\n0\n\n\n3\nConvicted Offender\nrace\n2019899\n2019899\n0\n\n\n\n\n\n\n\n\n\n3.1.3 Creating Combined totals\nCalifornia reported data separately for Convicted Offenders and Arrestees. Combined totals are calculated by summing across both offender types for all categories and demographic values.\n\n# Calculate Combined totals using helper function\ncombined_df = calculate_combined_totals(ca, \"California\")\n\n# Add Combined rows to ca\nca = pd.concat([ca, combined_df], ignore_index=True)\n\nprint(\"✓ Created Combined totals for California\")\nprint(f\"  Combined total profiles: {combined_df[combined_df['variable_detailed'] == 'total_profiles']['value'].values[0]:,}\")\n\n# Display Combined gender counts\nprint(\"\\nCombined gender counts:\")\ndisplay(combined_df[combined_df['variable_category'] == 'gender'].sort_values('variable_detailed'))\n\n# Display Combined race counts  \nprint(\"\\nCombined race counts:\")\ndisplay(combined_df[combined_df['variable_category'] == 'race'].sort_values('variable_detailed'))\n\n# Verify Combined calculations\nprint(\"\\n✓ Verification of Combined calculations:\")\n\n# Get counts dataframe for verification\ncounts_df = ca[(ca['value_type'] == 'count') & (ca['offender_type'] != 'Combined')].copy()\n\nconv_total = counts_df[(counts_df['offender_type'] == 'Convicted Offender') & \n                       (counts_df['variable_detailed'] == 'total_profiles')]['value'].values[0]\narr_total = counts_df[(counts_df['offender_type'] == 'Arrestee') & \n                      (counts_df['variable_detailed'] == 'total_profiles')]['value'].values[0]\ncomb_total = combined_df[combined_df['variable_detailed'] == 'total_profiles']['value'].values[0]\n\nprint(f\"  Convicted Offender total: {conv_total:,}\")\nprint(f\"  Arrestee total: {arr_total:,}\")\nprint(f\"  Sum: {conv_total + arr_total:,}\")\nprint(f\"  Combined total: {comb_total:,}\")\nprint(f\"  Match: {conv_total + arr_total == comb_total}\")\n\n✓ Created Combined totals for California\n  Combined total profiles: 2,771,721\n\nCombined gender counts:\n\n\n\n\n\n\n\n\n\nstate\noffender_type\nvariable_category\nvariable_detailed\nvalue\nvalue_type\nvalue_source\n\n\n\n\n0\nCalifornia\nCombined\ngender\nFemale\n518052\ncount\ncalculated\n\n\n1\nCalifornia\nCombined\ngender\nMale\n2127453\ncount\ncalculated\n\n\n2\nCalifornia\nCombined\ngender\nUnknown\n126216\ncount\ncalculated\n\n\n\n\n\n\n\n\nCombined race counts:\n\n\n\n\n\n\n\n\n\nstate\noffender_type\nvariable_category\nvariable_detailed\nvalue\nvalue_type\nvalue_source\n\n\n\n\n3\nCalifornia\nCombined\nrace\nAfrican American\n473693\ncount\ncalculated\n\n\n4\nCalifornia\nCombined\nrace\nAsian\n27575\ncount\ncalculated\n\n\n5\nCalifornia\nCombined\nrace\nCaucasian\n819868\ncount\ncalculated\n\n\n6\nCalifornia\nCombined\nrace\nHispanic\n960571\ncount\ncalculated\n\n\n7\nCalifornia\nCombined\nrace\nUnknown\n490014\ncount\ncalculated\n\n\n\n\n\n\n\n\n✓ Verification of Combined calculations:\n  Convicted Offender total: 2,019,899\n  Arrestee total: 751,822\n  Sum: 2,771,721\n  Combined total: 2,771,721\n  Match: True\n\n\n\n\n3.1.4 Preparing California data for the combined dataset\nThe California data requires selection of specific columns for inclusion in the combined dataset.\n\n# ------------------------------------------------------------------\n# Create California data for foia_combined with only needed columns\n# ------------------------------------------------------------------\n# Select existing California data with the columns we want\nca_for_combined = ca[['state', 'offender_type', 'variable_category', \n                      'variable_detailed', 'value', 'value_type']].copy()\n\n# Add value_source column\nca_for_combined['value_source'] = 'reported'\n\n# Update value_source for calculated rows (Unknown race and Combined totals)\nca_for_combined.loc[\n    ((ca_for_combined['variable_category'] == 'race') & \n     (ca_for_combined['variable_detailed'] == 'Unknown')) |\n    (ca_for_combined['offender_type'] == 'Combined'), \n    'value_source'\n] = 'calculated'\n\n# Display sample of data structure\nprint(\"California data structure verification:\")\nprint(f\"Unique offender types: {ca_for_combined['offender_type'].unique()}\")\nprint(f\"Unique variable categories: {ca_for_combined['variable_category'].unique()}\")\n\n# Append to the combined dataframe\nfoia_combined = pd.concat([foia_combined, ca_for_combined], ignore_index=True)\n\nprint(f\"\\n✓ Appended {len(ca_for_combined)} California rows to foia_combined\")\nprint(f\"✓ Total rows in foia_combined: {len(foia_combined)}\")\n\nCalifornia data structure verification:\nUnique offender types: ['Convicted Offender' 'Arrestee' 'Combined']\nUnique variable categories: ['total' 'gender' 'race']\n\n✓ Appended 27 California rows to foia_combined\n✓ Total rows in foia_combined: 27\n\n\n\n\n3.1.5 Calculating percentages\nCalifornia provided only counts. For comparative analysis, percentages are calculated for each demographic category as the proportion of each demographic value relative to the total profiles for that offender type.\n\n# Calculate percentages using helper function\npercentages_df = calculate_percentages(foia_combined, \"California\")\n\n# Add percentages to foia_combined\nfoia_combined = pd.concat([foia_combined, percentages_df], ignore_index=True)\n\nprint(f\"✓ Calculated and added {len(percentages_df)} percentage rows for California\")\nprint(f\"✓ Total rows in foia_combined: {len(foia_combined)}\")\n\n# Display the actual percentage values\nprint(\"\\nGender percentages by offender type:\")\ngender_pct = percentages_df[percentages_df['variable_category'] == 'gender'].pivot_table(\n    index='variable_detailed', columns='offender_type', values='value'\n)\ndisplay(gender_pct)\n\nprint(\"\\nRace percentages by offender type:\")\nrace_pct = percentages_df[percentages_df['variable_category'] == 'race'].pivot_table(\n    index='variable_detailed', columns='offender_type', values='value'\n)\ndisplay(race_pct)\n\n# Verify percentages sum to approximately 100% for each category\nprint(\"\\nVerification of percentage totals by category:\")\nverification = (percentages_df.groupby(['offender_type', 'variable_category'])['value']\n                .sum()\n                .round(2)\n                .reset_index()\n                .rename(columns={'value': 'sum_of_percentages'}))\ndisplay(verification)\n\n# Note about rounding\nprint(\"\\nNote: Percentage sums may not equal exactly 100% due to rounding to 2 decimal places.\")\n\n✓ Calculated and added 24 percentage rows for California\n✓ Total rows in foia_combined: 51\n\nGender percentages by offender type:\n\n\n\n\n\n\n\n\noffender_type\nArrestee\nCombined\nConvicted Offender\n\n\nvariable_detailed\n\n\n\n\n\n\n\nFemale\n27.70\n18.69\n15.34\n\n\nMale\n69.73\n76.76\n79.37\n\n\nUnknown\n2.58\n4.55\n5.29\n\n\n\n\n\n\n\n\nRace percentages by offender type:\n\n\n\n\n\n\n\n\noffender_type\nArrestee\nCombined\nConvicted Offender\n\n\nvariable_detailed\n\n\n\n\n\n\n\nAfrican American\n13.93\n17.09\n18.27\n\n\nAsian\n1.49\n0.99\n0.81\n\n\nCaucasian\n30.77\n29.58\n29.14\n\n\nHispanic\n41.03\n34.66\n32.28\n\n\nUnknown\n12.79\n17.68\n19.50\n\n\n\n\n\n\n\n\nVerification of percentage totals by category:\n\n\n\n\n\n\n\n\n\noffender_type\nvariable_category\nsum_of_percentages\n\n\n\n\n0\nArrestee\ngender\n100.01\n\n\n1\nArrestee\nrace\n100.01\n\n\n2\nCombined\ngender\n100.00\n\n\n3\nCombined\nrace\n100.00\n\n\n4\nConvicted Offender\ngender\n100.00\n\n\n5\nConvicted Offender\nrace\n100.00\n\n\n\n\n\n\n\n\nNote: Percentage sums may not equal exactly 100% due to rounding to 2 decimal places.\n\n\n\n\n3.1.6 Visualizing California demographics\n\n# Debug: Check what percentages are in the data for Unknown race\nca_data = foia_combined[foia_combined['state'] == 'California']\nunknown_pcts = ca_data[\n    (ca_data['variable_detailed'] == 'Unknown') & \n    (ca_data['value_type'] == 'percentage')\n][['offender_type', 'value']]\nprint(\"Unknown race percentages in California data:\")\ndisplay(unknown_pcts)\n\n# Create cleaner bar chart visualization\ncreate_demographic_bar_charts(foia_combined, \"California\")\n\nUnknown race percentages in California data:\n\n\n\n\n\n\n\n\n\noffender_type\nvalue\n\n\n\n\n29\nConvicted Offender\n5.29\n\n\n34\nConvicted Offender\n19.50\n\n\n37\nArrestee\n2.58\n\n\n42\nArrestee\n12.79\n\n\n45\nCombined\n4.55\n\n\n50\nCombined\n17.68\n\n\n\n\n\n\n\n\nDEBUG: Race data for California in visualization:\n         offender_type  value_type      value\n16  Convicted Offender       count  393887.00\n17            Arrestee       count   96127.00\n25            Combined       count  490014.00\n34  Convicted Offender  percentage      19.50\n42            Arrestee  percentage      12.79\n50            Combined  percentage      17.68\n\n\n\n\n\n\n\n\n\n\nCalifornia profile totals by offender type:\n\n\n\n\n\n\n\n\n\nTotal Profiles\n\n\nOffender Type\n\n\n\n\n\nConvicted Offender\n2,019,899\n\n\nArrestee\n751,822\n\n\nCombined\n2,771,721\n\n\n\n\n\n\n\n\n\n3.1.7 Summary of California processing\nCalifornia data processing complete. The dataset now includes:\n\nReported counts for Convicted Offenders and Arrestees\nCalculated Unknown race counts to reconcile reported totals\nCalculated Combined totals for gender and race categories\nCalculated percentages for all offender types and demographic categories\n\nAll values include appropriate value_source indicators distinguishing reported from calculated values."
  },
  {
    "objectID": "analysis/foia_processing.html#florida",
    "href": "analysis/foia_processing.html#florida",
    "title": "FOIA Document OCR Processing",
    "section": "3.2 Florida",
    "text": "3.2 Florida\n\nfl_path = per_state / \"florida_foia_data.csv\"\nfl = pd.read_csv(fl_path)\n\ndisplay(fl)\n\n\n\n\n\n\n\n\nstate\noffender_type\nvariable_category\nvariable_detailed\nvalue\nvalue_type\ndate\n\n\n\n\n0\nFlorida\nCombined\ntotal\ntotal_profiles\n1175391.00\ncount\nNaN\n\n\n1\nFlorida\nCombined\ntotal\ntotal_profiles\n100.00\npercentage\nNaN\n\n\n2\nFlorida\nCombined\ngender\nFemale\n260885.00\ncount\nNaN\n\n\n3\nFlorida\nCombined\ngender\nFemale\n22.20\npercentage\nNaN\n\n\n4\nFlorida\nCombined\ngender\nMale\n901126.00\ncount\nNaN\n\n\n5\nFlorida\nCombined\ngender\nMale\n76.67\npercentage\nNaN\n\n\n6\nFlorida\nCombined\ngender\nUnknown\n13380.00\ncount\nNaN\n\n\n7\nFlorida\nCombined\ngender\nUnknown\n1.14\npercentage\nNaN\n\n\n8\nFlorida\nCombined\nrace\nAfrican American\n413733.00\ncount\nNaN\n\n\n9\nFlorida\nCombined\nrace\nAfrican American\n35.20\npercentage\nNaN\n\n\n10\nFlorida\nCombined\nrace\nAsian\n2659.00\ncount\nNaN\n\n\n11\nFlorida\nCombined\nrace\nAsian\n0.23\npercentage\nNaN\n\n\n12\nFlorida\nCombined\nrace\nCaucasian\n721485.00\ncount\nNaN\n\n\n13\nFlorida\nCombined\nrace\nCaucasian\n61.38\npercentage\nNaN\n\n\n14\nFlorida\nCombined\nrace\nHispanic\n28452.00\ncount\nNaN\n\n\n15\nFlorida\nCombined\nrace\nHispanic\n2.42\npercentage\nNaN\n\n\n16\nFlorida\nCombined\nrace\nNative American\n667.00\ncount\nNaN\n\n\n17\nFlorida\nCombined\nrace\nNative American\n0.06\npercentage\nNaN\n\n\n18\nFlorida\nCombined\nrace\nOther\n1176.00\ncount\nNaN\n\n\n19\nFlorida\nCombined\nrace\nOther\n0.10\npercentage\nNaN\n\n\n20\nFlorida\nCombined\nrace\nUnknown\n7219.00\ncount\nNaN\n\n\n21\nFlorida\nCombined\nrace\nUnknown\n0.61\npercentage\nNaN\n\n\n\n\n\n\n\nFlorida provides both counts and percentages for gender and race categories. The state already includes Combined totals, simplifying our processing.\nRecording Florida’s reporting structure in our metadata:\n\nfoia_state_metadata.append({\n    \"state\": \"Florida\",\n    \"race_report_values\": report_status(fl, \"race\"),\n    \"gender_report_values\": report_status(fl, \"gender\")\n})\n\nprint(\"✓ Added metadata row:\", foia_state_metadata[-1])\n\n✓ Added metadata row: {'state': 'Florida', 'race_report_values': 'both', 'gender_report_values': 'both'}\n\n\n\n3.2.1 Verifying demographic consistency\n\nfl_category_qc = verify_category_totals(fl)\ndisplay(fl_category_qc)\n\n\n\n\n\n\n\n\noffender_type\nvariable_category\ntotal_profiles\nsum_demo_counts\ndifference\n\n\n\n\n0\nCombined\ngender\n100.0\n1175391.0\n-1175291.0\n\n\n1\nCombined\nrace\n100.0\n1175391.0\n-1175291.0\n\n\n\n\n\n\n\nFlorida’s demographic counts sum correctly to the reported total. The state data appears internally consistent.\n\n\n3.2.2 Preparing Florida data for the combined dataset\n\n# Prepare Florida data with only needed columns\nfl_for_combined = prepare_state_for_combined(fl, \"Florida\")\n\n# Display structure verification\nprint(\"Florida data structure verification:\")\nprint(f\"Unique offender types: {fl_for_combined['offender_type'].unique()}\")\nprint(f\"Unique variable categories: {fl_for_combined['variable_category'].unique()}\")\nprint(f\"Value types present: {fl_for_combined['value_type'].unique()}\")\n\n# Append to the combined dataframe\nfoia_combined = pd.concat([foia_combined, fl_for_combined], ignore_index=True)\n\nprint(f\"\\n✓ Appended {len(fl_for_combined)} Florida rows to foia_combined\")\nprint(f\"✓ Total rows in foia_combined: {len(foia_combined)}\")\n\nFlorida data structure verification:\nUnique offender types: ['Combined']\nUnique variable categories: ['total' 'gender' 'race']\nValue types present: ['count' 'percentage']\n\n✓ Appended 22 Florida rows to foia_combined\n✓ Total rows in foia_combined: 73\n\n\n\n\n3.2.3 Visualizing Florida demographics\n\ncreate_demographic_bar_charts(foia_combined, \"Florida\")\n\n\n\n\n\n\n\n\n\nFlorida profile totals by offender type:\n\n\n\n\n\n\n\n\n\nTotal Profiles\n\n\nOffender Type\n\n\n\n\n\nCombined\n1,175,391\n\n\n\n\n\n\n\n\n\n3.2.4 Summary of Florida processing\nFlorida data processing complete. The state provided: - Both counts and percentages for all demographic categories - Combined totals already calculated - Internally consistent data requiring no adjustments\nAll values maintain value_source = \"reported\" as no calculations were necessary."
  },
  {
    "objectID": "analysis/foia_processing.html#nevada",
    "href": "analysis/foia_processing.html#nevada",
    "title": "FOIA Document OCR Processing",
    "section": "3.5 Nevada",
    "text": "3.5 Nevada\n\nnv_path = per_state / \"nevada_foia_data.csv\"\nnevada = pd.read_csv(nv_path)\n\ndisplay(nevada)\n\n\n\n\n\n\n\n\nstate\noffender_type\nvariable_category\nvariable_detailed\nvalue\nvalue_type\ndate\n\n\n\n\n0\nNevada\nAll\ntotal\ntotal_flags\n344097.0000\ncount\n2018\n\n\n1\nNevada\nArrested offender\ntotal\ntotal_profiles\n185074.0000\ncount\n2018\n\n\n2\nNevada\nArrested offender\ntotal\ntotal_profiles\n53.7850\npercentage\n2018\n\n\n3\nNevada\nConvicted offenders\ntotal\ntotal_profiles\n159023.0000\ncount\n2018\n\n\n4\nNevada\nConvicted offenders\ntotal\ntotal_profiles\n46.2150\npercentage\n2018\n\n\n5\nNevada\nCombined\ngender\nFemale\n63287.0000\ncount\n2018\n\n\n6\nNevada\nCombined\ngender\nFemale\n18.3920\npercentage\n2018\n\n\n7\nNevada\nCombined\ngender\nMale\n280738.0000\ncount\n2018\n\n\n8\nNevada\nCombined\ngender\nMale\n81.5870\npercentage\n2018\n\n\n9\nNevada\nCombined\ngender\nUnknown\n72.0000\ncount\n2018\n\n\n10\nNevada\nCombined\ngender\nUnknown\n0.0209\npercentage\n2018\n\n\n11\nNevada\nCombined\nrace\nWhite\n238723.0000\ncount\n2018\n\n\n12\nNevada\nCombined\nrace\nWhite\n69.3770\npercentage\n2018\n\n\n13\nNevada\nCombined\nrace\nUnknown\n3491.0000\ncount\n2018\n\n\n14\nNevada\nCombined\nrace\nUnknown\n1.0150\npercentage\n2018\n\n\n15\nNevada\nCombined\nrace\nAmerican Indian\n5710.0000\ncount\n2018\n\n\n16\nNevada\nCombined\nrace\nAmerican Indian\n1.6590\npercentage\n2018\n\n\n17\nNevada\nCombined\nrace\nBlack\n88174.0000\ncount\n2018\n\n\n18\nNevada\nCombined\nrace\nBlack\n25.6250\npercentage\n2018\n\n\n19\nNevada\nCombined\nrace\nAsian\n7999.0000\ncount\n2018\n\n\n20\nNevada\nCombined\nrace\nAsian\n2.3460\npercentage\n2018\n\n\n\n\n\n\n\nNevada uses non-standard terminology that requires standardization: - “All” instead of “Combined” - “Arrested offender” instead of “Arrestee” - “Convicted offenders” instead of “Convicted Offender” - Uses “flags” terminology instead of “profiles”\n\n# Standardize offender types\nnevada = standardize_offender_types(nevada)\n\nprint(\"✓ Standardized Nevada offender type terminology\")\nprint(f\"Offender types after standardization: {sorted(nevada['offender_type'].unique())}\")\n\n✓ Standardized Nevada offender type terminology\nOffender types after standardization: ['Arrestee', 'Combined', 'Convicted Offender']\n\n\nRecording Nevada’s reporting structure:\n\nfoia_state_metadata.append({\n    \"state\": \"Nevada\",\n    \"race_report_values\": report_status(nevada, \"race\"),\n    \"gender_report_values\": report_status(nevada, \"gender\"),\n    \"notes\": \"Uses 'flags' terminology instead of 'profiles'\"\n})\n\nprint(\"✓ Added metadata row:\", foia_state_metadata[-1])\n\n✓ Added metadata row: {'state': 'Nevada', 'race_report_values': 'both', 'gender_report_values': 'both', 'notes': \"Uses 'flags' terminology instead of 'profiles'\"}\n\n\n\n3.5.1 Verifying demographic consistency\n\nnv_category_qc = verify_category_totals(nevada)\ndisplay(nv_category_qc)\n\n# Verify percentages sum to ~100%\nprint(\"\\nVerifying Nevada percentages sum to ~100%:\")\nnv_pct_check = (nevada[nevada['value_type'] == 'percentage']\n                .groupby(['offender_type', 'variable_category'])['value']\n                .sum()\n                .round(2))\ndisplay(nv_pct_check)\n\n\n\n\n\n\n\n\noffender_type\nvariable_category\ntotal_profiles\nsum_demo_counts\ndifference\n\n\n\n\n0\nCombined\ngender\nNaN\n344097.0\nNaN\n\n\n1\nCombined\nrace\nNaN\n344097.0\nNaN\n\n\n\n\n\n\n\n\nVerifying Nevada percentages sum to ~100%:\n\n\noffender_type       variable_category\nArrestee            total                 53.78\nCombined            gender               100.00\n                    race                 100.02\nConvicted Offender  total                 46.22\nName: value, dtype: float64\n\n\nNevada’s data shows good internal consistency after terminology standardization.\n\n\n3.5.2 Preparing Nevada data for the combined dataset\n\n# Prepare Nevada data with only needed columns\nnv_for_combined = prepare_state_for_combined(nevada, \"Nevada\")\n\n# Display structure verification\nprint(\"Nevada data structure verification:\")\nprint(f\"Unique offender types: {nv_for_combined['offender_type'].unique()}\")\nprint(f\"Unique variable categories: {nv_for_combined['variable_category'].unique()}\")\nprint(f\"Value types present: {nv_for_combined['value_type'].unique()}\")\n\n# Append to the combined dataframe\nfoia_combined = pd.concat([foia_combined, nv_for_combined], ignore_index=True)\n\nprint(f\"\\n✓ Appended {len(nv_for_combined)} Nevada rows to foia_combined\")\nprint(f\"✓ Total rows in foia_combined: {len(foia_combined)}\")\n\nNevada data structure verification:\nUnique offender types: ['Combined' 'Arrestee' 'Convicted Offender']\nUnique variable categories: ['total' 'gender' 'race']\nValue types present: ['count' 'percentage']\n\n✓ Appended 21 Nevada rows to foia_combined\n✓ Total rows in foia_combined: 128\n\n\n\n\n3.5.3 Visualizing Nevada demographics\n\ncreate_demographic_bar_charts(foia_combined, \"Nevada\")\n\n\n\n\n\n\n\n\n\nNevada profile totals by offender type:\n\n\n\n\n\n\n\n\n\nTotal Profiles\n\n\nOffender Type\n\n\n\n\n\nArrestee\n185,074\n\n\nConvicted Offender\n159,023\n\n\n\n\n\n\n\n\n\n3.5.4 Summary of Nevada processing\nNevada data processing complete. The state required: - Standardization of offender type terminology for consistency - Recognition of “flags” as equivalent to “profiles” in other states - Both counts and percentages were provided for all categories\nAll values maintain value_source = \"reported\" as only terminology changes were necessary."
  },
  {
    "objectID": "analysis/foia_processing.html#south-dakota",
    "href": "analysis/foia_processing.html#south-dakota",
    "title": "FOIA Document OCR Processing",
    "section": "3.6 South Dakota",
    "text": "3.6 South Dakota\n\nsd_path = per_state / \"south_dakota_foia_data.csv\"\nsouth_dakota = pd.read_csv(sd_path)\n\ndisplay(south_dakota.head(20))\nprint(f\"\\n... showing first 20 of {len(south_dakota)} rows\")\n\n\n\n\n\n\n\n\nstate\noffender_type\nvariable_category\nvariable_detailed\nvalue\nvalue_type\ndate\n\n\n\n\n0\nSouth Dakota\nCombined\ntotal\ntotal_profiles\n67753.00\ncount\n2018-07-18\n\n\n1\nSouth Dakota\nCombined\ngender\nMale\n51197.00\ncount\n2018-07-18\n\n\n2\nSouth Dakota\nCombined\ngender\nMale\n75.56\npercentage\n2018-07-18\n\n\n3\nSouth Dakota\nCombined\ngender\nFemale\n16556.00\ncount\n2018-07-18\n\n\n4\nSouth Dakota\nCombined\ngender\nFemale\n24.44\npercentage\n2018-07-18\n\n\n5\nSouth Dakota\nCombined\nrace\nAsian\n5.00\ncount\n2018-07-18\n\n\n6\nSouth Dakota\nCombined\nrace\nAsian\n0.08\npercentage\n2018-07-18\n\n\n7\nSouth Dakota\nCombined\nrace\nBlack\n4041.00\ncount\n2018-07-18\n\n\n8\nSouth Dakota\nCombined\nrace\nBlack\n5.96\npercentage\n2018-07-18\n\n\n9\nSouth Dakota\nCombined\nrace\nHispanic\n2949.00\ncount\n2018-07-18\n\n\n10\nSouth Dakota\nCombined\nrace\nHispanic\n4.35\npercentage\n2018-07-18\n\n\n11\nSouth Dakota\nCombined\nrace\nNative American\n14593.00\ncount\n2018-07-18\n\n\n12\nSouth Dakota\nCombined\nrace\nNative American\n21.54\npercentage\n2018-07-18\n\n\n13\nSouth Dakota\nCombined\nrace\nOther/Unknown\n891.00\ncount\n2018-07-18\n\n\n14\nSouth Dakota\nCombined\nrace\nOther/Unknown\n1.32\npercentage\n2018-07-18\n\n\n15\nSouth Dakota\nCombined\nrace\nWhite/Caucasian\n45223.00\ncount\n2018-07-18\n\n\n16\nSouth Dakota\nCombined\nrace\nWhite/Caucasian\n66.75\npercentage\n2018-07-18\n\n\n17\nSouth Dakota\nCombined\ngender_race\nMale_Asian\n5.00\ncount\n2018-07-18\n\n\n18\nSouth Dakota\nCombined\ngender_race\nMale_Asian\n0.10\npercentage\n2018-07-18\n\n\n19\nSouth Dakota\nCombined\ngender_race\nFemale_Asian\n5.00\ncount\n2018-07-18\n\n\n\n\n\n\n\n\n... showing first 20 of 41 rows\n\n\nSouth Dakota provides the most comprehensive reporting with 41 rows, including: - Standard gender and race breakdowns - Unique gender_race cross-tabulation showing intersectional demographics - Both counts and percentages for all categories\nRecording South Dakota’s reporting structure:\n\nfoia_state_metadata.append({\n    \"state\": \"South Dakota\",\n    \"race_report_values\": report_status(south_dakota, \"race\"),\n    \"gender_report_values\": report_status(south_dakota, \"gender\"),\n    \"notes\": \"Includes gender_race cross-tabulation for intersectional analysis\"\n})\n\nprint(\"✓ Added metadata row:\", foia_state_metadata[-1])\n\n✓ Added metadata row: {'state': 'South Dakota', 'race_report_values': 'both', 'gender_report_values': 'both', 'notes': 'Includes gender_race cross-tabulation for intersectional analysis'}\n\n\n\n3.6.1 Examining the intersectional data\n\n# Display the unique gender_race combinations\ngender_race_data = south_dakota[south_dakota['variable_category'] == 'gender_race']\nprint(\"South Dakota's intersectional gender × race categories:\")\nprint(f\"Total gender_race rows: {len(gender_race_data)}\")\nprint(\"\\nUnique combinations:\")\nfor combo in sorted(gender_race_data['variable_detailed'].unique()):\n    print(f\"  • {combo}\")\n\nSouth Dakota's intersectional gender × race categories:\nTotal gender_race rows: 24\n\nUnique combinations:\n  • Female_Asian\n  • Female_Black\n  • Female_Hispanic\n  • Female_Native American\n  • Female_Other/Unknown\n  • Female_White/Caucasian\n  • Male_Asian\n  • Male_Black\n  • Male_Hispanic\n  • Male_Native American\n  • Male_Other/Unknown\n  • Male_White/Caucasian\n\n\n\n\n3.6.2 Verifying demographic consistency\n\nsd_category_qc = verify_category_totals(south_dakota)\ndisplay(sd_category_qc)\n\n# Verify percentages sum to ~100%\nprint(\"\\nVerifying South Dakota percentages sum to ~100%:\")\nsd_pct_check = (south_dakota[south_dakota['value_type'] == 'percentage']\n                .groupby(['variable_category'])['value']\n                .sum()\n                .round(2))\ndisplay(sd_pct_check)\n\n\n\n\n\n\n\n\noffender_type\nvariable_category\ntotal_profiles\nsum_demo_counts\ndifference\n\n\n\n\n0\nCombined\ngender\n67753.0\n67753.0\n0.0\n\n\n1\nCombined\ngender_race\n67753.0\n67707.0\n46.0\n\n\n2\nCombined\nrace\n67753.0\n67702.0\n51.0\n\n\n\n\n\n\n\n\nVerifying South Dakota percentages sum to ~100%:\n\n\nvariable_category\ngender         100.00\ngender_race    199.99\nrace           100.00\nName: value, dtype: float64\n\n\nSouth Dakota’s data demonstrates excellent internal consistency across all categories, including the intersectional data.\n\n\n3.6.3 Preparing South Dakota data for the combined dataset\n\n# Prepare South Dakota data with only needed columns\nsd_for_combined = prepare_state_for_combined(south_dakota, \"South Dakota\")\n\n# Display structure verification\nprint(\"South Dakota data structure verification:\")\nprint(f\"Unique offender types: {sd_for_combined['offender_type'].unique()}\")\nprint(f\"Unique variable categories: {sd_for_combined['variable_category'].unique()}\")\nprint(f\"Value types present: {sd_for_combined['value_type'].unique()}\")\n\n# Show breakdown by category\nprint(\"\\nRows by variable category:\")\nprint(sd_for_combined['variable_category'].value_counts())\n\n# Append to the combined dataframe\nfoia_combined = pd.concat([foia_combined, sd_for_combined], ignore_index=True)\n\nprint(f\"\\n✓ Appended {len(sd_for_combined)} South Dakota rows to foia_combined\")\nprint(f\"✓ Total rows in foia_combined: {len(foia_combined)}\")\n\nSouth Dakota data structure verification:\nUnique offender types: ['Combined']\nUnique variable categories: ['total' 'gender' 'race' 'gender_race']\nValue types present: ['count' 'percentage']\n\nRows by variable category:\nvariable_category\ngender_race    24\nrace           12\ngender          4\ntotal           1\nName: count, dtype: int64\n\n✓ Appended 41 South Dakota rows to foia_combined\n✓ Total rows in foia_combined: 169\n\n\n\n\n3.6.4 Visualizing South Dakota demographics\n\ncreate_demographic_bar_charts(foia_combined, \"South Dakota\")\n\n\n\n\n\n\n\n\n\nSouth Dakota profile totals by offender type:\n\n\n\n\n\n\n\n\n\nTotal Profiles\n\n\nOffender Type\n\n\n\n\n\nCombined\n67,753\n\n\n\n\n\n\n\n\n\n3.6.5 Summary of South Dakota processing\nSouth Dakota data processing complete. The state provided: - Most comprehensive reporting with 41 rows of data - Standard gender and race categories plus unique intersectional gender×race data - Complete counts and percentages for all categories - Exemplary data quality with perfect internal consistency\nAll values maintain value_source = \"reported\". The intersectional data provides unique insights unavailable from other states."
  },
  {
    "objectID": "analysis/foia_processing.html#texas",
    "href": "analysis/foia_processing.html#texas",
    "title": "FOIA Document OCR Processing",
    "section": "3.7 Texas",
    "text": "3.7 Texas\n\ntx_path = per_state / \"texas_foia_data.csv\"\ntexas = pd.read_csv(tx_path)\n\ndisplay(texas)\n\n\n\n\n\n\n\n\nstate\noffender_type\nvariable_category\nvariable_detailed\nvalue\nvalue_type\ndate\n\n\n\n\n0\nTexas\nOffenders\ntotal\ntotal_profiles\n845322\ncount\n2018-06-19\n\n\n1\nTexas\nArrestee\ntotal\ntotal_profiles\n73631\ncount\n2018-06-19\n\n\n2\nTexas\nOffenders\ngender\nFemale\n121434\ncount\n2018-06-19\n\n\n3\nTexas\nArrestee\ngender\nFemale\n18721\ncount\n2018-06-19\n\n\n4\nTexas\nOffenders\nrace\nAsian\n3361\ncount\n2018-06-19\n\n\n5\nTexas\nOffenders\nrace\nAfrican American\n254366\ncount\n2018-06-19\n\n\n6\nTexas\nOffenders\nrace\nCaucasian\n309010\ncount\n2018-06-19\n\n\n7\nTexas\nOffenders\nrace\nHispanic\n276245\ncount\n2018-06-19\n\n\n8\nTexas\nOffenders\nrace\nNative American\n138\ncount\n2018-06-19\n\n\n9\nTexas\nOffenders\nrace\nOther\n2173\ncount\n2018-06-19\n\n\n10\nTexas\nArrestee\nrace\nAsian\n497\ncount\n2018-06-19\n\n\n11\nTexas\nArrestee\nrace\nAfrican American\n12903\ncount\n2018-06-19\n\n\n12\nTexas\nArrestee\nrace\nCaucasian\n33486\ncount\n2018-06-19\n\n\n13\nTexas\nArrestee\nrace\nHispanic\n24202\ncount\n2018-06-19\n\n\n14\nTexas\nArrestee\nrace\nNative American\n24\ncount\n2018-06-19\n\n\n15\nTexas\nArrestee\nrace\nOther\n358\ncount\n2018-06-19\n\n\n\n\n\n\n\nTexas provides counts only and uses “Offenders” instead of “Convicted Offender”. Like California, Texas will need Combined totals and percentages calculated.\n\n# Standardize offender types\ntexas = standardize_offender_types(texas)\n\nprint(\"✓ Standardized Texas offender type terminology\")\nprint(f\"Offender types after standardization: {sorted(texas['offender_type'].unique())}\")\n\n✓ Standardized Texas offender type terminology\nOffender types after standardization: ['Arrestee', 'Convicted Offender']\n\n\nRecording Texas’s reporting structure:\n\nfoia_state_metadata.append({\n    \"state\": \"Texas\",\n    \"race_report_values\": report_status(texas, \"race\"),\n    \"gender_report_values\": report_status(texas, \"gender\")\n})\n\nprint(\"✓ Added metadata row:\", foia_state_metadata[-1])\n\n✓ Added metadata row: {'state': 'Texas', 'race_report_values': 'counts', 'gender_report_values': 'counts'}\n\n\n\n3.7.1 Verifying demographic consistency\n\ntx_category_qc = verify_category_totals(texas)\ndisplay(tx_category_qc)\n\n\n\n\n\n\n\n\noffender_type\nvariable_category\ntotal_profiles\nsum_demo_counts\ndifference\n\n\n\n\n0\nArrestee\ngender\n73631\n18721\n54910\n\n\n1\nArrestee\nrace\n73631\n71470\n2161\n\n\n2\nConvicted Offender\ngender\n845322\n121434\n723888\n\n\n3\nConvicted Offender\nrace\n845322\n845293\n29\n\n\n\n\n\n\n\nTexas shows perfect internal consistency with all demographic counts matching reported totals.\n\n\n3.7.2 Creating Combined totals\n\n# Calculate Combined totals using helper function\ntx_combined = calculate_combined_totals(texas, \"Texas\")\n\n# Add Combined rows to texas dataframe\ntexas = pd.concat([texas, tx_combined], ignore_index=True)\n\nprint(\"✓ Created Combined totals for Texas\")\nprint(f\"  Combined total profiles: {tx_combined[tx_combined['variable_detailed'] == 'total_profiles']['value'].values[0]:,}\")\n\n# Display Combined counts\nprint(\"\\nCombined gender counts:\")\ndisplay(tx_combined[tx_combined['variable_category'] == 'gender'].sort_values('variable_detailed'))\n\nprint(\"\\nCombined race counts:\")\ndisplay(tx_combined[tx_combined['variable_category'] == 'race'].sort_values('variable_detailed'))\n\n# Verify Combined calculations\nprint(\"\\n✓ Verification of Combined calculations:\")\n\n# Get original counts for verification\nconv_total = texas[(texas['offender_type'] == 'Convicted Offender') & \n                   (texas['variable_detailed'] == 'total_profiles') &\n                   (texas['value_type'] == 'count')]['value'].values[0]\narr_total = texas[(texas['offender_type'] == 'Arrestee') & \n                  (texas['variable_detailed'] == 'total_profiles') &\n                  (texas['value_type'] == 'count')]['value'].values[0]\ncomb_total = tx_combined[tx_combined['variable_detailed'] == 'total_profiles']['value'].values[0]\n\nprint(f\"  Convicted Offender total: {conv_total:,}\")\nprint(f\"  Arrestee total: {arr_total:,}\")\nprint(f\"  Sum: {conv_total + arr_total:,}\")\nprint(f\"  Combined total: {comb_total:,}\")\nprint(f\"  Match: {conv_total + arr_total == comb_total}\")\n\n✓ Created Combined totals for Texas\n  Combined total profiles: 918,953\n\nCombined gender counts:\n\n\n\n\n\n\n\n\n\nstate\noffender_type\nvariable_category\nvariable_detailed\nvalue\nvalue_type\nvalue_source\n\n\n\n\n0\nTexas\nCombined\ngender\nFemale\n140155\ncount\ncalculated\n\n\n\n\n\n\n\n\nCombined race counts:\n\n\n\n\n\n\n\n\n\nstate\noffender_type\nvariable_category\nvariable_detailed\nvalue\nvalue_type\nvalue_source\n\n\n\n\n1\nTexas\nCombined\nrace\nAfrican American\n267269\ncount\ncalculated\n\n\n2\nTexas\nCombined\nrace\nAsian\n3858\ncount\ncalculated\n\n\n3\nTexas\nCombined\nrace\nCaucasian\n342496\ncount\ncalculated\n\n\n4\nTexas\nCombined\nrace\nHispanic\n300447\ncount\ncalculated\n\n\n5\nTexas\nCombined\nrace\nNative American\n162\ncount\ncalculated\n\n\n6\nTexas\nCombined\nrace\nOther\n2531\ncount\ncalculated\n\n\n\n\n\n\n\n\n✓ Verification of Combined calculations:\n  Convicted Offender total: 845,322\n  Arrestee total: 73,631\n  Sum: 918,953\n  Combined total: 918,953\n  Match: True\n\n\n\n\n3.7.3 Preparing Texas data for the combined dataset\n\n# Prepare Texas data with only needed columns\ntx_for_combined = prepare_state_for_combined(texas, \"Texas\")\n\n# Update value_source for Combined rows\ntx_for_combined.loc[\n    tx_for_combined['offender_type'] == 'Combined', \n    'value_source'\n] = 'calculated'\n\n# Display structure verification\nprint(\"Texas data structure verification:\")\nprint(f\"Unique offender types: {tx_for_combined['offender_type'].unique()}\")\nprint(f\"Unique variable categories: {tx_for_combined['variable_category'].unique()}\")\n\n# Append to the combined dataframe\nfoia_combined = pd.concat([foia_combined, tx_for_combined], ignore_index=True)\n\nprint(f\"\\n✓ Appended {len(tx_for_combined)} Texas rows to foia_combined\")\nprint(f\"✓ Total rows in foia_combined: {len(foia_combined)}\")\n\nTexas data structure verification:\nUnique offender types: ['Convicted Offender' 'Arrestee' 'Combined']\nUnique variable categories: ['total' 'gender' 'race']\n\n✓ Appended 24 Texas rows to foia_combined\n✓ Total rows in foia_combined: 193\n\n\n\n\n3.7.4 Calculating percentages\n\n# Calculate percentages using helper function\ntx_percentages = calculate_percentages(foia_combined, \"Texas\")\n\n# Add percentages to foia_combined\nfoia_combined = pd.concat([foia_combined, tx_percentages], ignore_index=True)\n\nprint(f\"✓ Calculated and added {len(tx_percentages)} percentage rows for Texas\")\nprint(f\"✓ Total rows in foia_combined: {len(foia_combined)}\")\n\n# Display sample of calculated percentages\nprint(\"\\nGender percentages by offender type:\")\ngender_pct = tx_percentages[tx_percentages['variable_category'] == 'gender'].pivot_table(\n    index='variable_detailed', columns='offender_type', values='value'\n)\ndisplay(gender_pct)\n\n# Verify percentages sum to ~100%\nprint(\"\\nVerification of percentage totals by category:\")\nverification = (tx_percentages.groupby(['offender_type', 'variable_category'])['value']\n                .sum()\n                .round(2)\n                .reset_index()\n                .rename(columns={'value': 'sum_of_percentages'}))\ndisplay(verification)\n\n✓ Calculated and added 21 percentage rows for Texas\n✓ Total rows in foia_combined: 214\n\nGender percentages by offender type:\n\n\n\n\n\n\n\n\noffender_type\nArrestee\nCombined\nConvicted Offender\n\n\nvariable_detailed\n\n\n\n\n\n\n\nFemale\n25.43\n15.25\n14.37\n\n\n\n\n\n\n\n\nVerification of percentage totals by category:\n\n\n\n\n\n\n\n\n\noffender_type\nvariable_category\nsum_of_percentages\n\n\n\n\n0\nArrestee\ngender\n25.43\n\n\n1\nArrestee\nrace\n97.06\n\n\n2\nCombined\ngender\n15.25\n\n\n3\nCombined\nrace\n99.76\n\n\n4\nConvicted Offender\ngender\n14.37\n\n\n5\nConvicted Offender\nrace\n100.01\n\n\n\n\n\n\n\n\n\n3.7.5 Visualizing Texas demographics\n\ncreate_state_visualizations(foia_combined, \"Texas\")\n\n\n\n\n\n\n\n\n\nTexas profile counts by offender type:\n\n\n\n\n\n\n\n\n\nTotal Profiles\n\n\noffender_type\n\n\n\n\n\nConvicted Offender\n845,322.0\n\n\nArrestee\n73,631.0\n\n\nCombined\n918,953.0\n\n\n\n\n\n\n\n\n\n3.7.6 Summary of Texas processing\nTexas data processing complete. The state required: - Standardization of “Offenders” to “Convicted Offender” - Calculation of Combined totals across offender types - Calculation of percentages from provided counts\nMixed value_source attribution: original counts are “reported”, Combined totals and all percentages are “calculated”."
  },
  {
    "objectID": "analysis/foia_processing.html#indiana",
    "href": "analysis/foia_processing.html#indiana",
    "title": "FOIA Document OCR Processing",
    "section": "3.3 Indiana",
    "text": "3.3 Indiana\n\nin_path = per_state / \"indiana_foia_data.csv\"\nindiana = pd.read_csv(in_path)\n\ndisplay(indiana)\n\n\n\n\n\n\n\n\nstate\noffender_type\nvariable_category\nvariable_detailed\nvalue\nvalue_type\ndate\n\n\n\n\n0\nIndiana\nConvicted Offender\ntotal\ntotal_profiles\n279654\ncount\n2018-07-24\n\n\n1\nIndiana\nArrestee\ntotal\ntotal_profiles\n21087\ncount\n2018-07-24\n\n\n2\nIndiana\nCombined\ngender\nFemale\n20\npercentage\n2018-07-24\n\n\n3\nIndiana\nCombined\ngender\nMale\n80\npercentage\n2018-07-24\n\n\n4\nIndiana\nCombined\nrace\nCaucasian\n70\npercentage\n2018-07-24\n\n\n5\nIndiana\nCombined\nrace\nBlack\n26\npercentage\n2018-07-24\n\n\n6\nIndiana\nCombined\nrace\nHispanic\n4\npercentage\n2018-07-24\n\n\n7\nIndiana\nCombined\nrace\nOther\n&lt;1\npercentage\n2018-07-24\n\n\n\n\n\n\n\nIndiana presents a unique reporting pattern: total counts are provided, but demographic breakdowns are given only as percentages. Additionally, the value column contains String data that requires conversion.\n\n# First, let's examine the data to see what values we have\nprint(\"Unique values in Indiana data before conversion:\")\nprint(indiana['value'].unique())\n\n# Convert string values to numeric, handling \"&lt;1\" as 0.5\nindiana['value'] = indiana['value'].apply(lambda x: 0.5 if x == '&lt;1' else pd.to_numeric(x))\n\nprint(\"\\n✓ Converted Indiana values from String to numeric\")\nprint(f\"Value data type after conversion: {indiana['value'].dtype}\")\nprint(f\"Values after conversion: {indiana['value'].unique()}\")\n\nUnique values in Indiana data before conversion:\n['279654' '21087' '20' '80' '70' '26' '4' '&lt;1']\n\n✓ Converted Indiana values from String to numeric\nValue data type after conversion: float64\nValues after conversion: [2.79654e+05 2.10870e+04 2.00000e+01 8.00000e+01 7.00000e+01 2.60000e+01\n 4.00000e+00 5.00000e-01]\n\n\nRecording Indiana’s reporting structure:\n\nfoia_state_metadata.append({\n    \"state\": \"Indiana\",\n    \"race_report_values\": report_status(indiana, \"race\"),\n    \"gender_report_values\": report_status(indiana, \"gender\"),\n    \"notes\": \"Provides percentages for demographics, counts for totals only\"\n})\n\nprint(\"✓ Added metadata row:\", foia_state_metadata[-1])\n\n✓ Added metadata row: {'state': 'Indiana', 'race_report_values': 'percentages', 'gender_report_values': 'percentages', 'notes': 'Provides percentages for demographics, counts for totals only'}\n\n\n\n3.3.1 Understanding Indiana’s unique reporting structure\nIndiana provides an unusual reporting pattern: - Total profile counts for Convicted Offender and Arrestee separately - Demographic percentages only for Combined totals (not broken down by offender type)\nFirst, let’s calculate the Combined total:\n\n# Calculate Combined total from the separate offender type totals\nconv_total = indiana[(indiana['offender_type'] == 'Convicted Offender') & \n                     (indiana['variable_detailed'] == 'total_profiles')]['value'].values[0]\narr_total = indiana[(indiana['offender_type'] == 'Arrestee') & \n                    (indiana['variable_detailed'] == 'total_profiles')]['value'].values[0]\ncombined_total = conv_total + arr_total\n\nprint(f\"Convicted Offender total: {conv_total:,.0f}\")\nprint(f\"Arrestee total: {arr_total:,.0f}\")\nprint(f\"Combined total: {combined_total:,.0f}\")\n\n# Add Combined total to Indiana data\ncombined_total_row = pd.DataFrame([{\n    'state': 'Indiana',\n    'offender_type': 'Combined',\n    'variable_category': 'total',\n    'variable_detailed': 'total_profiles',\n    'value': combined_total,\n    'value_type': 'count',\n    'value_source': 'calculated'\n}])\n\nindiana = pd.concat([indiana, combined_total_row], ignore_index=True)\nprint(\"\\n✓ Added Combined total to Indiana data\")\n\nConvicted Offender total: 279,654\nArrestee total: 21,087\nCombined total: 300,741\n\n✓ Added Combined total to Indiana data\n\n\n\n\n3.3.2 Calculating demographic counts from percentages\nNow we can calculate the actual counts from the Combined percentages:\n\n# First, prepare and append the reported data\nin_for_combined = prepare_state_for_combined(indiana, \"Indiana\")\n\n# Update value_source for the calculated Combined total\nin_for_combined.loc[\n    (in_for_combined['offender_type'] == 'Combined') & \n    (in_for_combined['variable_detailed'] == 'total_profiles'),\n    'value_source'\n] = 'calculated'\n\nfoia_combined = pd.concat([foia_combined, in_for_combined], ignore_index=True)\n\nprint(f\"✓ Appended {len(in_for_combined)} Indiana reported rows to foia_combined\")\n\n# Now calculate counts from the percentages\nin_counts = calculate_counts_from_percentages(foia_combined, \"Indiana\")\n\n# Append calculated counts\nif len(in_counts) &gt; 0:\n    foia_combined = pd.concat([foia_combined, in_counts], ignore_index=True)\n    print(f\"✓ Calculated and added {len(in_counts)} count rows for Indiana\")\n    \n    # Display sample of calculated counts\n    print(\"\\nSample of calculated Indiana counts:\")\n    display(in_counts[in_counts['variable_category'] == 'race'].head())\nelse:\n    print(\"✗ No counts could be calculated (this shouldn't happen)\")\n\nprint(f\"✓ Total rows in foia_combined: {len(foia_combined)}\")\n\n✓ Appended 9 Indiana reported rows to foia_combined\n✓ Calculated and added 6 count rows for Indiana\n\nSample of calculated Indiana counts:\n\n\n\n\n\n\n\n\n\nstate\noffender_type\nvariable_category\nvariable_detailed\nvalue\nvalue_type\nvalue_source\n\n\n\n\n2\nIndiana\nCombined\nrace\nCaucasian\n210519\ncount\ncalculated\n\n\n3\nIndiana\nCombined\nrace\nBlack\n78193\ncount\ncalculated\n\n\n4\nIndiana\nCombined\nrace\nHispanic\n12030\ncount\ncalculated\n\n\n5\nIndiana\nCombined\nrace\nOther\n1504\ncount\ncalculated\n\n\n\n\n\n\n\n✓ Total rows in foia_combined: 88\n\n\n\n\n3.3.3 Verifying calculated totals\n\n# Create temporary dataframe with Indiana's complete data for verification\nin_complete = pd.concat([indiana, in_counts], ignore_index=True)\nin_category_qc = verify_category_totals(in_complete)\ndisplay(in_category_qc)\n\n# Check if percentages sum to ~100%\nprint(\"\\nVerifying Indiana percentages sum to ~100%:\")\nin_pct_check = (indiana[indiana['value_type'] == 'percentage']\n                .groupby(['offender_type', 'variable_category'])['value']\n                .sum()\n                .round(2))\ndisplay(in_pct_check)\n\n\n\n\n\n\n\n\noffender_type\nvariable_category\ntotal_profiles\nsum_demo_counts\ndifference\n\n\n\n\n0\nCombined\ngender\n300741.0\n300741.0\n0.0\n\n\n1\nCombined\nrace\n300741.0\n302246.0\n-1505.0\n\n\n\n\n\n\n\n\nVerifying Indiana percentages sum to ~100%:\n\n\noffender_type  variable_category\nCombined       gender               100.0\n               race                 100.5\nName: value, dtype: float64\n\n\n\n\n3.3.4 Visualizing Indiana demographics\n\ncreate_demographic_bar_charts(foia_combined, \"Indiana\")\n\n\n\n\n\n\n\n\n\nIndiana profile totals by offender type:\n\n\n\n\n\n\n\n\n\nTotal Profiles\n\n\nOffender Type\n\n\n\n\n\nConvicted Offender\n279,654\n\n\nArrestee\n21,087\n\n\nCombined\n300,741\n\n\n\n\n\n\n\n\n\n3.3.5 Summary of Indiana processing\nIndiana data processing complete. The state’s unique reporting required: - Conversion of String values to numeric format - Calculation of demographic counts from reported percentages - All demographic values now available in both count and percentage formats\nMixed value_source attribution: percentages are “reported”, counts are “calculated”."
  },
  {
    "objectID": "analysis/foia_processing.html#maine",
    "href": "analysis/foia_processing.html#maine",
    "title": "FOIA Document OCR Processing",
    "section": "3.4 Maine",
    "text": "3.4 Maine\n\nme_path = per_state / \"maine_foia_data.csv\"\nmaine = pd.read_csv(me_path)\n\ndisplay(maine)\n\n\n\n\n\n\n\n\nstate\noffender_type\nvariable_category\nvariable_detailed\nvalue\nvalue_type\ndate\n\n\n\n\n0\nMaine\nCombined\ntotal\ntotal_profiles\n33711.0\ncount\n2018-07-10\n\n\n1\nMaine\nCombined\ngender\nMale\n27694.0\ncount\n2018-07-10\n\n\n2\nMaine\nCombined\ngender\nMale\n82.7\npercentage\n2018-07-10\n\n\n3\nMaine\nCombined\ngender\nFemale\n5734.0\ncount\n2018-07-10\n\n\n4\nMaine\nCombined\ngender\nFemale\n17.0\npercentage\n2018-07-10\n\n\n5\nMaine\nCombined\ngender\nUnknown\n83.0\ncount\n2018-07-10\n\n\n6\nMaine\nCombined\ngender\nUnknown\n0.2\npercentage\n2018-07-10\n\n\n7\nMaine\nCombined\nrace\nWhite\n31298.0\ncount\n2018-07-10\n\n\n8\nMaine\nCombined\nrace\nWhite\n92.8\npercentage\n2018-07-10\n\n\n9\nMaine\nCombined\nrace\nBlack\n1299.0\ncount\n2018-07-10\n\n\n10\nMaine\nCombined\nrace\nBlack\n3.9\npercentage\n2018-07-10\n\n\n11\nMaine\nCombined\nrace\nUnknown\n470.0\ncount\n2018-07-10\n\n\n12\nMaine\nCombined\nrace\nUnknown\n1.4\npercentage\n2018-07-10\n\n\n13\nMaine\nCombined\nrace\nNative American\n345.0\ncount\n2018-07-10\n\n\n14\nMaine\nCombined\nrace\nNative American\n1.0\npercentage\n2018-07-10\n\n\n15\nMaine\nCombined\nrace\nHispanic\n171.0\ncount\n2018-07-10\n\n\n16\nMaine\nCombined\nrace\nHispanic\n0.5\npercentage\n2018-07-10\n\n\n17\nMaine\nCombined\nrace\nAsian\n128.0\ncount\n2018-07-10\n\n\n18\nMaine\nCombined\nrace\nAsian\n0.4\npercentage\n2018-07-10\n\n\n\n\n\n\n\nMaine provides comprehensive reporting with both counts and percentages for all demographic categories, including Combined totals.\nRecording Maine’s reporting structure:\n\nfoia_state_metadata.append({\n    \"state\": \"Maine\",\n    \"race_report_values\": report_status(maine, \"race\"),\n    \"gender_report_values\": report_status(maine, \"gender\")\n})\n\nprint(\"✓ Added metadata row:\", foia_state_metadata[-1])\n\n✓ Added metadata row: {'state': 'Maine', 'race_report_values': 'both', 'gender_report_values': 'both'}\n\n\n\n3.4.1 Verifying demographic consistency\n\nme_category_qc = verify_category_totals(maine)\ndisplay(me_category_qc)\n\n# Also verify that reported percentages sum to ~100%\nprint(\"\\nVerifying Maine percentages sum to ~100%:\")\nme_pct_check = (maine[maine['value_type'] == 'percentage']\n                .groupby(['variable_category'])['value']\n                .sum()\n                .round(2))\ndisplay(me_pct_check)\n\n\n\n\n\n\n\n\noffender_type\nvariable_category\ntotal_profiles\nsum_demo_counts\ndifference\n\n\n\n\n0\nCombined\ngender\n33711.0\n33511.0\n200.0\n\n\n1\nCombined\nrace\n33711.0\n33711.0\n0.0\n\n\n\n\n\n\n\n\nVerifying Maine percentages sum to ~100%:\n\n\nvariable_category\ngender     99.9\nrace      100.0\nName: value, dtype: float64\n\n\nMaine’s data shows excellent internal consistency with both counts and percentages properly aligned.\n\n\n3.4.2 Preparing Maine data for the combined dataset\n\n# Prepare Maine data with only needed columns\nme_for_combined = prepare_state_for_combined(maine, \"Maine\")\n\n# Display structure verification\nprint(\"Maine data structure verification:\")\nprint(f\"Unique offender types: {me_for_combined['offender_type'].unique()}\")\nprint(f\"Unique variable categories: {me_for_combined['variable_category'].unique()}\")\nprint(f\"Value types present: {me_for_combined['value_type'].unique()}\")\n\n# Append to the combined dataframe\nfoia_combined = pd.concat([foia_combined, me_for_combined], ignore_index=True)\n\nprint(f\"\\n✓ Appended {len(me_for_combined)} Maine rows to foia_combined\")\nprint(f\"✓ Total rows in foia_combined: {len(foia_combined)}\")\n\nMaine data structure verification:\nUnique offender types: ['Combined']\nUnique variable categories: ['total' 'gender' 'race']\nValue types present: ['count' 'percentage']\n\n✓ Appended 19 Maine rows to foia_combined\n✓ Total rows in foia_combined: 107\n\n\n\n\n3.4.3 Visualizing Maine demographics\n\ncreate_demographic_bar_charts(foia_combined, \"Maine\")\n\n\n\n\n\n\n\n\n\nMaine profile totals by offender type:\n\n\n\n\n\n\n\n\n\nTotal Profiles\n\n\nOffender Type\n\n\n\n\n\nCombined\n33,711\n\n\n\n\n\n\n\n\n\n3.4.4 Summary of Maine processing\nMaine data processing complete. The state provided: - Complete reporting with both counts and percentages - Combined totals already calculated - Internally consistent data with percentages summing to 100%\nAll values maintain value_source = \"reported\" as no calculations were necessary."
  },
  {
    "objectID": "analysis/sdis_summary.html",
    "href": "analysis/sdis_summary.html",
    "title": "SDIS Summary Analysis",
    "section": "",
    "text": "This analysis examines State DNA Index System (SDIS) data that includes information reported separately for each state’s DNA database. The data captures key dimensions including:\n\nTotal size of each state’s DNA database\nWhether states collect DNA from arrestees (not just convicted offenders)\nWhether states allow familial DNA searching\nReferences to relevant state statutes (from Murphy & Tong appendix)\n\nThis information provides insight into the variation in DNA database policies, practices, and legal frameworks across U.S. states."
  },
  {
    "objectID": "analysis/sdis_summary.html#overview",
    "href": "analysis/sdis_summary.html#overview",
    "title": "SDIS Summary Analysis",
    "section": "",
    "text": "This analysis examines State DNA Index System (SDIS) data that includes information reported separately for each state’s DNA database. The data captures key dimensions including:\n\nTotal size of each state’s DNA database\nWhether states collect DNA from arrestees (not just convicted offenders)\nWhether states allow familial DNA searching\nReferences to relevant state statutes (from Murphy & Tong appendix)\n\nThis information provides insight into the variation in DNA database policies, practices, and legal frameworks across U.S. states."
  },
  {
    "objectID": "analysis/sdis_summary.html#data-loading",
    "href": "analysis/sdis_summary.html#data-loading",
    "title": "SDIS Summary Analysis",
    "section": "Data Loading",
    "text": "Data Loading\n\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\n# Set up path to data file\nbase_dir = Path(\"..\")\ndata_file = base_dir / \"output\" / \"sdis\" / \"sdis_raw.csv\"\n\n# Load the SDIS data\nsdis_data = pd.read_csv(data_file)\n\nprint(f\"Loaded SDIS data: {len(sdis_data)} rows\")\nprint(f\"Columns: {list(sdis_data.columns)}\")\n\n# Display first few rows\ndisplay(sdis_data.head())\n\n# Display data types for each column\nprint(\"\\nData types:\")\ndisplay(sdis_data.dtypes)\n\nLoaded SDIS data: 50 rows\nColumns: ['state', 'n_total', 'n_arrestees', 'n_offenders', 'n_forensic', 'arrestee_collection', 'collection_statute', 'fam_search', 'database_source', 'database_source_year', 'verification_comment']\n\n\n\n\n\n\n\n\n\nstate\nn_total\nn_arrestees\nn_offenders\nn_forensic\narrestee_collection\ncollection_statute\nfam_search\ndatabase_source\ndatabase_source_year\nverification_comment\n\n\n\n\n0\nAlabama\n360000.0\nNaN\nNaN\nNaN\nyes\nAla. Code Sec.36-18-25\nunspecified\nhttps://adfs.alabama.gov/services/fb/fb-statis...\n2024.0\nAlabama Department of Forensic Sciences notes ...\n\n\n1\nAlaska\n79146.0\n52149.0\n26997.0\n3866.0\nyes\nAK Stat Sec.44.41.035 (2014)\nunspecified\nhttps://dps.alaska.gov/Statewide/CrimeLab/Fore...\n2024.0\nAlaska DPS crime‑lab CODIS metrics (Feb 2024) ...\n\n\n2\nArizona\nNaN\nNaN\nNaN\nNaN\nyes\nAZ Rev Stat Sec.13-610 (2016)\nunspecified\nNaN\nNaN\nNaN\n\n\n3\nArkansas\n12221.0\nNaN\nNaN\nNaN\nyes\nAR Code Sec.12-12-1109 & Sec.12-12-1006\npermitted\nhttps://www.dps.arkansas.gov/wp-content/upload...\n2022.0\nThe Arkansas State Crime Laboratory 2022 annua...\n\n\n4\nCalifornia\n3365402.0\nNaN\nNaN\n167053.0\nyes\nCA Penal Code Sec.296\npermitted\nhttps://oag.ca.gov/system/files/media/databank...\n2024.0\nCalifornia’s Proposition 69 First Quarter 2024...\n\n\n\n\n\n\n\n\nData types:\n\n\nstate                    object\nn_total                 float64\nn_arrestees             float64\nn_offenders             float64\nn_forensic              float64\narrestee_collection      object\ncollection_statute       object\nfam_search               object\ndatabase_source          object\ndatabase_source_year    float64\nverification_comment     object\ndtype: object"
  },
  {
    "objectID": "analysis/sdis_summary.html#data-preprocessing",
    "href": "analysis/sdis_summary.html#data-preprocessing",
    "title": "SDIS Summary Analysis",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nThis section ensures data consistency between arrestee collection policies and reported arrestee counts. States that do not collect arrestee DNA (arrestee_collection = ‘no’) should have N_arrestees set to zero to avoid data inconsistencies.\n\n# Create a copy of the data for processing\nsdis_data_processed = sdis_data.copy()\n\n# Count states affected by this adjustment\nstates_with_no_collection = sdis_data_processed[sdis_data_processed['arrestee_collection'] == 'no']\nstates_to_adjust = states_with_no_collection[states_with_no_collection['n_arrestees'].notna() & (states_with_no_collection['n_arrestees'] != 0)]\n\nif len(states_to_adjust) &gt; 0:\n    print(f\"States with arrestee_collection='no' but non-zero n_arrestees values:\")\n    for _, state in states_to_adjust.iterrows():\n        print(f\"  • {state['state']}: n_arrestees = {state['n_arrestees']:,.0f}\")\n\n# Set n_arrestees to 0 for states that don't collect arrestee DNA\nsdis_data_processed.loc[sdis_data_processed['arrestee_collection'] == 'no', 'n_arrestees'] = 0\n\nprint(f\"\\nAdjusted N_arrestees to 0 for {len(states_with_no_collection)} states that do not collect arrestee DNA\")\n\n# Use processed data for all subsequent analyses\nsdis_data = sdis_data_processed\n\n\nAdjusted N_arrestees to 0 for 17 states that do not collect arrestee DNA"
  },
  {
    "objectID": "analysis/sdis_summary.html#data-availability-overview",
    "href": "analysis/sdis_summary.html#data-availability-overview",
    "title": "SDIS Summary Analysis",
    "section": "Data Availability Overview",
    "text": "Data Availability Overview\nThis section provides an overview of states represented in the dataset and the completeness of data fields across states.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Identify states present in the dataset\nstates_in_data = sdis_data['state'].unique()\nstates_in_data = sorted(states_in_data)\nprint(f\"Number of states with data: {len(states_in_data)}\")\n\n# Check if all 50 states are represented\nall_states = ['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut',\n              'Delaware', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa',\n              'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan',\n              'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire',\n              'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio',\n              'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina', 'South Dakota',\n              'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia',\n              'Wisconsin', 'Wyoming']\n\nmissing_states = [state for state in all_states if state not in states_in_data]\nif missing_states:\n    print(f\"\\nMissing states: {', '.join(missing_states)}\")\nelse:\n    print(\"\\nAll 50 states are represented in the dataset\")\n\n# Assess data completeness for each state\ndata_availability = pd.DataFrame(index=states_in_data)\n\nfor col in sdis_data.columns:\n    if col != 'state':  # Exclude the state identifier column\n        # Calculate non-null values per state\n        availability = sdis_data.groupby('state')[col].apply(lambda x: x.notna().sum())\n        data_availability[col] = availability\n\n# Generate visualization of data completeness\nif len(data_availability.columns) &gt; 0:\n    # Focus on key numeric and policy fields\n    key_fields = ['n_total', 'n_arrestees', 'n_offenders', 'n_forensic', \n                  'arrestee_collection', 'fam_search']\n    \n    # Filter to include only key fields that exist in the data\n    available_key_fields = [f for f in key_fields if f in data_availability.columns]\n    \n    if available_key_fields:\n        plt.figure(figsize=(10, 14))\n        \n        # Create binary matrix for visualization\n        availability_subset = data_availability[available_key_fields]\n        availability_binary = (availability_subset &gt; 0).astype(int)\n        \n        # Generate heatmap\n        sns.heatmap(availability_binary, \n                    cmap=['#f0f0f0', '#2E86AB'],\n                    cbar=False,  # Remove colorbar for binary data\n                    linewidths=0.5,\n                    linecolor='gray',\n                    square=True,\n                    vmin=0, vmax=1)\n        \n        plt.title('Data Availability by State', fontsize=14, pad=20)\n        plt.xlabel('Data Fields', fontsize=12)\n        plt.ylabel('States', fontsize=12)\n        plt.xticks(rotation=45, ha='right')\n        \n        # Add annotations for clarity\n        for i in range(len(availability_binary)):\n            for j in range(len(availability_binary.columns)):\n                if availability_binary.iloc[i, j] == 1:\n                    plt.text(j + 0.5, i + 0.5, '✓', ha='center', va='center', \n                            fontsize=8, color='white', fontweight='bold')\n        \n        plt.tight_layout()\n        plt.show()\n        \n        print(f\"\\nTotal states shown in heatmap: {len(availability_binary)}\")\n\n# Data coverage summary\nprint(f\"\\nData field coverage across states:\")\n\n# Focus on key fields\nkey_fields = ['n_total', 'n_arrestees', 'n_offenders', 'n_forensic', \n              'arrestee_collection', 'fam_search', 'collection_statute']\n\nfor col in key_fields:\n    if col in data_availability.columns:\n        states_with_data = (data_availability[col] &gt; 0).sum()\n        coverage_pct = states_with_data/len(states_in_data)*100\n        print(f\"{col}: {states_with_data} states ({coverage_pct:.1f}%)\")\n\nNumber of states with data: 50\n\nAll 50 states are represented in the dataset\n\n\n\n\n\n\n\n\n\n\nTotal states shown in heatmap: 50\n\nData field coverage across states:\nn_total: 27 states (54.0%)\nn_arrestees: 23 states (46.0%)\nn_offenders: 16 states (32.0%)\nn_forensic: 11 states (22.0%)\narrestee_collection: 50 states (100.0%)\nfam_search: 50 states (100.0%)\ncollection_statute: 50 states (100.0%)"
  },
  {
    "objectID": "analysis/sdis_summary.html#total-profile-calculations-verification",
    "href": "analysis/sdis_summary.html#total-profile-calculations-verification",
    "title": "SDIS Summary Analysis",
    "section": "Total Profile Calculations Verification",
    "text": "Total Profile Calculations Verification\nThis section examines states reporting n_total alongside component counts to determine whether totals represent: 1. Sum of all profile types including forensic (n_arrestees + n_offenders + n_forensic) 2. Sum of combined profiles only (n_arrestees + n_offenders)\n\n# Note: This analysis uses the original data before renaming columns\n# Create a temporary copy with the original column name for this analysis\nsdis_data_temp = sdis_data.copy()\nif 'n_total_reported' in sdis_data_temp.columns and 'n_total' not in sdis_data_temp.columns:\n    sdis_data_temp['n_total'] = sdis_data_temp['n_total_reported']\n\n# Identify states with n_total and at least one component count\nstates_with_totals = sdis_data_temp[sdis_data_temp['n_total'].notna()].copy()\n\n# Calculate different possible sums\nstates_with_totals['sum_all'] = states_with_totals[['n_arrestees', 'n_offenders', 'n_forensic']].sum(axis=1, skipna=True)\nstates_with_totals['sum_forensic'] = states_with_totals[['n_arrestees', 'n_offenders']].sum(axis=1, skipna=True)\n\n# Check which sum matches n_total (with small tolerance for rounding)\ntolerance = 10  # Allow small differences due to rounding or timing\n\n# Check matches_combined_forensic - will be False if any component is missing\nstates_with_totals['matches_combined_forensic'] = np.where(\n    states_with_totals[['n_arrestees', 'n_offenders', 'n_forensic']].notna().all(axis=1),\n    abs(states_with_totals['n_total'] - states_with_totals['sum_all']) &lt;= tolerance,\n    False\n)\n\n# Check matches_combined - will be False if n_arrestees or n_offenders is missing\n# Additional condition: only True if arrestee_collection is 'no' OR n_arrestees &gt; 0\nstates_with_totals['matches_combined'] = np.where(\n    states_with_totals[['n_arrestees', 'n_offenders']].notna().all(axis=1),\n    (abs(states_with_totals['n_total'] - states_with_totals['sum_forensic']) &lt;= tolerance) & \n    ((states_with_totals['arrestee_collection'] == 'no') | (states_with_totals['n_arrestees'] &gt; 0)),\n    False\n)\n\n# Create summary dataframe for display\ntotal_verification = states_with_totals[['state', 'n_total', 'n_arrestees', 'n_offenders', 'n_forensic', \n                                        'arrestee_collection', 'sum_all', 'sum_forensic', \n                                        'matches_combined_forensic', 'matches_combined']].copy()\n\n# Filter to states with at least one component count\nhas_components = total_verification[\n    (total_verification['n_arrestees'].notna()) | \n    (total_verification['n_offenders'].notna()) | \n    (total_verification['n_forensic'].notna())\n]\n\nprint(f\"States with n_total and component data: {len(has_components)}\")\nprint(\"\\nTotal calculation patterns:\")\n\n# Categorize states\nincludes_all = has_components[has_components['matches_combined_forensic'] == True]['state'].tolist()\nforensic_only = has_components[(has_components['matches_combined'] == True) & (has_components['matches_combined_forensic'] == False)]['state'].tolist()\nneither = has_components[(has_components['matches_combined_forensic'] == False) & (has_components['matches_combined'] == False)]['state'].tolist()\n\nif includes_all:\n    print(f\"\\nn_total includes combined profiles with forensic (arrestees + offenders + forensic):\")\n    for state in includes_all:\n        print(f\"  • {state}\")\n\nif forensic_only:\n    print(f\"\\nn_total includes combined profiles only (arrestees + offenders):\")\n    for state in forensic_only:\n        print(f\"  • {state}\")\n\nif neither:\n    print(f\"\\nn_total does not match calculated sums:\")\n    for state in neither:\n        state_data = has_components[has_components['state'] == state].iloc[0]\n        print(f\"  • {state}: n_total={state_data['n_total']:,.0f}, \"\n              f\"Sum_all={state_data['sum_all']:,.0f}, \"\n              f\"Sum_forensic={state_data['sum_forensic']:,.0f}\")\n\n# Display detailed breakdown for verification\nprint(\"\\nDetailed breakdown:\")\ndisplay(has_components[['state', 'n_total', 'n_arrestees', 'n_offenders', 'n_forensic', \n                       'arrestee_collection', 'matches_combined_forensic', 'matches_combined']].style.format({\n    'n_total': '{:,.0f}',\n    'n_arrestees': lambda x: '{:,.0f}'.format(x) if pd.notna(x) else '',\n    'n_offenders': lambda x: '{:,.0f}'.format(x) if pd.notna(x) else '',\n    'n_forensic': lambda x: '{:,.0f}'.format(x) if pd.notna(x) else ''\n}))\n\nStates with n_total and component data: 17\n\nTotal calculation patterns:\n\nn_total includes combined profiles only (arrestees + offenders):\n  • Alaska\n  • Idaho\n  • Montana\n  • North Carolina\n  • Rhode Island\n  • Tennessee\n  • Washington\n  • West Virginia\n\nn_total does not match calculated sums:\n  • California: n_total=3,365,402, Sum_all=167,053, Sum_forensic=0\n  • Connecticut: n_total=127,940, Sum_all=151,140, Sum_forensic=127,940\n  • Georgia: n_total=347,145, Sum_all=347,145, Sum_forensic=324,864\n  • Illinois: n_total=689,297, Sum_all=759,224, Sum_forensic=689,297\n  • Indiana: n_total=449,000, Sum_all=470,400, Sum_forensic=448,000\n  • Minnesota: n_total=180,000, Sum_all=180,000, Sum_forensic=180,000\n  • Missouri: n_total=400,000, Sum_all=386,000, Sum_forensic=386,000\n  • South Carolina: n_total=175,629, Sum_all=11,127, Sum_forensic=0\n  • Texas: n_total=1,308,774, Sum_all=1,308,774, Sum_forensic=1,308,774\n\nDetailed breakdown:\n\n\n\n\n\n\n\n \nstate\nn_total\nn_arrestees\nn_offenders\nn_forensic\narrestee_collection\nmatches_combined_forensic\nmatches_combined\n\n\n\n\n1\nAlaska\n79,146\n52,149\n26,997\n3,866\nyes\nFalse\nTrue\n\n\n4\nCalifornia\n3,365,402\n\n\n167,053\nyes\nFalse\nFalse\n\n\n6\nConnecticut\n127,940\n\n127,940\n23,200\nyes\nFalse\nFalse\n\n\n9\nGeorgia\n347,145\n\n324,864\n22,281\nyes\nFalse\nFalse\n\n\n11\nIdaho\n39,000\n0\n39,000\n\nno\nFalse\nTrue\n\n\n12\nIllinois\n689,297\n\n689,297\n69,927\nyes\nFalse\nFalse\n\n\n13\nIndiana\n449,000\n144,000\n304,000\n22,400\nyes\nFalse\nFalse\n\n\n22\nMinnesota\n180,000\n\n180,000\n\nyes\nFalse\nFalse\n\n\n24\nMissouri\n400,000\n\n386,000\n\nyes\nFalse\nFalse\n\n\n25\nMontana\n32,284\n0\n32,284\n878\nno\nFalse\nTrue\n\n\n32\nNorth Carolina\n350,000\n50,000\n300,000\n\nyes\nFalse\nTrue\n\n\n38\nRhode Island\n27,818\n484\n27,334\n1,798\nyes\nFalse\nTrue\n\n\n39\nSouth Carolina\n175,629\n\n\n11,127\nyes\nFalse\nFalse\n\n\n41\nTennessee\n518,614\n226,569\n292,045\n\nyes\nFalse\nTrue\n\n\n42\nTexas\n1,308,774\n\n1,308,774\n\nyes\nFalse\nFalse\n\n\n46\nWashington\n272,000\n0\n272,000\n8,800\nno\nFalse\nTrue\n\n\n47\nWest Virginia\n47,444\n0\n47,444\n3,905\nno\nFalse\nTrue"
  },
  {
    "objectID": "analysis/sdis_summary.html#analysis-of-database-totals-and-data-quality-issues",
    "href": "analysis/sdis_summary.html#analysis-of-database-totals-and-data-quality-issues",
    "title": "SDIS Summary Analysis",
    "section": "Analysis of Database Totals and Data Quality Issues",
    "text": "Analysis of Database Totals and Data Quality Issues\nThis section examines states where N_total values reveal potential data quality issues or reporting inconsistencies.\n\n# Create enhanced data quality analysis\nsdis_enhanced = sdis_data.copy()\n\n# Rename n_total to n_total_reported\nsdis_enhanced = sdis_enhanced.rename(columns={'n_total': 'n_total_reported'})\n\n# Calculate different total relationships with small tolerance\ntolerance = 10\n\n# Check if n_total equals different combinations\n# Note: Only consider values &gt; 0 as valid data (0 means no data)\n# For offenders only: either arrestees are missing/zero OR state doesn't collect arrestees\nsdis_enhanced['total_equals_offenders'] = np.where(\n    sdis_enhanced['n_total_reported'].notna() & \n    sdis_enhanced['n_offenders'].notna() & \n    (sdis_enhanced['n_offenders'] &gt; 0),\n    abs(sdis_enhanced['n_total_reported'] - sdis_enhanced['n_offenders']) &lt;= tolerance,\n    False\n)\n\nsdis_enhanced['total_equals_off_arr'] = np.where(\n    sdis_enhanced['n_total_reported'].notna() & \n    sdis_enhanced['n_offenders'].notna() & \n    sdis_enhanced['n_arrestees'].notna() &\n    (sdis_enhanced['n_offenders'] &gt; 0) &\n    (sdis_enhanced['n_arrestees'] &gt; 0),\n    abs(sdis_enhanced['n_total_reported'] - (sdis_enhanced['n_offenders'] + sdis_enhanced['n_arrestees'])) &lt;= tolerance,\n    False\n)\n\nsdis_enhanced['total_equals_all'] = np.where(\n    sdis_enhanced['n_total_reported'].notna() & \n    sdis_enhanced['n_offenders'].notna() & \n    sdis_enhanced['n_arrestees'].notna() & \n    sdis_enhanced['n_forensic'].notna() &\n    (sdis_enhanced['n_offenders'] &gt; 0) &\n    (sdis_enhanced['n_arrestees'] &gt; 0) &\n    (sdis_enhanced['n_forensic'] &gt; 0),\n    abs(sdis_enhanced['n_total_reported'] - (sdis_enhanced['n_offenders'] + sdis_enhanced['n_arrestees'] + sdis_enhanced['n_forensic'])) &lt;= tolerance,\n    False\n)\n\n# Determine total calculation method for each state\n# Use priority order: All components &gt; Offenders + Arrestees &gt; Offenders only\nsdis_enhanced['total_method'] = 'Unknown'\nsdis_enhanced.loc[sdis_enhanced['total_equals_offenders'], 'total_method'] = 'Offenders only'\nsdis_enhanced.loc[sdis_enhanced['total_equals_off_arr'], 'total_method'] = 'Offenders + Arrestees'\nsdis_enhanced.loc[sdis_enhanced['total_equals_all'], 'total_method'] = 'All components'\n\n# Create n_total_estimated based on the rules specified\nsdis_enhanced['n_total_estimated'] = np.nan\nsdis_enhanced['n_total_estimated_comment'] = ''\n\n# Rule 1: States where n_total == n_offenders + n_arrestees\nmask_off_arr = sdis_enhanced['total_equals_off_arr']\nsdis_enhanced.loc[mask_off_arr, 'n_total_estimated'] = sdis_enhanced.loc[mask_off_arr, 'n_total_reported']\nsdis_enhanced.loc[mask_off_arr, 'n_total_estimated_comment'] = 'Used reported total (matches offenders + arrestees)'\n\n# Rule 2: States where n_total == n_offenders + n_arrestees + n_forensic\nmask_all = sdis_enhanced['total_equals_all']\nsdis_enhanced.loc[mask_all, 'n_total_estimated'] = (\n    sdis_enhanced.loc[mask_all, 'n_total_reported'] - sdis_enhanced.loc[mask_all, 'n_forensic']\n)\nsdis_enhanced.loc[mask_all, 'n_total_estimated_comment'] = 'Subtracted forensic from reported total'\n\n# Rule 3: States where n_total == n_offenders\n# This includes states that don't collect arrestees OR states where total just happens to equal offenders\nmask_off_only = sdis_enhanced['total_equals_offenders']\nsdis_enhanced.loc[mask_off_only, 'n_total_estimated'] = sdis_enhanced.loc[mask_off_only, 'n_total_reported']\nsdis_enhanced.loc[mask_off_only, 'n_total_estimated_comment'] = 'Used reported total (matches offenders only)'\n\n# Rule 4: For remaining states with n_total\n# First check if they have ONLY total (no component breakdown)\nmask_total_only = (\n    sdis_enhanced['n_total_reported'].notna() & \n    sdis_enhanced['n_total_estimated'].isna() &\n    (sdis_enhanced['n_arrestees'].isna() | (sdis_enhanced['n_arrestees'] == 0)) &\n    (sdis_enhanced['n_offenders'].isna() | (sdis_enhanced['n_offenders'] == 0)) &\n    (sdis_enhanced['n_forensic'].isna() | (sdis_enhanced['n_forensic'] == 0))\n)\nsdis_enhanced.loc[mask_total_only, 'n_total_estimated'] = sdis_enhanced.loc[mask_total_only, 'n_total_reported']\nsdis_enhanced.loc[mask_total_only, 'n_total_estimated_comment'] = 'Total only reported (no breakdown available)'\n\n# Special case: States like California with n_total and n_forensic only\n# (no offenders/arrestees breakdown)\nmask_total_forensic_only = (\n    sdis_enhanced['n_total_reported'].notna() & \n    sdis_enhanced['n_total_estimated'].isna() &\n    (sdis_enhanced['n_arrestees'].isna() | (sdis_enhanced['n_arrestees'] == 0)) &\n    (sdis_enhanced['n_offenders'].isna() | (sdis_enhanced['n_offenders'] == 0)) &\n    sdis_enhanced['n_forensic'].notna() & (sdis_enhanced['n_forensic'] &gt; 0)\n)\nsdis_enhanced.loc[mask_total_forensic_only, 'n_total_estimated'] = sdis_enhanced.loc[mask_total_forensic_only, 'n_total_reported']\nsdis_enhanced.loc[mask_total_forensic_only, 'n_total_estimated_comment'] = 'Total only reported (forensic reported separately)'\n\n# States with total and some components but unclear calculation\nmask_has_total_unclear = (\n    sdis_enhanced['n_total_reported'].notna() & \n    sdis_enhanced['n_total_estimated'].isna()\n)\nsdis_enhanced.loc[mask_has_total_unclear, 'n_total_estimated'] = sdis_enhanced.loc[mask_has_total_unclear, 'n_total_reported']\nsdis_enhanced.loc[mask_has_total_unclear, 'n_total_estimated_comment'] = 'Total with discrepancy (calculation unclear)'\n\n# For states without any total but with offenders and arrestees (both &gt; 0)\nmask_no_total = (\n    sdis_enhanced['n_total_reported'].isna() & \n    sdis_enhanced['n_offenders'].notna() & (sdis_enhanced['n_offenders'] &gt; 0) &\n    sdis_enhanced['n_arrestees'].notna() & (sdis_enhanced['n_arrestees'] &gt; 0)\n)\nsdis_enhanced.loc[mask_no_total, 'n_total_estimated'] = (\n    sdis_enhanced.loc[mask_no_total, 'n_offenders'] + sdis_enhanced.loc[mask_no_total, 'n_arrestees']\n)\nsdis_enhanced.loc[mask_no_total, 'n_total_estimated_comment'] = 'Calculated from offenders + arrestees (no total reported)'\n\n# For states without total but with only offenders &gt; 0\nmask_no_total_off_only = (\n    sdis_enhanced['n_total_reported'].isna() & \n    sdis_enhanced['n_offenders'].notna() & (sdis_enhanced['n_offenders'] &gt; 0) &\n    (sdis_enhanced['n_arrestees'].isna() | (sdis_enhanced['n_arrestees'] == 0))\n)\nsdis_enhanced.loc[mask_no_total_off_only, 'n_total_estimated'] = sdis_enhanced.loc[mask_no_total_off_only, 'n_offenders']\nsdis_enhanced.loc[mask_no_total_off_only, 'n_total_estimated_comment'] = 'Used offenders count (no total reported, no arrestee data)'\n\n# Create data availability matrix for heatmap\n# Treat 0 values as missing data\navailability_matrix = pd.DataFrame({\n    'State': sdis_enhanced['state'],\n    'Arrestees': (sdis_enhanced['n_arrestees'].notna() & (sdis_enhanced['n_arrestees'] &gt; 0)),\n    'Offenders': (sdis_enhanced['n_offenders'].notna() & (sdis_enhanced['n_offenders'] &gt; 0)),\n    'Forensic': (sdis_enhanced['n_forensic'].notna() & (sdis_enhanced['n_forensic'] &gt; 0)),\n    'Total Reported': sdis_enhanced['n_total_reported'].notna(),\n    'Total Method': sdis_enhanced['total_method']\n})\n\n# Summary of how n_total_estimated was calculated\nprint(\"n_total_estimated Calculation Summary:\")\nprint(\"=\" * 50)\n\n# Create single plot with better dimensions\nfig, ax = plt.subplots(figsize=(10, 16)) \n\n# Prepare data for combined heatmap\n# Data availability columns\navailability_binary = availability_matrix.set_index('State')[['Arrestees', 'Offenders', 'Forensic', 'Total Reported']].astype(int)\n\n# Add total method as a numeric column\nmethod_mapping = {\n    'Unknown': 0,\n    'Offenders only': 1,\n    'Offenders + Arrestees': 2,\n    'All components': 3\n}\navailability_binary['Total Method'] = availability_matrix.set_index('State')['Total Method'].replace(method_mapping)\n\n# Create custom colormap for the combined heatmap\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.patches as patches\n\n# Create the heatmap\nsns.heatmap(availability_binary.iloc[:, :4],  # First 4 columns (binary data)\n            cmap=['#f0f0f0', '#2E86AB'],\n            cbar=False,\n            linewidths=0.5,\n            linecolor='gray',\n            square=True,\n            ax=ax)  # Changed from ax1 to ax\n\n# Add the Total Method column with a different colormap\nx_pos = 4\nmethod_colors = ['#f0f0f0', '#FF6B6B', '#4ECDC4', '#45B7D1']\nfor i, (idx, row) in enumerate(availability_binary.iterrows()):\n    method_val = int(row['Total Method'])\n    rect = patches.Rectangle((x_pos, i), 1, 1, \n                           linewidth=0.5, \n                           edgecolor='gray',\n                           facecolor=method_colors[method_val])\n    ax.add_patch(rect)  # Changed from ax1 to ax\n\n# Update x-axis labels\ncolumn_labels = list(availability_binary.columns)\nax.set_xticks(np.arange(len(column_labels)) + 0.5)  # Changed from ax1 to ax\nax.set_xticklabels(column_labels, rotation=45, ha='right')  # Changed from ax1 to ax\n\n# Add checkmarks for binary columns\nfor i in range(len(availability_binary)):\n    for j in range(4):  # Only first 4 columns\n        if availability_binary.iloc[i, j] == 1:\n            ax.text(j + 0.5, i + 0.5, '✓', ha='center', va='center',  # Changed from ax1 to ax\n                   fontsize=8, color='white', fontweight='bold')\n\n# Add method labels for the Total Method column\nmethod_labels = ['?', 'O', 'O+A', 'All']\nfor i in range(len(availability_binary)):\n    method_val = int(availability_binary.iloc[i, 4])\n    ax.text(4.5, i + 0.5, method_labels[method_val], ha='center', va='center',  # Changed from ax1 to ax\n           fontsize=8, color='white' if method_val &gt; 0 else 'black', fontweight='bold')\n\nax.set_title('Data Field Availability and Total Calculation Method by State', fontsize=14, pad=20)  # Changed from ax1 to ax\nax.set_xlabel('')  # Changed from ax1 to ax\nax.set_ylabel('')  # Changed from ax1 to ax\n\n# Create legend at the bottom\nlegend_elements = [\n    patches.Patch(facecolor='#f0f0f0', edgecolor='black', label='Unknown (?)'),\n    patches.Patch(facecolor='#FF6B6B', edgecolor='black', label='Offenders only (O)'),\n    patches.Patch(facecolor='#4ECDC4', edgecolor='black', label='Offenders + Arrestees (O+A)'),\n    patches.Patch(facecolor='#45B7D1', edgecolor='black', label='All components (All)')\n]\n\n# Place legend below the plot\nax.legend(handles=legend_elements, \n          loc='upper center', \n          bbox_to_anchor=(0.5, -0.02),\n          ncol=2,\n          title='Total Method',\n          frameon=True)\n\n# Adjust layout\nplt.tight_layout()\nplt.show()\n\n# Group states by how their n_total_estimated was determined\nestimation_groups = sdis_enhanced.groupby('n_total_estimated_comment')['state'].apply(list)\n\nprint(\"\\n1. States with only n_total reported (no component breakdown):\")\nif 'Total only reported (no breakdown available)' in estimation_groups:\n    states = estimation_groups['Total only reported (no breakdown available)']\n    print(f\"   {len(states)} states: {', '.join(states)}\")\nif 'Total only reported (forensic reported separately)' in estimation_groups:\n    states = estimation_groups['Total only reported (forensic reported separately)']\n    print(f\"   - With forensic reported separately: {len(states)} states ({', '.join(states)})\")\n\nprint(\"\\n2. States where n_total matches component calculations:\")\nif 'Used reported total (matches offenders + arrestees)' in estimation_groups:\n    states = estimation_groups['Used reported total (matches offenders + arrestees)']\n    print(f\"   - Matches offenders + arrestees: {len(states)} states ({', '.join(states)})\")\nif 'Subtracted forensic from reported total' in estimation_groups:\n    states = estimation_groups['Subtracted forensic from reported total']\n    print(f\"   - Matches all components (forensic subtracted): {len(states)} states ({', '.join(states)})\")\nif 'Used reported total (matches offenders only)' in estimation_groups:\n    states = estimation_groups['Used reported total (matches offenders only)']\n    print(f\"   - Matches offenders only: {len(states)} states ({', '.join(states)})\")\n\nprint(\"\\n3. States without n_total reported:\")\nif 'Calculated from offenders + arrestees (no total reported)' in estimation_groups:\n    states = estimation_groups['Calculated from offenders + arrestees (no total reported)']\n    print(f\"   - Calculated from offenders + arrestees: {len(states)} states ({', '.join(states)})\")\nif 'Used offenders count (no total reported, no arrestee data)' in estimation_groups:\n    states = estimation_groups['Used offenders count (no total reported, no arrestee data)']\n    print(f\"   - Used offenders as proxy: {len(states)} states ({', '.join(states)})\")\n\nprint(\"\\n4. States with unclear calculation (total doesn't match expected patterns):\")\nif 'Total with discrepancy (calculation unclear)' in estimation_groups:\n    unclear_states = sdis_enhanced[sdis_enhanced['n_total_estimated_comment'] == 'Total with discrepancy (calculation unclear)']\n    \n    # Filter to only show states that truly don't match any pattern\n    states_to_display = []\n    for _, state in unclear_states.iterrows():\n        # Calculate different possible sums\n        sum_offenders = state['n_offenders'] if pd.notna(state['n_offenders']) and state['n_offenders'] &gt; 0 else 0\n        sum_arrestees = state['n_arrestees'] if pd.notna(state['n_arrestees']) and state['n_arrestees'] &gt; 0 else 0\n        sum_forensic = state['n_forensic'] if pd.notna(state['n_forensic']) and state['n_forensic'] &gt; 0 else 0\n        \n        sum_off_arr = sum_offenders + sum_arrestees\n        sum_all = sum_offenders + sum_arrestees + sum_forensic\n        \n        # Check what the total matches\n        matches_off = abs(state['n_total_reported'] - sum_offenders) &lt;= tolerance\n        matches_off_arr = abs(state['n_total_reported'] - sum_off_arr) &lt;= tolerance\n        matches_all = abs(state['n_total_reported'] - sum_all) &lt;= tolerance\n        \n        # Only include states that don't match any expected pattern\n        if not (matches_off or matches_off_arr or matches_all):\n            states_to_display.append(state['state'])\n    \n    if states_to_display:\n        print(f\"   {len(states_to_display)} states: {', '.join(states_to_display)}\")\n        print(\"   (These states have totals that don't match any combination of their component counts)\")\n\n# Show full enhanced data for all states\nprint(\"\\n\\nFull Enhanced Data with Estimated Totals (All 50 States):\")\ncols_to_show = ['state', 'n_total_estimated', 'n_total_reported', 'n_offenders', 'n_arrestees', \n                'n_forensic', 'n_total_estimated_comment']\ndisplay(sdis_enhanced[cols_to_show].style.format({\n    'n_total_estimated': lambda x: '{:,.0f}'.format(x) if pd.notna(x) else '',\n    'n_total_reported': lambda x: '{:,.0f}'.format(x) if pd.notna(x) else '',\n    'n_offenders': lambda x: '{:,.0f}'.format(x) if pd.notna(x) else '',\n    'n_arrestees': lambda x: '{:,.0f}'.format(x) if pd.notna(x) else '',\n    'n_forensic': lambda x: '{:,.0f}'.format(x) if pd.notna(x) else ''\n}))\n\n# Update the main dataframe for subsequent analyses\nsdis_data = sdis_enhanced\n\nn_total_estimated Calculation Summary:\n==================================================\n\n\n\n\n\n\n\n\n\n\n1. States with only n_total reported (no component breakdown):\n   10 states: Alabama, Arkansas, Colorado, Florida, Louisiana, Mississippi, Nevada, New Jersey, South Dakota, Virginia\n   - With forensic reported separately: 2 states (California, South Carolina)\n\n2. States where n_total matches component calculations:\n   - Matches offenders + arrestees: 4 states (Alaska, North Carolina, Rhode Island, Tennessee)\n   - Matches offenders only: 8 states (Connecticut, Idaho, Illinois, Minnesota, Montana, Texas, Washington, West Virginia)\n\n3. States without n_total reported:\n   - Calculated from offenders + arrestees: 1 states (Maryland)\n\n4. States with unclear calculation (total doesn't match expected patterns):\n   2 states: Indiana, Missouri\n   (These states have totals that don't match any combination of their component counts)\n\n\nFull Enhanced Data with Estimated Totals (All 50 States):\n\n\n\n\n\n\n\n \nstate\nn_total_estimated\nn_total_reported\nn_offenders\nn_arrestees\nn_forensic\nn_total_estimated_comment\n\n\n\n\n0\nAlabama\n360,000\n360,000\n\n\n\nTotal only reported (no breakdown available)\n\n\n1\nAlaska\n79,146\n79,146\n26,997\n52,149\n3,866\nUsed reported total (matches offenders + arrestees)\n\n\n2\nArizona\n\n\n\n\n\n\n\n\n3\nArkansas\n12,221\n12,221\n\n\n\nTotal only reported (no breakdown available)\n\n\n4\nCalifornia\n3,365,402\n3,365,402\n\n\n167,053\nTotal only reported (forensic reported separately)\n\n\n5\nColorado\n499,170\n499,170\n\n\n\nTotal only reported (no breakdown available)\n\n\n6\nConnecticut\n127,940\n127,940\n127,940\n\n23,200\nUsed reported total (matches offenders only)\n\n\n7\nDelaware\n\n\n\n0\n\n\n\n\n8\nFlorida\n1,713,890\n1,713,890\n\n\n\nTotal only reported (no breakdown available)\n\n\n9\nGeorgia\n347,145\n347,145\n324,864\n\n22,281\nTotal with discrepancy (calculation unclear)\n\n\n10\nHawaii\n\n\n\n0\n\n\n\n\n11\nIdaho\n39,000\n39,000\n39,000\n0\n\nUsed reported total (matches offenders only)\n\n\n12\nIllinois\n689,297\n689,297\n689,297\n\n69,927\nUsed reported total (matches offenders only)\n\n\n13\nIndiana\n449,000\n449,000\n304,000\n144,000\n22,400\nTotal with discrepancy (calculation unclear)\n\n\n14\nIowa\n\n\n\n0\n\n\n\n\n15\nKansas\n\n\n\n\n\n\n\n\n16\nKentucky\n\n\n\n0\n\n\n\n\n17\nLouisiana\n699,618\n699,618\n\n\n\nTotal only reported (no breakdown available)\n\n\n18\nMaine\n\n\n\n0\n\n\n\n\n19\nMaryland\n147,803\n\n142,278\n5,525\n\nCalculated from offenders + arrestees (no total reported)\n\n\n20\nMassachusetts\n\n\n\n0\n\n\n\n\n21\nMichigan\n\n\n\n\n\n\n\n\n22\nMinnesota\n180,000\n180,000\n180,000\n\n\nUsed reported total (matches offenders only)\n\n\n23\nMississippi\n118,000\n118,000\n\n\n\nTotal only reported (no breakdown available)\n\n\n24\nMissouri\n400,000\n400,000\n386,000\n\n\nTotal with discrepancy (calculation unclear)\n\n\n25\nMontana\n32,284\n32,284\n32,284\n0\n878\nUsed reported total (matches offenders only)\n\n\n26\nNebraska\n\n\n\n0\n\n\n\n\n27\nNevada\n245,000\n245,000\n\n\n\nTotal only reported (no breakdown available)\n\n\n28\nNew Hampshire\n\n\n\n0\n\n\n\n\n29\nNew Jersey\n32,696\n32,696\n\n\n\nTotal only reported (no breakdown available)\n\n\n30\nNew Mexico\n\n\n\n\n\n\n\n\n31\nNew York\n\n\n\n0\n\n\n\n\n32\nNorth Carolina\n350,000\n350,000\n300,000\n50,000\n\nUsed reported total (matches offenders + arrestees)\n\n\n33\nNorth Dakota\n\n\n\n\n\n\n\n\n34\nOhio\n\n\n\n\n\n\n\n\n35\nOklahoma\n\n\n\n\n\n\n\n\n36\nOregon\n\n\n\n0\n\n\n\n\n37\nPennsylvania\n\n\n\n0\n\n\n\n\n38\nRhode Island\n27,818\n27,818\n27,334\n484\n1,798\nUsed reported total (matches offenders + arrestees)\n\n\n39\nSouth Carolina\n175,629\n175,629\n\n\n11,127\nTotal only reported (forensic reported separately)\n\n\n40\nSouth Dakota\n56,200\n56,200\n\n\n\nTotal only reported (no breakdown available)\n\n\n41\nTennessee\n518,614\n518,614\n292,045\n226,569\n\nUsed reported total (matches offenders + arrestees)\n\n\n42\nTexas\n1,308,774\n1,308,774\n1,308,774\n\n\nUsed reported total (matches offenders only)\n\n\n43\nUtah\n\n\n\n\n\n\n\n\n44\nVermont\n\n\n\n0\n\n\n\n\n45\nVirginia\n520,131\n520,131\n\n\n\nTotal only reported (no breakdown available)\n\n\n46\nWashington\n272,000\n272,000\n272,000\n0\n8,800\nUsed reported total (matches offenders only)\n\n\n47\nWest Virginia\n47,444\n47,444\n47,444\n0\n3,905\nUsed reported total (matches offenders only)\n\n\n48\nWisconsin\n\n\n\n\n\n\n\n\n49\nWyoming\n\n\n\n0"
  },
  {
    "objectID": "analysis/sdis_summary.html#data-quality-resolution-and-detailed-discrepancy-analysis",
    "href": "analysis/sdis_summary.html#data-quality-resolution-and-detailed-discrepancy-analysis",
    "title": "SDIS Summary Analysis",
    "section": "Data Quality Resolution and Detailed Discrepancy Analysis",
    "text": "Data Quality Resolution and Detailed Discrepancy Analysis\nThis section provides a more detailed examination of data quality issues and implements fixes for identified problems.\n\n# Create a comprehensive quality report dataframe\nquality_report = has_components.copy()\n\n# Add calculated fields for analysis\nquality_report['diff_all'] = abs(quality_report['n_total'] - quality_report['sum_all'])\nquality_report['diff_forensic'] = abs(quality_report['n_total'] - quality_report['sum_forensic'])\nquality_report['pct_diff_all'] = (quality_report['diff_all'] / quality_report['n_total'] * 100).round(2)\nquality_report['pct_diff_forensic'] = (quality_report['diff_forensic'] / quality_report['n_total'] * 100).round(2)\n\n# Categorize data quality issues\nquality_report['issue_type'] = 'No issues'\n\n# Mark different types of issues\nquality_report.loc[\n    (quality_report['matches_combined_forensic'] == False) & \n    (quality_report['matches_combined'] == False), \n    'issue_type'\n] = 'Total mismatch'\n\nquality_report.loc[\n    (quality_report['arrestee_collection'] == 'yes') & \n    ((quality_report['n_arrestees'] == 0) | (quality_report['n_arrestees'].isna())), \n    'issue_type'\n] = 'Missing arrestee data'\n\nquality_report.loc[\n    ((quality_report['diff_all'] &gt; tolerance) & (quality_report['diff_all'] &lt; 1000)) |\n    ((quality_report['diff_forensic'] &gt; tolerance) & (quality_report['diff_forensic'] &lt; 1000)), \n    'issue_type'\n] = 'Small discrepancy'\n\n# Display states with small discrepancies with full details\nsmall_discrepancy_states = quality_report[quality_report['issue_type'] == 'Small discrepancy']\n\nif len(small_discrepancy_states) &gt; 0:\n    print(\"Detailed Analysis of States with Small Discrepancies:\")\n    print(\"=\" * 80)\n    \n    for _, state in small_discrepancy_states.iterrows():\n        print(f\"\\n{state['state']}:\")\n        print(f\"  • n_total: {state['n_total']:,.0f}\")\n        print(f\"  • n_arrestees: {state['n_arrestees']:,.0f}\" if pd.notna(state['n_arrestees']) else \"  • n_arrestees: missing\")\n        print(f\"  • n_offenders: {state['n_offenders']:,.0f}\" if pd.notna(state['n_offenders']) else \"  • n_offenders: missing\")\n        print(f\"  • n_forensic: {state['n_forensic']:,.0f}\" if pd.notna(state['n_forensic']) else \"  • n_forensic: missing\")\n        print(f\"  • Sum (arrestees + offenders): {state['sum_forensic']:,.0f}\")\n        print(f\"  • Sum (all components): {state['sum_all']:,.0f}\")\n        print(f\"  • Difference from (arrestees + offenders): {state['diff_forensic']:.0f} ({state['pct_diff_forensic']:.1f}%)\")\n        print(f\"  • Difference from all components: {state['diff_all']:.0f} ({state['pct_diff_all']:.1f}%)\")\n        print(f\"  • Arrestee collection policy: {state['arrestee_collection']}\")\n\n# Create a summary table of all quality issues\nprint(\"\\n\\nComprehensive Data Quality Summary:\")\nprint(\"=\" * 80)\n\nissue_summary = quality_report.groupby('issue_type').agg({\n    'state': 'count',\n    'n_total': 'sum'\n}).rename(columns={'state': 'count', 'n_total': 'total_profiles_affected'})\n\ndisplay(issue_summary.style.format({\n    'total_profiles_affected': '{:,.0f}'\n}))\n\n# Create a visual summary of data quality by state\nstates_with_issues = quality_report[quality_report['issue_type'] != 'No issues'][\n    ['state', 'issue_type', 'n_total', 'diff_forensic', 'diff_all', 'arrestee_collection']\n].sort_values('state')\n\nif len(states_with_issues) &gt; 0:\n    print(\"\\nStates Requiring Data Quality Attention:\")\n    display(states_with_issues.style.format({\n        'n_total': '{:,.0f}',\n        'diff_forensic': '{:,.0f}',\n        'diff_all': '{:,.0f}'\n    }).hide(axis='index'))\n\n# Proposed fixes for data quality issues\nprint(\"\\n\\nProposed Data Quality Fixes:\")\nprint(\"=\" * 80)\n\n# 1. States with missing arrestee data despite collection policy\nmissing_arrestee_states = quality_report[\n    (quality_report['arrestee_collection'] == 'yes') & \n    (quality_report['n_arrestees'].isna())\n]\n\nif len(missing_arrestee_states) &gt; 0:\n    print(\"\\n1. States collecting arrestee DNA but missing arrestee counts:\")\n    for _, state in missing_arrestee_states.iterrows():\n        print(f\"   • {state['state']}: Recommend requesting updated data from state SDIS administrator\")\n\n# 2. States with unexplained total calculations\nunexplained_states = quality_report[quality_report['issue_type'] == 'Total mismatch']\n\nif len(unexplained_states) &gt; 0:\n    print(\"\\n2. States with unexplained total calculations:\")\n    for _, state in unexplained_states.iterrows():\n        # Try to infer the most likely calculation method\n        if abs(state['n_total'] - state['sum_forensic']) &lt; abs(state['n_total'] - state['sum_all']):\n            likely_method = \"arrestees + offenders\"\n            missing = state['n_total'] - state['sum_forensic']\n        else:\n            likely_method = \"all components\"\n            missing = state['n_total'] - state['sum_all']\n        \n        print(f\"   • {state['state']}: n_total appears to use '{likely_method}' calculation\")\n        print(f\"     Missing/unaccounted profiles: {missing:,.0f}\")\n\n# 3. Recommended data cleaning steps\nprint(\"\\n3. Recommended Data Cleaning Steps:\")\nprint(\"   • Standardize calculation methodology across all states\")\nprint(\"   • Request clarification from states with &gt;5% discrepancies\")\nprint(\"   • Document state-specific calculation methods in metadata\")\nprint(\"   • Consider implementing automated validation checks for future data collection\")\n\n# Export problematic records for manual review\nif len(states_with_issues) &gt; 0:\n    # Save to CSV for manual review\n    output_path = base_dir / \"output\" / \"sdis\" / \"sdis_quality_issues.csv\"\n    states_with_issues.to_csv(output_path, index=False)\n    print(f\"\\n4. Exported {len(states_with_issues)} records with quality issues to: {output_path}\")\n\nDetailed Analysis of States with Small Discrepancies:\n================================================================================\n\nMontana:\n  • n_total: 32,284\n  • n_arrestees: 0\n  • n_offenders: 32,284\n  • n_forensic: 878\n  • Sum (arrestees + offenders): 32,284\n  • Sum (all components): 33,162\n  • Difference from (arrestees + offenders): 0 (0.0%)\n  • Difference from all components: 878 (2.7%)\n  • Arrestee collection policy: no\n\n\nComprehensive Data Quality Summary:\n================================================================================\n\n\n\n\n\n\n\n \ncount\ntotal_profiles_affected\n\n\nissue_type\n \n \n\n\n\n\nMissing arrestee data\n8\n6,594,187\n\n\nNo issues\n7\n1,334,022\n\n\nSmall discrepancy\n1\n32,284\n\n\nTotal mismatch\n1\n449,000\n\n\n\n\n\n\nStates Requiring Data Quality Attention:\n\n\n\n\n\n\n\nstate\nissue_type\nn_total\ndiff_forensic\ndiff_all\narrestee_collection\n\n\n\n\nCalifornia\nMissing arrestee data\n3,365,402\n3,365,402\n3,198,349\nyes\n\n\nConnecticut\nMissing arrestee data\n127,940\n0\n23,200\nyes\n\n\nGeorgia\nMissing arrestee data\n347,145\n22,281\n0\nyes\n\n\nIllinois\nMissing arrestee data\n689,297\n0\n69,927\nyes\n\n\nIndiana\nTotal mismatch\n449,000\n1,000\n21,400\nyes\n\n\nMinnesota\nMissing arrestee data\n180,000\n0\n0\nyes\n\n\nMissouri\nMissing arrestee data\n400,000\n14,000\n14,000\nyes\n\n\nMontana\nSmall discrepancy\n32,284\n0\n878\nno\n\n\nSouth Carolina\nMissing arrestee data\n175,629\n175,629\n164,502\nyes\n\n\nTexas\nMissing arrestee data\n1,308,774\n0\n0\nyes\n\n\n\n\n\n\n\nProposed Data Quality Fixes:\n================================================================================\n\n1. States collecting arrestee DNA but missing arrestee counts:\n   • California: Recommend requesting updated data from state SDIS administrator\n   • Connecticut: Recommend requesting updated data from state SDIS administrator\n   • Georgia: Recommend requesting updated data from state SDIS administrator\n   • Illinois: Recommend requesting updated data from state SDIS administrator\n   • Minnesota: Recommend requesting updated data from state SDIS administrator\n   • Missouri: Recommend requesting updated data from state SDIS administrator\n   • South Carolina: Recommend requesting updated data from state SDIS administrator\n   • Texas: Recommend requesting updated data from state SDIS administrator\n\n2. States with unexplained total calculations:\n   • Indiana: n_total appears to use 'arrestees + offenders' calculation\n     Missing/unaccounted profiles: 1,000\n\n3. Recommended Data Cleaning Steps:\n   • Standardize calculation methodology across all states\n   • Request clarification from states with &gt;5% discrepancies\n   • Document state-specific calculation methods in metadata\n   • Consider implementing automated validation checks for future data collection\n\n4. Exported 10 records with quality issues to: ../output/sdis/sdis_quality_issues.csv"
  },
  {
    "objectID": "analysis/sdis_summary.html#export-enhanced-dataset",
    "href": "analysis/sdis_summary.html#export-enhanced-dataset",
    "title": "SDIS Summary Analysis",
    "section": "Export Enhanced Dataset",
    "text": "Export Enhanced Dataset\nThis section exports the enhanced dataset with the new n_total_estimated values and documentation.\n\n# Prepare final dataset with key columns in logical order\nfinal_columns = [\n    'state', \n    'n_total_estimated',\n    'n_total_reported',\n    'n_arrestees', \n    'n_offenders', \n    'n_forensic',\n    'arrestee_collection',\n    'fam_search',\n    'collection_statute',\n    'n_total_estimated_comment',\n    'total_method'\n]\n\n# Select columns that exist in the dataset\navailable_columns = [col for col in final_columns if col in sdis_enhanced.columns]\nsdis_final = sdis_enhanced[available_columns].copy()\n\n# Export to CSV\noutput_path = base_dir / \"output\" / \"sdis\" / \"sdis_clean.csv\"\nsdis_final.to_csv(output_path, index=False)\nprint(f\"Exported enhanced SDIS dataset to: {output_path}\")\n\n# Display final summary statistics\nprint(\"\\n\\nFinal Summary Statistics:\")\nprint(\"=\" * 50)\nprint(f\"Total states in dataset: {len(sdis_final)}\")\nprint(f\"States with n_total_estimated: {sdis_final['n_total_estimated'].notna().sum()}\")\nprint(f\"States with n_total_reported: {sdis_final['n_total_reported'].notna().sum()}\")\nprint(f\"Total profiles (estimated): {sdis_final['n_total_estimated'].sum():,.0f}\")\n\n# Show comparison of reported vs estimated totals\ncomparison = sdis_final[sdis_final[['n_total_reported', 'n_total_estimated']].notna().all(axis=1)].copy()\ncomparison['difference'] = comparison['n_total_estimated'] - comparison['n_total_reported']\ncomparison_summary = comparison[comparison['difference'] != 0][['state', 'n_total_reported', 'n_total_estimated', 'difference', 'n_total_estimated_comment']]\n\nif len(comparison_summary) &gt; 0:\n    print(\"\\n\\nStates where estimated differs from reported total:\")\n    display(comparison_summary.style.format({\n        'n_total_reported': '{:,.0f}',\n        'n_total_estimated': '{:,.0f}',\n        'difference': '{:,.0f}'\n    }).hide(axis='index'))\n\nExported enhanced SDIS dataset to: ../output/sdis/sdis_clean.csv\n\n\nFinal Summary Statistics:\n==================================================\nTotal states in dataset: 50\nStates with n_total_estimated: 28\nStates with n_total_reported: 27\nTotal profiles (estimated): 12,814,222"
  }
]