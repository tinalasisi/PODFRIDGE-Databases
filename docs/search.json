[
  {
    "objectID": "qmd_root/sdis_summary.html",
    "href": "qmd_root/sdis_summary.html",
    "title": "SDIS Summary Analysis",
    "section": "",
    "text": "This analysis examines State DNA Index System (SDIS) data that includes information reported separately for each stateâ€™s DNA database. The data captures key dimensions including:\n\nTotal size of each stateâ€™s DNA database\nWhether states collect DNA from arrestees (not just convicted offenders)\nWhether states allow familial DNA searching\nReferences to relevant state statutes (from Murphy & Tong appendix)\n\nThis information provides insight into the variation in DNA database policies, practices, and legal frameworks across U.S. states."
  },
  {
    "objectID": "qmd_root/sdis_summary.html#overview",
    "href": "qmd_root/sdis_summary.html#overview",
    "title": "SDIS Summary Analysis",
    "section": "",
    "text": "This analysis examines State DNA Index System (SDIS) data that includes information reported separately for each stateâ€™s DNA database. The data captures key dimensions including:\n\nTotal size of each stateâ€™s DNA database\nWhether states collect DNA from arrestees (not just convicted offenders)\nWhether states allow familial DNA searching\nReferences to relevant state statutes (from Murphy & Tong appendix)\n\nThis information provides insight into the variation in DNA database policies, practices, and legal frameworks across U.S. states."
  },
  {
    "objectID": "qmd_root/sdis_summary.html#setup",
    "href": "qmd_root/sdis_summary.html#setup",
    "title": "SDIS Summary Analysis",
    "section": "Setup",
    "text": "Setup\nLoad necessary packages for the analysis.\n\n\nShow setup code\n# List of required packages\nrequired_packages &lt;- c(\n  \"tidyverse\",    # Data manipulation and visualization\n  \"readr\",      # Reading CSV files\n  \"dplyr\",      # Data manipulation\n  \"tidyr\",      # Data tidying\n  \"purrr\",      # Functional programming tools\n  \"ggplot2\",    # Data visualization\n  \"heatmaply\",  # Interactive heatmaps\n  \"kableExtra\", # Enhanced tables for reporting\n  \"DT\",         # Interactive tables\n  \"flextable\",  # Flexible tables for reporting\n  \"maps\",       # Mapping tools\n  \"here\",       # File path management\n  \"remotes\",    # To install urbnmapr package\n  \"sf\",\n  \"patchwork\"\n)\n\n# Function to install missing packages\ninstall_missing &lt;- function(packages) {\n  for (pkg in packages) {\n    if (!requireNamespace(pkg, quietly = TRUE)) {\n      message(paste(\"Installing missing package:\", pkg))\n      install.packages(pkg, dependencies = TRUE)\n    }\n  }\n}\n\n# Install any missing packages\ninstall_missing(required_packages)\n\n# Load all packages\nsuppressPackageStartupMessages({\n  library(tidyverse)\n  library(readr)\n  library(dplyr)\n  library(tidyr)\n  library(purrr)\n  library(ggplot2)\n  library(heatmaply)\n  library(kableExtra)\n  library(DT)\n  library(flextable)\n  library(maps)\n  library(here)\n  library(remotes)\n  library(sf)\n  library(patchwork)\n})\n\n# Verify all packages loaded successfully\nloaded_packages &lt;- sapply(required_packages, require, character.only = TRUE)\nif (all(loaded_packages)) {\n  message(\"ðŸ“š All packages loaded successfully!\")\n} else {\n  warning(\"The following packages failed to load: \", \n          paste(names(loaded_packages)[!loaded_packages], collapse = \", \"))\n}"
  },
  {
    "objectID": "qmd_root/sdis_summary.html#import-sdis-data",
    "href": "qmd_root/sdis_summary.html#import-sdis-data",
    "title": "SDIS Summary Analysis",
    "section": "Import SDIS data",
    "text": "Import SDIS data\nLoad the SDIS dataset and display a glimpse of its structure, including data types, missing values, and unique counts.\nDatabase Columns Definition\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\nstate\nName of the U.S. state or jurisdiction\n\n\nn_total\nTotal number of DNA profiles in the state database\n\n\nn_arrestees\nNumber of arrestee DNA profiles in the database\n\n\nn_offenders\nNumber of convicted offender DNA profiles in the database\n\n\nn_forensic\nNumber of forensic/crime scene DNA profiles in the database\n\n\narrestee_collection\nWhether the state collects DNA from arrestees (â€˜yesâ€™/â€˜noâ€™)\n\n\ncollection_statute\nCitation to the stateâ€™s DNA collection statute\n\n\nfam_search\nFamily search policy status (â€˜permittedâ€™, â€˜prohibitedâ€™, â€˜unspecifiedâ€™)\n\n\ndatabase_source\nURL source for the database statistics\n\n\ndatabase_source_year\nYear the database statistics were reported\n\n\nverification_comment\nAdditional notes or comments about data verification\n\n\n\n\n\nShow import code\n# Set up path to data file\ndata_file &lt;- file.path(here(\"data\", \"sdis\", \"raw\", \"sdis_raw.csv\"))\n\n# Load the SDIS data\nsdis_data &lt;- read_csv(data_file)\n\n# Display data types for each column\nenhanced_glimpse &lt;- function(df) {\n  glimpse_data &lt;- data.frame(\n    Column = names(df),\n    Type = sapply(df, function(x) paste(class(x), collapse = \", \")),\n    Rows = nrow(df),\n    Missing = sapply(df, function(x) sum(is.na(x))),\n    Unique = sapply(df, function(x) length(unique(x))),\n    First_Values = sapply(df, function(x) {\n      if(is.numeric(x)) {\n        paste(round(head(x, 3), 2), collapse = \", \")\n      } else {\n        paste(encodeString(head(as.character(x), 3)), collapse = \", \")\n      }\n    })\n  )\n  \n  ft &lt;- flextable(glimpse_data) %&gt;%\n    theme_zebra() %&gt;%\n    set_caption(paste(\"Enhanced Data Glimpse:\", deparse(substitute(df)))) %&gt;%\n    autofit() %&gt;%\n    align(align = \"left\", part = \"all\") %&gt;%\n    colformat_num(j = c(\"Rows\", \"Missing\", \"Unique\"), big.mark = \"\") %&gt;%\n    bg(j = \"Missing\", bg = function(x) ifelse(x &gt; 0, \"#FFF3CD\", \"transparent\")) %&gt;%\n    bg(j = \"Unique\", bg = function(x) ifelse(x == 1, \"#FFF3CD\", \"transparent\")) %&gt;%\n    add_footer_lines(paste(\"Data frame dimensions:\", nrow(df), \"rows Ã—\", ncol(df), \"columns\")) %&gt;%\n    fontsize(size = 10, part = \"all\") %&gt;%\n    set_table_properties(layout = \"autofit\", width = 1)\n  \n  return(ft)\n}\n\nenhanced_glimpse(sdis_data)\n\n\nColumnTypeRowsMissingUniqueFirst_Valuesstatecharacter50050Alabama, Alaska, Arizonan_totalnumeric502328360000, 79146, NAn_arresteesnumeric50278NA, 52149, NAn_offendersnumeric503417NA, 26997, NAn_forensicnumeric503912NA, 3866, NAarrestee_collectioncharacter5002yes, yes, yescollection_statutecharacter50050Ala. Code Sec.36-18-25, AK Stat Sec.44.41.035 (2014), AZ Rev Stat Sec.13-610 (2016)fam_searchcharacter5003unspecified, unspecified, unspecifieddatabase_sourcecharacter502031https://adfs.alabama.gov/services/fb/fb-statistics, https://dps.alaska.gov/Statewide/CrimeLab/Forensic-Biology/DNA, &lt;NA&gt;database_source_yearnumeric5020112024, 2024, NAverification_commentcharacter503516Alabama Department of Forensic Sciences notes that as of JanuaryÂ 2024 the state DNA database contains about 360,000 convicted offender and arrestee profileshttps://adfs.alabama.gov/services/fb/fb-statistics#:~:text=The%20DNA%20Databank%20laboratory%20receives,DNA%20profiles%20are%20being%20searched., Alaska DPS crimeâ€‘lab CODIS metrics (FebÂ 2024) list 52,149 arrestee profiles, 26,997 convicted offender profiles and 3,866 forensic profiles in the Alaska state DNA databasehttps://dps.alaska.gov/Statewide/CrimeLab/Forensic-Biology/DNA#:~:text=CODIS%20Metrics%20as%20of%C2%A0February%202024,Offenders%20in%20database%2026%2C997%2015%2C268%2C774., &lt;NA&gt;Data frame dimensions: 50 rows Ã— 11 columns"
  },
  {
    "objectID": "qmd_root/sdis_summary.html#arrestee-collection-information",
    "href": "qmd_root/sdis_summary.html#arrestee-collection-information",
    "title": "SDIS Summary Analysis",
    "section": "Arrestee Collection Information",
    "text": "Arrestee Collection Information\nIn the United States, the collection of DNA from individuals upon arrest, prior to any conviction, is a significant law enforcement practice with complex legal and ethical dimensions.\nThe following map and table illustrate the current patchwork of state laws governing this practice.\n\n\nShow arrestee collection visualization code\n# Create summary data for arrestee collection\narrestee_summary &lt;- sdis_data %&gt;%\n  group_by(arrestee_collection) %&gt;%\n  summarise(\n    count = n(),\n    states = list(state),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(desc(count))\n\n# Get US state map data\nus_states &lt;- map_data(\"state\")\n\n# Prepare map data\nmap_data &lt;- sdis_data %&gt;%\n  mutate(region = tolower(state)) %&gt;%\n  right_join(us_states, by = \"region\") %&gt;%\n  filter(!is.na(arrestee_collection))\n\n# Option 1: US Map Visualization\nmap_plot &lt;- ggplot(map_data, aes(x = long, y = lat, group = group, fill = arrestee_collection)) +\n  geom_polygon(color = \"white\", linewidth = 0.2) +\n  coord_fixed(1.3) +\n  scale_fill_manual(\n    values = c(\n      \"yes\" = \"#0072B2\",      # Blue for collecting states\n      \"no\" = \"#666666\"       # Gray for non-collecting states\n    ),\n    name = \"Collects DNA from Arrestees\",\n    labels = c(\"No\", \"Yes\")\n  ) +\n  labs(\n    title = \"DNA Collection from Arrestees by State\",\n    subtitle = \"States that collect DNA samples from individuals upon arrest\"\n  ) +\n  theme_void() +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5, size = 16),\n    plot.subtitle = element_text(hjust = 0.5, color = \"gray40\", size = 12),\n    legend.position = \"right\",\n    legend.title = element_text(face = \"bold\")\n  )\n\n# Create detailed summary table\nsummary_table &lt;- arrestee_summary %&gt;%\n  mutate(\n    state_list = map_chr(states, ~paste(.x, collapse = \", \")),\n    percentage = round(count / sum(count) * 100, 1)\n  ) %&gt;%\n  select(Status = arrestee_collection, Count = count, Percentage = percentage, States = state_list)\n\n# Display the visualizations\nprint(map_plot)\n\n\n\n\n\n\n\n\n\nShow arrestee collection visualization code\n# Create a nice flextable for the report\nft &lt;- flextable(summary_table) %&gt;%\n  theme_zebra() %&gt;%\n  set_caption(\"Arrestee DNA Collection Status by State\") %&gt;%\n  autofit() %&gt;%\n  align(align = \"left\", part = \"all\") %&gt;%\n  bg(j = \"Status\", bg = function(x) ifelse(x == \"yes\", \"#1f77b4\", ifelse(x == \"no\", \"#666666\", \"#CCCCCC\"))) %&gt;%\n  color(j = \"Status\", color = \"white\") %&gt;%\n  bold(j = \"Status\") %&gt;%\n  fontsize(size = 11, part = \"all\")\n\nft\n\n\nStatusCountPercentageStatesyes3366Alabama, Alaska, Arizona, Arkansas, California, Colorado, Connecticut, Florida, Georgia, Illinois, Indiana, Kansas, Louisiana, Maryland, Michigan, Minnesota, Mississippi, Missouri, Nevada, New Jersey, New Mexico, North Carolina, North Dakota, Ohio, Oklahoma, Rhode Island, South Carolina, South Dakota, Tennessee, Texas, Utah, Virginia, Wisconsinno1734Delaware, Hawaii, Idaho, Iowa, Kentucky, Maine, Massachusetts, Montana, Nebraska, New Hampshire, New York, Oregon, Pennsylvania, Vermont, Washington, West Virginia, Wyoming\n\n\n\nAdjusting n_arrestees\n\nAdjust the dataset to ensure consistency in arrestee DNA collection reporting.\nSetting n_arrestees to 0 for states that do not collect arrestee DNA, even if they report non-zero values.\nRemoved the arrestee_colection column after adjustments were made.\n\n\n\nShow pre-processing code\n# Create a copy of the data for processing\nsdis_data_processed &lt;- sdis_data\n\n# Count states affected by this adjustment\nstates_with_no_collection &lt;- sdis_data_processed %&gt;% \n  filter(arrestee_collection == 'no')\n\nstates_to_adjust &lt;- states_with_no_collection %&gt;% \n  filter(!is.na(n_arrestees) & n_arrestees != 0)\n\nif (nrow(states_to_adjust) &gt; 0) {\n  cat(\"States with arrestee_collection='no' but non-zero n_arrestees values:\\n\")\n  for (i in 1:nrow(states_to_adjust)) {\n    state &lt;- states_to_adjust[i, ]\n    cat(paste0(\"  â€¢ \", state$state, \": n_arrestees = \", format(state$n_arrestees, big.mark = \",\"), \"\\n\"))\n  }\n}\n\n# Set n_arrestees to 0 for states that don't collect arrestee DNA\nsdis_data_processed &lt;- sdis_data_processed %&gt;%\n  mutate(n_arrestees = ifelse(arrestee_collection == 'no', 0, n_arrestees))\n\n# Use processed data for all subsequent analyses\nsdis_data &lt;- sdis_data_processed"
  },
  {
    "objectID": "qmd_root/sdis_summary.html#family-search-policy-status",
    "href": "qmd_root/sdis_summary.html#family-search-policy-status",
    "title": "SDIS Summary Analysis",
    "section": "Family Search Policy Status",
    "text": "Family Search Policy Status\nFamilial DNA searching is an advanced forensic technique that uses partial DNA matches to identify potential relatives of an unknown suspect in a criminal database.\nThe following visualization maps the current patchwork of state policies (â€˜permittedâ€™, â€˜prohibitedâ€™, â€˜unspecifiedâ€™).\n\n\nShow family search policy visualization code\n# Create summary data for family search policies\nfamily_search_summary &lt;- sdis_data %&gt;%\n  mutate(\n    fam_search = factor(fam_search, \n                       levels = c(\"permitted\", \"prohibited\", \"unspecified\"),\n                       labels = c(\"Permitted\", \"Prohibited\", \"Unspecified\")),\n    fam_search_simple = case_when(\n      fam_search == \"Permitted\" ~ \"Permitted\",\n      fam_search == \"Prohibited\" ~ \"Prohibited\",\n      TRUE ~ \"Unspecified/No Policy\"\n    )\n  ) %&gt;%\n  group_by(fam_search, fam_search_simple) %&gt;%\n  summarise(\n    count = n(),\n    states = list(state),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(desc(count))\n\n# Get complete data for mapping (include states with missing data)\nall_states_map &lt;- map_data(\"state\") %&gt;%\n  as_tibble() %&gt;%\n  distinct(region) %&gt;%\n  mutate(\n    state_name = str_to_title(region),\n    has_data = state_name %in% sdis_data$state\n  )\n\n# Prepare map data for family search policies\nfamily_search_map_data &lt;- sdis_data %&gt;%\n  mutate(\n    region = tolower(state)) %&gt;%\n  right_join(map_data(\"state\"), by = c(\"region\" = \"region\")) %&gt;%\n  filter(!is.na(fam_search))\n\n# Option 1: US Map Visualization\nfamily_map_plot &lt;- ggplot(family_search_map_data, \n                         aes(x = long, y = lat, group = group, fill = fam_search)) +\n  geom_polygon(color = \"white\", linewidth = 0.2) +\n  coord_fixed(1.3) +\n  scale_fill_manual(\n    values = c(\n      \"permitted\" = \"#1f77b4\",    # Blue for permitted\n      \"prohibited\" = \"#000000ff\",   # Black for prohibited\n      \"unspecified\" = \"#666666\"   # Gray for unspecified\n    ),\n    name = \"Familial Search Policy\",\n    drop = FALSE\n  ) +\n  labs(\n    title = \"Familial DNA Search Policies by State\",\n    subtitle = \"State policies regarding familial DNA searching in databases\",\n    caption = \"Source: SDIS Database Analysis\"\n  ) +\n  theme_void() +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5, size = 16),\n    plot.subtitle = element_text(hjust = 0.5, color = \"gray40\", size = 12),\n    legend.position = \"right\",\n    legend.title = element_text(face = \"bold\")\n  )\n\n# Display the visualizations\nprint(family_map_plot)\n\n\n\n\n\n\n\n\n\nShow family search policy visualization code\n# Create a comprehensive summary table\nsummary_table_data &lt;- family_search_summary %&gt;%\n  mutate(\n    state_list = map_chr(states, ~paste(.x, collapse = \", \")),\n    percentage = round(count / sum(count) * 100, 1)\n  ) %&gt;%\n  select(Policy = fam_search, Count = count, Percentage = percentage, States = state_list)\n\n# Create a nice flextable for the report\nft &lt;- flextable(summary_table_data) %&gt;%\n  theme_zebra() %&gt;%\n  set_caption(\"Familial DNA Search Policy Status by State\") %&gt;%\n  autofit() %&gt;%\n  align(align = \"center\", part = \"all\") %&gt;%\n  align(align = \"left\", j = \"States\") %&gt;%\n  bg(j = \"Policy\", bg = function(x) {\n    case_when(\n      x == \"Permitted\" ~ \"#1f77b4\",\n      x == \"Prohibited\" ~ \"#000000ff\",\n      TRUE ~ \"#666666\"\n    )\n  }) %&gt;%\n  color(j = \"Policy\", color = \"white\") %&gt;%\n  bold(j = \"Policy\") %&gt;%\n  fontsize(size = 11, part = \"all\") %&gt;%\n  width(j = \"States\", width = 3)\n\nft\n\n\nPolicyCountPercentageStatesUnspecified3774Alabama, Alaska, Arizona, Connecticut, Delaware, Georgia, Hawaii, Idaho, Illinois, Indiana, Iowa, Kansas, Kentucky, Louisiana, Maine, Massachusetts, Minnesota, Mississippi, Missouri, Nebraska, Nevada, New Hampshire, New Jersey, New Mexico, North Carolina, North Dakota, Ohio, Oklahoma, Oregon, Pennsylvania, Rhode Island, South Carolina, South Dakota, Tennessee, Vermont, Washington, West VirginiaPermitted1224Arkansas, California, Colorado, Florida, Michigan, Montana, New York, Texas, Utah, Virginia, Wisconsin, WyomingProhibited12Maryland"
  },
  {
    "objectID": "qmd_root/sdis_summary.html#three-panel-policy-map-horizontal-layout",
    "href": "qmd_root/sdis_summary.html#three-panel-policy-map-horizontal-layout",
    "title": "SDIS Summary Analysis",
    "section": "Three-Panel Policy Map: Horizontal Layout",
    "text": "Three-Panel Policy Map: Horizontal Layout\nThis visualization presents three key policy dimensions side-by-side: arrestee DNA collection, familial search policies, and FOIA data availability.\n\n\nShow three-panel map code\nremotes::install_github(\"UrbanInstitute/urbnmapr\")\nlibrary(urbnmapr)\n\n# Create FOIA availability column\nsdis_data &lt;- sdis_data %&gt;%\n  mutate(\n    foia_availability = case_when(\n      state %in% c(\"California\", \"Florida\", \"Indiana\", \"Maine\", \n                   \"Nevada\", \"South Dakota\", \"Texas\") ~ \"provided\",\n      TRUE ~ \"not_provided\"\n    )\n  )\n\n# Load US map data with proper AK/HI positioning\nus_map_data &lt;- get_urbn_map(\"states\", sf = TRUE)\n\n# Prepare map data\nsdis_data_map &lt;- sdis_data %&gt;%\n  mutate(region = tolower(state))\n\nus_map_data &lt;- us_map_data %&gt;%\n  mutate(region = tolower(state_name))\n\n# Merge polygon data with policy data\nmap_data_repos &lt;- us_map_data %&gt;%\n  left_join(sdis_data_map, by = \"region\")\n\n# Function to create individual maps\ncreate_policy_map &lt;- function(fill_var, fill_title, fill_colors, plot_title) {\n  ggplot(map_data_repos) +\n    geom_sf(aes(fill = !!sym(fill_var)), color = \"white\", linewidth = 0.3) +\n    scale_fill_manual(\n      name = fill_title,\n      values = fill_colors,\n      na.value = \"gray90\",\n      na.translate = FALSE,\n      guide = guide_legend(\n        direction = \"vertical\",\n        title.position = \"top\",\n        title.hjust = 0.5\n      )\n    ) +\n    theme_void() +\n    labs(title = plot_title) +\n    theme(\n      plot.title = element_text(size = 25, face = \"bold\", hjust = 0.5, margin = margin(b = 5)),\n      legend.position = \"bottom\",\n      legend.title = element_text(size = 1, face = \"bold\"),\n      legend.text = element_text(size = 25),\n      legend.box = \"vertical\",\n      legend.margin = margin(t = 5, b = 0)\n    )\n}\n\n# Create the three maps\nmap_arrestee_repos &lt;- create_policy_map(\n  \"arrestee_collection\", \" \",\n  c(\"yes\" = \"#1f77b4\", \"no\" = \"#666666\"), \n  \"A. Arrestee DNA Collection Policy\"\n)\n\nmap_familial_repos &lt;- create_policy_map(\n  \"fam_search\", \" \",\n  c(\"permitted\" = \"#1f77b4\", \"prohibited\" = \"#000000ff\", \"unspecified\" = \"#666666\"),\n  \"B. Familial Search Policy\"\n)\n\nmap_foia_repos &lt;- create_policy_map(\n  \"foia_availability\", \" \",\n  c(\"provided\" = \"#1f77b4\", \"not_provided\" = \"#666666\"),\n  \"C. FOIA Response Status\"\n)\n\n# Combine maps horizontally\nfigure_combined &lt;- map_arrestee_repos | map_familial_repos | map_foia_repos\n\nfigure_combined + plot_annotation(\n  title = \" \",\n  theme = theme(plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5))\n)\n\n\n\n\n\n\n\n\n\nShow three-panel map code\n# Compute summary counts and percentages for each category\npolicy_summary &lt;- sdis_data %&gt;%\n  summarise(\n    `Arrestee Collection - Yes` = sum(arrestee_collection == \"yes\", na.rm = TRUE),\n    `Arrestee Collection - No` = sum(arrestee_collection == \"no\", na.rm = TRUE),\n    `Familial Search - Permitted` = sum(fam_search == \"permitted\", na.rm = TRUE),\n    `Familial Search - Prohibited` = sum(fam_search == \"prohibited\", na.rm = TRUE),\n    `Familial Search - Unspecified` = sum(fam_search == \"unspecified\", na.rm = TRUE),\n    `FOIA - Provided` = sum(foia_availability == \"provided\", na.rm = TRUE),\n    `FOIA - Not Provided` = sum(foia_availability == \"not_provided\", na.rm = TRUE)\n  ) %&gt;%\n  pivot_longer(everything(), names_to = \"Metric\", values_to = \"Count\") %&gt;%\n  mutate(\n    Group = case_when(\n      grepl(\"Arrestee Collection\", Metric) ~ \"Arrestee Collection\",\n      grepl(\"Familial Search\", Metric) ~ \"Familial Search\",\n      grepl(\"FOIA\", Metric) ~ \"FOIA Availability\"\n    ),\n    Percentage = round(Count / 50 * 100, 1)\n  )\n\n# Split into subtables by group\npolicy_tables &lt;- policy_summary %&gt;%\n  group_split(Group)\n\n# Create function to build consistent flextables\nmake_policy_table &lt;- function(df) {\n  flextable(df %&gt;% select(Metric, Count, Percentage)) %&gt;%\n    theme_zebra() %&gt;%\n    set_caption(paste0(unique(df$Group), \" (n = 50 states)\")) %&gt;%\n    colformat_num(j = \"Percentage\", suffix = \"%\") %&gt;%\n    align(align = \"left\", part = \"all\") %&gt;%\n    bold(j = \"Metric\") %&gt;%\n    fontsize(size = 10, part = \"all\") %&gt;%\n    autofit()\n}\n\n# Build three subtables\nft_1 &lt;- make_policy_table(policy_tables[[1]])\nft_2 &lt;- make_policy_table(policy_tables[[2]])\nft_3 &lt;- make_policy_table(policy_tables[[3]])\n\nft_1\n\nft_2\n\nft_3\n\n\nMetricCountPercentageArrestee Collection - Yes3366%Arrestee Collection - No1734%\nMetricCountPercentageFOIA - Provided714%FOIA - Not Provided4386%\nMetricCountPercentageFamilial Search - Permitted1224%Familial Search - Prohibited12%Familial Search - Unspecified3774%"
  },
  {
    "objectID": "qmd_root/sdis_summary.html#data-availability-overview",
    "href": "qmd_root/sdis_summary.html#data-availability-overview",
    "title": "SDIS Summary Analysis",
    "section": "Data Availability Overview",
    "text": "Data Availability Overview\nAssess the completeness of data across states, including which states are missing and the availability of key fields.\n\n\nShow overview code\n# Identify states present in the dataset\nstates_in_data &lt;- unique(sdis_data$state) %&gt;% sort()\n\n# Check if all 50 states are represented\nall_states &lt;- c('Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut',\n                'Delaware', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa',\n                'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan',\n                'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire',\n                'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio',\n                'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina', 'South Dakota',\n                'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia',\n                'Wisconsin', 'Wyoming')\n\nmissing_states &lt;- setdiff(all_states, states_in_data)\nif (length(missing_states) &gt; 0) {\n  cat(paste(\"\\nMissing states:\", paste(missing_states, collapse = \", \"), \"\\n\"))\n} else {\n  cat(\"\\nAll 50 states are represented in the dataset\\n\")\n}\n\n# Assess data completeness for each state\n# Improved heatmap with enhanced visualization\ndata_availability &lt;- sdis_data %&gt;%\n  group_by(state) %&gt;%\n  summarise(across(everything(), ~sum(!is.na(.)))) %&gt;%\n  select(-state) %&gt;%\n  as.data.frame()\n\nrownames(data_availability) &lt;- unique(sdis_data$state)\n\n# Generate visualization of data completeness\nkey_fields &lt;- c('n_total', 'n_arrestees', 'n_offenders', 'n_forensic')\n\n# Filter to include only key fields that exist in the data\navailable_key_fields &lt;- intersect(key_fields, names(data_availability))\n\nif (length(available_key_fields) &gt; 0) {\n  availability_subset &lt;- data_availability[, available_key_fields, drop = FALSE]\n  availability_binary &lt;- as.data.frame(ifelse(availability_subset &gt; 0, 1, 0))\n  \n  # Create heatmap data in long format\n  heatmap_long &lt;- availability_binary %&gt;%\n    rownames_to_column(\"state\") %&gt;%\n    pivot_longer(cols = -state, names_to = \"field\", values_to = \"available\") %&gt;%\n    mutate(\n      state = factor(state, levels = rownames(availability_binary)),\n      state = factor(state, levels = sort(unique(state), decreasing = TRUE)),\n      field = factor(field, levels = available_key_fields),\n      # Create labels with different symbols\n      label = case_when(\n        available == 1 ~ \"âœ“\",\n        available == 0 ~ \"!\",\n        TRUE ~ \"\"\n      ),\n      # Color coding for different values\n      fill_color = case_when(\n        available == 1 ~ \"Available\",\n        available == 0 ~ \"Missing\",\n        TRUE ~ \"Unknown\"\n      )\n    )\n  \n  # Create the enhanced heatmap\nggplot(heatmap_long, aes(x = field, y = state, fill = fill_color)) +\n  geom_tile(color = \"white\", linewidth = 0.8, width = 0.9, height = 0.9) +\n  geom_text(aes(label = label, color = fill_color), \n            size = 4, fontface = \"bold\", vjust = 0.8) +\n  scale_fill_manual(values = c(\n    \"Available\" = \"#2E86AB\",\n    \"Missing\" = \"#FF6B6B\",\n    \"Unknown\" = \"#f0f0f0\"\n  )) +\n  scale_color_manual(values = c(\n    \"Available\" = \"white\",\n    \"Missing\" = \"white\",\n    \"Unknown\" = \"gray30\"\n  )) +\n  labs(\n    title = \"Data Availability Heatmap by State\",\n    subtitle = \"âœ“ = Data available | ! = Data missing (0 values)\",\n    x = NULL,\n    y = NULL,\n    fill = \"Data Status\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    axis.text.x.top = element_text(angle = 315, hjust = 0.5, face = \"bold\"), \n    axis.text.y = element_text(face = \"bold\"),\n    axis.title.y = element_text(face = \"bold\", vjust = 0),\n    plot.title = element_text(face = \"bold\", hjust = 0.5, size = 16),\n    plot.subtitle = element_text(hjust = 0.5, color = \"gray40\", size = 10),\n    panel.grid = element_blank(),\n    legend.position = \"top\",\n    legend.title = element_text(face = \"bold\"),\n    axis.ticks.x = element_line(),\n    axis.ticks.x.top = element_line()\n  ) +\n  guides(color = \"none\") +\n  coord_fixed() +\n  scale_x_discrete(position = \"top\")\n  \n} else {\n  message(\"No key fields available for heatmap visualization\")\n}\n\n\n\n\n\n\n\n\n\nShow overview code\n# Data coverage summary\ncat(\"\\nData field coverage across states:\\n\")\n\nkey_fields &lt;- c('n_total', 'n_arrestees', 'n_offenders', 'n_forensic', 'fam_search', 'collection_statute')\n\nfor (col in key_fields) {\n  if (col %in% names(sdis_data)) {\n    states_with_data &lt;- sum(!is.na(sdis_data[[col]]))\n    coverage_pct &lt;- states_with_data / length(states_in_data) * 100\n    cat(paste0(col, \": \", states_with_data, \" states (\", round(coverage_pct, 1), \"%)\\n\"))\n  }\n}\n\n\n\nAll 50 states are represented in the dataset\n\nData field coverage across states:\nn_total: 27 states (54%)\nn_arrestees: 23 states (46%)\nn_offenders: 16 states (32%)\nn_forensic: 11 states (22%)\nfam_search: 50 states (100%)\ncollection_statute: 50 states (100%)"
  },
  {
    "objectID": "qmd_root/sdis_summary.html#total-profile-calculations-verification",
    "href": "qmd_root/sdis_summary.html#total-profile-calculations-verification",
    "title": "SDIS Summary Analysis",
    "section": "Total Profile Calculations Verification",
    "text": "Total Profile Calculations Verification\nThis section examines states reporting n_total alongside component counts to determine whether totals represent:\n\nSum of all profile types including forensic (n_arrestees + n_offenders + n_forensic)\nSum of combined profiles only (n_arrestees + n_offenders)\n\n\n\nShow calculations code\n# Identify states with n_total and at least one component count\nstates_with_totals &lt;- sdis_data %&gt;%\n  filter(!is.na(n_total)) %&gt;%\n  mutate(\n    sum_all = rowSums(select(., n_arrestees, n_offenders, n_forensic), na.rm = TRUE),\n    sum_arrestees_offenders = rowSums(select(., n_arrestees, n_offenders), na.rm = TRUE),\n    # Check if all components are available\n    all_components_available = !is.na(n_arrestees) & !is.na(n_offenders) & !is.na(n_forensic),\n    both_arrestees_offenders = !is.na(n_arrestees) & !is.na(n_offenders)\n  )\n\n# Check matches with tolerance\ntolerance &lt;- 10\n\nstates_with_totals &lt;- states_with_totals %&gt;%\n  mutate(\n    matches_combined_forensic = ifelse(\n      all_components_available,\n      abs(n_total - sum_all) &lt;= tolerance,\n      FALSE\n    ),\n    matches_combined = ifelse(\n      both_arrestees_offenders,\n      (abs(n_total - sum_arrestees_offenders) &lt;= tolerance) & \n        (n_arrestees &gt; 0),\n      FALSE\n    )\n  )\n\n# Filter to states with at least one component count\nhas_components &lt;- states_with_totals %&gt;%\n  filter(!is.na(n_arrestees) | !is.na(n_offenders) | !is.na(n_forensic))\n\ncat(paste(\"States with n_total and component data:\", nrow(has_components), \"\\n\"))\ncat(\"\\nTotal calculation patterns:\\n\")\n\n# Categorize states\nincludes_all &lt;- has_components %&gt;% filter(matches_combined_forensic) %&gt;% pull(state)\nforensic_only &lt;- has_components %&gt;% filter(matches_combined & !matches_combined_forensic) %&gt;% pull(state)\nneither &lt;- has_components %&gt;% filter(!matches_combined_forensic & !matches_combined) %&gt;% pull(state)\n\nif (length(includes_all) &gt; 0) {\n  cat(\"\\nn_total matches combined profiles with forensic (arrestees + offenders + forensic):\\n\")\n  for (state in includes_all) {\n    cat(paste(\"  â€¢\", state, \"\\n\"))\n  }\n} else {\n  cat(\"\\nStates where n_total matches the sum of arrestees + offenders + forensic: None\\n\")\n}\n\nif (length(forensic_only) &gt; 0) {\n  cat(\"\\nStates where n_total includes combined profiles only (arrestees + offenders):\\n\")\n  for (state in forensic_only) {\n    cat(paste(\"  â€¢\", state, \"\\n\"))\n  }\n}\n\nif (length(neither) &gt; 0) {\n  cat(\"\\nStates where n_total does not match calculated sums:\\n\")\n  for (state in neither) {\n    state_data &lt;- has_components %&gt;% filter(state == !!state)\n    cat(paste0(\"  â€¢ \", state, \":\\n n_total=\", format(state_data$n_total, big.mark = \",\"), \n               \"\\n Sum_all=\", format(state_data$sum_all, big.mark = \",\"),\n               \"\\n Sum_arrestees_offenders =\", format(state_data$sum_arrestees_offenders, big.mark = \",\"), \"\\n\\n\"))\n  }\n}\n\n# Display detailed breakdown\ncat(\"\\nDetailed breakdown:\\n\")\nhas_components %&gt;%\n  select(state, n_total, n_arrestees, n_offenders, n_forensic, matches_combined_forensic, matches_combined) %&gt;%\n  mutate(across(where(is.numeric), ~ifelse(is.na(.), \"\", format(., big.mark = \",\")))) %&gt;%\n  flextable()\n\n\nStates with n_total and component data: 17 \n\nTotal calculation patterns:\n\nStates where n_total matches the sum of arrestees + offenders + forensic: None\n\nStates where n_total includes combined profiles only (arrestees + offenders):\n  â€¢ Alaska \n  â€¢ North Carolina \n  â€¢ Rhode Island \n  â€¢ Tennessee \n\nStates where n_total does not match calculated sums:\n  â€¢ California:\n n_total=3,365,402\n Sum_all=167,053\n Sum_arrestees_offenders =0\n\n  â€¢ Connecticut:\n n_total=127,940\n Sum_all=151,140\n Sum_arrestees_offenders =127,940\n\n  â€¢ Georgia:\n n_total=347,145\n Sum_all=347,145\n Sum_arrestees_offenders =324,864\n\n  â€¢ Idaho:\n n_total=39,000\n Sum_all=39,000\n Sum_arrestees_offenders =39,000\n\n  â€¢ Illinois:\n n_total=689,297\n Sum_all=759,224\n Sum_arrestees_offenders =689,297\n\n  â€¢ Indiana:\n n_total=449,000\n Sum_all=470,400\n Sum_arrestees_offenders =448,000\n\n  â€¢ Minnesota:\n n_total=180,000\n Sum_all=180,000\n Sum_arrestees_offenders =180,000\n\n  â€¢ Missouri:\n n_total=4e+05\n Sum_all=386,000\n Sum_arrestees_offenders =386,000\n\n  â€¢ Montana:\n n_total=32,284\n Sum_all=33,162\n Sum_arrestees_offenders =32,284\n\n  â€¢ South Carolina:\n n_total=175,629\n Sum_all=11,127\n Sum_arrestees_offenders =0\n\n  â€¢ Texas:\n n_total=1,308,774\n Sum_all=1,308,774\n Sum_arrestees_offenders =1,308,774\n\n  â€¢ Washington:\n n_total=272,000\n Sum_all=280,800\n Sum_arrestees_offenders =272,000\n\n  â€¢ West Virginia:\n n_total=47,444\n Sum_all=51,349\n Sum_arrestees_offenders =47,444\n\n\nDetailed breakdown:\n\n\nstaten_totaln_arresteesn_offendersn_forensicmatches_combined_forensicmatches_combinedAlaska   79,146 52,149   26,997  3,866falsetrueCalifornia3,365,402167,053falsefalseConnecticut  127,940  127,940 23,200falsefalseGeorgia  347,145  324,864 22,281falsefalseIdaho   39,000      0   39,000falsefalseIllinois  689,297  689,297 69,927falsefalseIndiana  449,000144,000  304,000 22,400falsefalseMinnesota  180,000  180,000falsefalseMissouri  400,000  386,000falsefalseMontana   32,284      0   32,284    878falsefalseNorth Carolina  350,000 50,000  300,000falsetrueRhode Island   27,818    484   27,334  1,798falsetrueSouth Carolina  175,629 11,127falsefalseTennessee  518,614226,569  292,045falsetrueTexas1,308,7741,308,774falsefalseWashington  272,000      0  272,000  8,800falsefalseWest Virginia   47,444      0   47,444  3,905falsefalse"
  },
  {
    "objectID": "qmd_root/sdis_summary.html#analysis-of-database-totals-and-data-quality-issues",
    "href": "qmd_root/sdis_summary.html#analysis-of-database-totals-and-data-quality-issues",
    "title": "SDIS Summary Analysis",
    "section": "Analysis of Database Totals and Data Quality Issues",
    "text": "Analysis of Database Totals and Data Quality Issues\nExamine states where N_total values reveal potential data quality issues or reporting inconsistencies.\n\n\nShow data quality assessment code\n# Create enhanced data quality analysis\nsdis_enhanced &lt;- sdis_data %&gt;%\n  rename(n_total_reported = n_total)\n\n# Calculate different total relationships with small tolerance\ntolerance &lt;- 10\n\nsdis_enhanced &lt;- sdis_enhanced %&gt;%\n  mutate(\n    total_equals_offenders = ifelse(\n      !is.na(n_total_reported) & !is.na(n_offenders) & n_offenders &gt; 0,\n      abs(n_total_reported - n_offenders) &lt;= tolerance,\n      FALSE\n    ),\n    total_equals_off_arr = ifelse(\n      !is.na(n_total_reported) & !is.na(n_offenders) & !is.na(n_arrestees) &\n        n_offenders &gt; 0 & n_arrestees &gt; 0,\n      abs(n_total_reported - (n_offenders + n_arrestees)) &lt;= tolerance,\n      FALSE\n    ),\n    total_equals_all = ifelse(\n      !is.na(n_total_reported) & !is.na(n_offenders) & !is.na(n_arrestees) & \n        !is.na(n_forensic) & n_offenders &gt; 0 & n_arrestees &gt; 0 & n_forensic &gt; 0,\n      abs(n_total_reported - (n_offenders + n_arrestees + n_forensic)) &lt;= tolerance,\n      FALSE\n    ),\n    total_method = case_when(\n      total_equals_all ~ \"All components\",\n      total_equals_off_arr ~ \"Offenders + Arrestees\",\n      total_equals_offenders ~ \"Offenders only\",\n      TRUE ~ \"Unknown\"\n    )\n  )\n\n# Create n_total_estimated based on the rules specified\nsdis_enhanced &lt;- sdis_enhanced %&gt;%\n  mutate(\n    n_total_estimated = NA_real_,\n    n_total_estimated_comment = \"\"\n  )\n\n# Rule 1: States where n_total == n_offenders + n_arrestees\nmask_off_arr &lt;- sdis_enhanced$total_equals_off_arr\nsdis_enhanced$n_total_estimated[mask_off_arr] &lt;- sdis_enhanced$n_total_reported[mask_off_arr]\nsdis_enhanced$n_total_estimated_comment[mask_off_arr] &lt;- \"Used reported total (matches offenders + arrestees)\"\n\n# Rule 2: States where n_total == n_offenders + n_arrestees + n_forensic\nmask_all &lt;- sdis_enhanced$total_equals_all\nsdis_enhanced$n_total_estimated[mask_all] &lt;- sdis_enhanced$n_total_reported[mask_all] - sdis_enhanced$n_forensic[mask_all]\nsdis_enhanced$n_total_estimated_comment[mask_all] &lt;- \"Subtracted forensic from reported total\"\n\n# Rule 3: States where n_total == n_offenders\nmask_off_only &lt;- sdis_enhanced$total_equals_offenders\nsdis_enhanced$n_total_estimated[mask_off_only] &lt;- sdis_enhanced$n_total_reported[mask_off_only]\nsdis_enhanced$n_total_estimated_comment[mask_off_only] &lt;- \"Used reported total (matches offenders only)\"\n\n# Rule 4: For remaining states with n_total\nmask_total_only &lt;- !is.na(sdis_enhanced$n_total_reported) & \n  is.na(sdis_enhanced$n_total_estimated) &\n  (is.na(sdis_enhanced$n_arrestees) | sdis_enhanced$n_arrestees == 0) &\n  (is.na(sdis_enhanced$n_offenders) | sdis_enhanced$n_offenders == 0) &\n  (is.na(sdis_enhanced$n_forensic) | sdis_enhanced$n_forensic == 0)\n\nsdis_enhanced$n_total_estimated[mask_total_only] &lt;- sdis_enhanced$n_total_reported[mask_total_only]\nsdis_enhanced$n_total_estimated_comment[mask_total_only] &lt;- \"Total only reported (no breakdown available)\"\n\n# Special case: States with n_total and n_forensic only\nmask_total_forensic_only &lt;- !is.na(sdis_enhanced$n_total_reported) & \n  is.na(sdis_enhanced$n_total_estimated) &\n  (is.na(sdis_enhanced$n_arrestees) | sdis_enhanced$n_arrestees == 0) &\n  (is.na(sdis_enhanced$n_offenders) | sdis_enhanced$n_offenders == 0) &\n  !is.na(sdis_enhanced$n_forensic) & sdis_enhanced$n_forensic &gt; 0\n\nsdis_enhanced$n_total_estimated[mask_total_forensic_only] &lt;- sdis_enhanced$n_total_reported[mask_total_forensic_only]\nsdis_enhanced$n_total_estimated_comment[mask_total_forensic_only] &lt;- \"Total only reported (forensic reported separately)\"\n\n# States with total and some components but unclear calculation\nmask_has_total_unclear &lt;- !is.na(sdis_enhanced$n_total_reported) & is.na(sdis_enhanced$n_total_estimated)\nsdis_enhanced$n_total_estimated[mask_has_total_unclear] &lt;- sdis_enhanced$n_total_reported[mask_has_total_unclear]\nsdis_enhanced$n_total_estimated_comment[mask_has_total_unclear] &lt;- \"Total with discrepancy (calculation unclear)\"\n\n# For states without any total but with offenders and arrestees\nmask_no_total &lt;- is.na(sdis_enhanced$n_total_reported) & \n  !is.na(sdis_enhanced$n_offenders) & sdis_enhanced$n_offenders &gt; 0 &\n  !is.na(sdis_enhanced$n_arrestees) & sdis_enhanced$n_arrestees &gt; 0\n\nsdis_enhanced$n_total_estimated[mask_no_total] &lt;- sdis_enhanced$n_offenders[mask_no_total] + sdis_enhanced$n_arrestees[mask_no_total]\nsdis_enhanced$n_total_estimated_comment[mask_no_total] &lt;- \"Calculated from offenders + arrestees (no total reported)\"\n\n# For states without total but with only offenders &gt; 0\nmask_no_total_off_only &lt;- is.na(sdis_enhanced$n_total_reported) & \n  !is.na(sdis_enhanced$n_offenders) & sdis_enhanced$n_offenders &gt; 0 &\n  (is.na(sdis_enhanced$n_arrestees) | sdis_enhanced$n_arrestees == 0)\n\nsdis_enhanced$n_total_estimated[mask_no_total_off_only] &lt;- sdis_enhanced$n_offenders[mask_no_total_off_only]\nsdis_enhanced$n_total_estimated_comment[mask_no_total_off_only] &lt;- \"Used offenders count (no total reported, no arrestee data)\"\n\n# Create enhanced data availability matrix with better visualization\navailability_matrix &lt;- sdis_enhanced %&gt;%\n  transmute(\n    State = state,\n    Arrestees = ifelse(!is.na(n_arrestees) & n_arrestees &gt; 0, \"âœ“\", \"!\"),\n    Offenders = ifelse(!is.na(n_offenders) & n_offenders &gt; 0, \"âœ“\", \"!\"),\n    Forensic = ifelse(!is.na(n_forensic) & n_forensic &gt; 0, \"âœ“\", \"!\"),\n    `Total Reported` = ifelse(!is.na(n_total_reported), \"âœ“\", \"!\"),\n    `Total Method` = case_when(\n      total_method == \"Offenders only\" ~ \"O\",\n      total_method == \"Offenders + Arrestees\" ~ \"O+A\",\n      total_method == \"All components\" ~ \"All\",\n      TRUE ~ \"?\"\n    )\n  )\n\n# Convert to long format for ggplot\navailability_long &lt;- availability_matrix %&gt;%\n  pivot_longer(cols = -State, names_to = \"Field\", values_to = \"Value\") %&gt;%\n  mutate(\n    State = factor(State, levels = sort(unique(State), decreasing = TRUE)),\n    Field = factor(Field, levels = c(\"Arrestees\", \"Offenders\", \"Forensic\", \"Total Reported\", \"Total Method\")),\n    # Create numeric values for coloring\n    NumericValue = case_when(\n      Value == \"âœ“\" ~ 1,\n      Value == \"!\" ~ 0,\n      Value == \"O\" ~ 2,\n      Value == \"O+A\" ~ 3,\n      Value == \"All\" ~ 4,\n      Value == \"?\" ~ 0,\n      TRUE ~ 0\n    ),\n    # Create display labels\n    DisplayLabel = case_when(\n      Field == \"Total Method\" & Value == \"?\" ~ \"?\",\n      Field == \"Total Method\" ~ Value,\n      TRUE ~ Value\n    )\n  )\n\n# Summary of how n_total_estimated was calculated\ncat(\"n_total_estimated Column Calculation Summary:\\n\")\ncat(\"=\", strrep(\"=\", 48), \"\\n\", sep = \"\")\n\n# Group states by how their n_total_estimated was determined\nestimation_groups &lt;- sdis_enhanced %&gt;%\n  group_by(n_total_estimated_comment) %&gt;%\n  summarise(\n    states = list(state),\n    count = n(),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(desc(count))\n\n# Print summary in organized sections\ncat(\"\\n1. States with n_total reported and matching patterns:\\n\")\nmatching_patterns &lt;- estimation_groups %&gt;%\n  filter(str_detect(n_total_estimated_comment, \"matches|Subtracted\"))\nfor (i in 1:nrow(matching_patterns)) {\n  group &lt;- matching_patterns[i, ]\n  cat(sprintf(\"   â€¢ %s: %d states \\n(%s)\\n\", \n              group$n_total_estimated_comment, \n              group$count,\n              paste(unlist(group$states), collapse = \", \")))\n}\n\ncat(\"\\n2. States with only n_total reported (no breakdown):\\n\")\ntotal_only &lt;- estimation_groups %&gt;%\n  filter(str_detect(n_total_estimated_comment, \"Total only reported\"))\nfor (i in 1:nrow(total_only)) {\n  group &lt;- total_only[i, ]\n  cat(sprintf(\"   â€¢ %s: %d states \\n(%s)\\n\", \n              group$n_total_estimated_comment, \n              group$count,\n              paste(unlist(group$states), collapse = \", \")))\n}\n\ncat(\"\\n3. States without n_total reported (calculated):\\n\")\ncalculated &lt;- estimation_groups %&gt;%\n  filter(str_detect(n_total_estimated_comment, \"Calculated|Used offenders count\"))\nfor (i in 1:nrow(calculated)) {\n  group &lt;- calculated[i, ]\n  cat(sprintf(\"   â€¢ %s: %d states (%s)\\n\", \n              group$n_total_estimated_comment, \n              group$count,\n              paste(unlist(group$states), collapse = \", \")))\n}\n\ncat(\"\\n4. States with unclear calculation patterns:\\n\")\nunclear &lt;- estimation_groups %&gt;%\n  filter(str_detect(n_total_estimated_comment, \"discrepancy|unclear\"))\nfor (i in 1:nrow(unclear)) {\n  group &lt;- unclear[i, ]\n  cat(sprintf(\"   â€¢ %s: %d states (%s)\\n\", \n              group$n_total_estimated_comment, \n              group$count,\n              paste(unlist(group$states), collapse = \", \")))\n  \n  # Show details for unclear states\n  unclear_states &lt;- sdis_enhanced %&gt;% \n    filter(state %in% unlist(group$states)) %&gt;%\n    select(state, n_total_reported, n_offenders, n_arrestees, n_forensic)\n  \n  for (j in 1:nrow(unclear_states)) {\n    s &lt;- unclear_states[j, ]\n    cat(sprintf(\"      - %s: Total=%s, Offenders=%s, Arrestees=%s, Forensic=%s\\n\",\n                s$state, \n                format(s$n_total_reported %||% 0, big.mark = \",\"),\n                format(s$n_offenders %||% 0, big.mark = \",\"),\n                format(s$n_arrestees %||% 0, big.mark = \",\"),\n                format(s$n_forensic %||% 0, big.mark = \",\")))\n  }\n}\n\n# Create enhanced heatmap with legend at top and x-labels on both top and bottom\np &lt;- ggplot(availability_long, aes(x = Field, y = State, fill = factor(NumericValue))) +\n  geom_tile(color = \"white\", linewidth = 0.8, width = 0.9, height = 0.9) +\n  geom_text(aes(label = DisplayLabel, color = factor(NumericValue)), \n            size = 3.5, fontface = \"bold\", vjust = 0.8) +\n  scale_fill_manual(\n    name = \"Legend\",\n    values = c(\n      \"0\" = \"#FF6B6B\",      # Red/Orange for missing/unknown\n      \"1\" = \"#2E86AB\",      # Blue for available\n      \"2\" = \"#4ECDC4\",      # Teal for Offenders only\n      \"3\" = \"#45B7D1\",      # Light blue for Offenders + Arrestees\n      \"4\" = \"#1A535C\"       # Dark teal for All components\n    ),\n    labels = c(\n      \"0\" = \"Missing/Unknown data\",\n      \"1\" = \"Data available\",\n      \"2\" = \"Total = Offenders only\",\n      \"3\" = \"Total = Offenders + Arrestees\",\n      \"4\" = \"Total = All components\"\n    )\n  ) +\n  scale_color_manual(values = c(\n    \"0\" = \"white\",        # White text on red\n    \"1\" = \"white\",        # White text on blue\n    \"2\" = \"white\",        # White text on teal\n    \"3\" = \"white\",        # White text on light blue\n    \"4\" = \"white\"         # White text on dark teal\n  )) +\n  labs(\n    title = \" \",\n    x = \"\", \n    y = \"\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    axis.text.x.top = element_text(angle = 315, hjust = 0.5, face = \"bold\"),\n    axis.text.y = element_text(face = \"bold\"),\n    plot.title = element_text(face = \"bold\", hjust = 0.5, size = 14),\n    plot.subtitle = element_text(hjust = 0.5, color = \"gray40\", size = 9),\n    panel.grid = element_blank(),\n    legend.position = \"top\",\n    legend.title = element_text(face = \"bold\"),\n    legend.text = element_text(size = 9),\n    axis.ticks.x = element_line(),\n    axis.ticks.x.top = element_line()\n  ) +\n  guides(color = \"none\") +\n  coord_fixed() +\n  scale_x_discrete(position = \"top\")\n\n# Update the main dataframe for subsequent analyses\nsdis_data &lt;- sdis_enhanced\n\n# Helper function for null coalescing\n`%||%` &lt;- function(a, b) if (!is.null(a) && !is.na(a)) a else b\n\n\nn_total_estimated Column Calculation Summary:\n=================================================\n\n1. States with n_total reported and matching patterns:\n   â€¢ Used reported total (matches offenders only): 8 states \n(Connecticut, Idaho, Illinois, Minnesota, Montana, Texas, Washington, West Virginia)\n   â€¢ Used reported total (matches offenders + arrestees): 4 states \n(Alaska, North Carolina, Rhode Island, Tennessee)\n\n2. States with only n_total reported (no breakdown):\n   â€¢ Total only reported (no breakdown available): 10 states \n(Alabama, Arkansas, Colorado, Florida, Louisiana, Mississippi, Nevada, New Jersey, South Dakota, Virginia)\n   â€¢ Total only reported (forensic reported separately): 2 states \n(California, South Carolina)\n\n3. States without n_total reported (calculated):\n   â€¢ Calculated from offenders + arrestees (no total reported): 1 states (Maryland)\n\n4. States with unclear calculation patterns:\n   â€¢ Total with discrepancy (calculation unclear): 3 states (Georgia, Indiana, Missouri)\n      - Georgia: Total=347,145, Offenders=324,864, Arrestees=NA, Forensic=22,281\n      - Indiana: Total=449,000, Offenders=304,000, Arrestees=144,000, Forensic=22,400\n      - Missouri: Total=4e+05, Offenders=386,000, Arrestees=NA, Forensic=NA\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow summary visualization code\n# Create summary data\nsummary_data &lt;- sdis_data %&gt;%\n  mutate(\n    data_category = case_when(\n      str_detect(n_total_estimated_comment, \"matches offenders only\") ~ \"Matches Offenders Only\",\n      str_detect(n_total_estimated_comment, \"matches offenders \\\\+ arrestees\") ~ \"Matches Offenders + Arrestees\",\n      str_detect(n_total_estimated_comment, \"Total only reported \\\\(no breakdown\\\\)\") ~ \"Total Only (No Breakdown)\",\n      str_detect(n_total_estimated_comment, \"Total only reported \\\\(forensic\") ~ \"Total Only (Forensic Separate)\",\n      str_detect(n_total_estimated_comment, \"Calculated from offenders \\\\+ arrestees\") ~ \"Calculated (Offenders + Arrestees)\",\n      str_detect(n_total_estimated_comment, \"Used offenders count\") ~ \"Calculated (Offenders Only)\",\n      str_detect(n_total_estimated_comment, \"discrepancy|unclear\") ~ \"Unclear Calculation\",\n      TRUE ~ \"Other\"\n    ),\n    has_arrestees = !is.na(n_arrestees) & n_arrestees &gt; 0,\n    has_offenders = !is.na(n_offenders) & n_offenders &gt; 0,\n    has_forensic = !is.na(n_forensic) & n_forensic &gt; 0,\n    has_total_reported = !is.na(n_total_reported)\n  )\n\n# Create a summary table by category\ncategory_summary &lt;- summary_data %&gt;%\n  group_by(data_category) %&gt;%\n  summarise(\n    count = n(),\n    states = paste(sort(state), collapse = \", \"),\n    avg_total = mean(n_total_estimated, na.rm = TRUE),\n    median_total = median(n_total_estimated, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(desc(count))\n\n# Print summary statistics\ncat(\"SUMMARY STATISTICS\\n\")\ncat(\"=================\\n\\n\")\n\ncat(\"Calculation Methods Distribution:\\n\")\nfor (i in 1:nrow(category_summary)) {\n  cat(sprintf(\"\\n%s: %d states (%s)\\n\", \n              category_summary$data_category[i], \n              category_summary$count[i],\n              category_summary$states[i]))\n}\n\ncat(\"\\nOverall Data Availability:\\n\")\ncat(sprintf(\"States with arrestee data: %d\\n\", sum(summary_data$has_arrestees)))\ncat(sprintf(\"States with offender data: %d\\n\", sum(summary_data$has_offenders)))\ncat(sprintf(\"States with forensic data: %d\\n\", sum(summary_data$has_forensic)))\ncat(sprintf(\"States with total reported: %d\\n\", sum(summary_data$has_total_reported)))\n\n\nSUMMARY STATISTICS\n=================\n\nCalculation Methods Distribution:\n\nOther: 32 states (Alabama, Arizona, Arkansas, Colorado, Delaware, Florida, Hawaii, Iowa, Kansas, Kentucky, Louisiana, Maine, Massachusetts, Michigan, Mississippi, Nebraska, Nevada, New Hampshire, New Jersey, New Mexico, New York, North Dakota, Ohio, Oklahoma, Oregon, Pennsylvania, South Dakota, Utah, Vermont, Virginia, Wisconsin, Wyoming)\n\nMatches Offenders Only: 8 states (Connecticut, Idaho, Illinois, Minnesota, Montana, Texas, Washington, West Virginia)\n\nMatches Offenders + Arrestees: 4 states (Alaska, North Carolina, Rhode Island, Tennessee)\n\nUnclear Calculation: 3 states (Georgia, Indiana, Missouri)\n\nTotal Only (Forensic Separate): 2 states (California, South Carolina)\n\nCalculated (Offenders + Arrestees): 1 states (Maryland)\n\nOverall Data Availability:\nStates with arrestee data: 6\nStates with offender data: 16\nStates with forensic data: 11\nStates with total reported: 27"
  },
  {
    "objectID": "qmd_root/sdis_summary.html#stacked-bar-chart-dna-profile-composition-by-state",
    "href": "qmd_root/sdis_summary.html#stacked-bar-chart-dna-profile-composition-by-state",
    "title": "SDIS Summary Analysis",
    "section": "Stacked Bar Chart: DNA Profile Composition by State",
    "text": "Stacked Bar Chart: DNA Profile Composition by State\nThis visualization shows the composition of DNA profiles across states, with separate colors for offender, arrestee, and forensic profiles. States without detailed breakdowns show total counts (reported or calculated).\n\n\nShow stacked bar chart code\n# Prepare data for stacked bar chart\nbar_data &lt;- sdis_data %&gt;%\n  mutate(\n    # Determine if state has detailed breakdown\n    has_breakdown = (!is.na(n_offenders) & n_offenders &gt; 0) | \n                    (!is.na(n_arrestees) & n_arrestees &gt; 0) | \n                    (!is.na(n_forensic) & n_forensic &gt; 0),\n    \n    # For states with breakdown, use actual values\n    offenders_display = ifelse(has_breakdown & !is.na(n_offenders), n_offenders, 0),\n    arrestees_display = ifelse(has_breakdown & !is.na(n_arrestees), n_arrestees, 0),\n    forensic_display = ifelse(has_breakdown & !is.na(n_forensic), n_forensic, 0),\n    \n    # For states without breakdown, use total\n    total_reported_display = ifelse(!has_breakdown & !is.na(n_total_reported), \n                                    n_total_reported, 0),\n    total_calculated_display = ifelse(!has_breakdown & is.na(n_total_reported) & \n                                      !is.na(n_total_estimated), \n                                      n_total_estimated, 0)\n  ) %&gt;%\n  filter(!is.na(n_total_estimated) | has_breakdown) %&gt;%\n  select(state, offenders_display, arrestees_display, forensic_display, \n         total_reported_display, total_calculated_display) %&gt;%\n  pivot_longer(cols = -state, names_to = \"profile_type\", values_to = \"count\") %&gt;%\n  filter(count &gt; 0) %&gt;%\n  mutate(\n    profile_type = factor(profile_type, \n                         levels = c(\"offenders_display\", \"arrestees_display\", \n                                   \"forensic_display\", \"total_reported_display\", \n                                   \"total_calculated_display\"),\n                         labels = c(\"Offender Profiles\", \"Arrestee Profiles\", \n                                   \"Forensic Profiles\", \"Total (Reported)\", \n                                   \"Total (Calculated)\"))\n  )\n\n# Calculate total for ordering states\nstate_totals &lt;- bar_data %&gt;%\n  group_by(state) %&gt;%\n  summarise(total = sum(count, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  arrange(desc(total))\n\n# Reorder states by total count\nbar_data &lt;- bar_data %&gt;%\n  mutate(state = factor(state, levels = state_totals$state))\n\n# Create custom label function for y-axis\nlabel_k_m &lt;- function(x) {\n  ifelse(x &gt;= 1e6, \n         paste0(round(x / 1e6, 1), \"M\"),\n         ifelse(x &gt;= 1e3,\n                paste0(round(x / 1e3, 0), \"k\"),\n                as.character(x)))\n}\n\n# Create stacked bar chart\n# Create stacked bar chart\nggplot(bar_data, aes(x = state, y = count, fill = profile_type)) +\n  geom_bar(stat = \"identity\", width = 0.8, color = \"white\", linewidth = 0.2) +\n  scale_fill_manual(\n    name = \"Profile Type\",\n    values = c(\n      \"Offender Profiles\" = \"#2E86AB\",\n      \"Arrestee Profiles\" = \"#F9A03F\",\n      \"Forensic Profiles\" = \"#2ca02c\",\n      \"Total (Reported)\" = \"#CCCCCC\",\n      \"Total (Calculated)\" = \"#808080\"\n    )\n  ) +\n  scale_y_continuous(\n    labels = label_k_m,\n    breaks = seq(0, max(bar_data %&gt;% group_by(state) %&gt;% summarise(total = sum(count)) %&gt;% pull(total), na.rm = TRUE), \n                 by = 250000)\n  ) +\n  labs(\n    title = \" \",\n    x = NULL,\n    y = \"Number of Profiles\"\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(\n    axis.text.x = element_text(angle = 90, hjust = 0, size = 15, vjust = 0, margin = margin(t = 0)),\n    axis.text.y = element_text(size = 15, margin = margin(t = 0)),\n    plot.title = element_text(face = \"bold\", hjust = 0.5, size = 18),\n    legend.position = \"top\",\n    legend.spacing.x = unit(1, \"cm\"),\n    legend.spacing.y = unit(0.2, \"cm\"),\n    legend.box.spacing = unit(0.2, \"cm\"),\n    legend.text = element_text(size = 15),\n    legend.title = element_text(face = \"bold\", size = 18),\n    axis.title.x = element_text(size = 18, margin = margin(t = 10)),\n    axis.title.y = element_text(size = 18, margin = margin(r = 10)),\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  guides(fill = guide_legend(nrow = 3, byrow = TRUE))"
  },
  {
    "objectID": "qmd_root/sdis_summary.html#export-enhanced-dataset",
    "href": "qmd_root/sdis_summary.html#export-enhanced-dataset",
    "title": "SDIS Summary Analysis",
    "section": "Export Enhanced Dataset",
    "text": "Export Enhanced Dataset\nExport the enhanced dataset with the new n_total_estimated values and documentation.\n\n\nShow exportation code\n# Prepare final dataset with key columns in logical order\nfinal_columns &lt;- c(\n  'state', \n  'n_total_estimated',\n  'n_total_reported',\n  'n_total_estimated_comment',\n  'total_method',\n  'n_arrestees', \n  'n_offenders', \n  'n_forensic',\n  'arrestee_collection',\n  'fam_search',\n  'collection_statute'\n)\n\n# Select columns that exist in the dataset\navailable_columns &lt;- intersect(final_columns, names(sdis_enhanced))\nsdis_final &lt;- sdis_enhanced %&gt;% select(all_of(available_columns))\n\n# Export to CSV\noutput_path &lt;- file.path(here(\"data\", \"sdis\", \"final\", \"SDIS_cross_section.csv\"))\nwrite_csv(sdis_final, output_path)\ncat(paste(\"Exported enhanced SDIS dataset to:\", output_path, \"\\n\"))\n\n# Display final summary statistics\ncat(\"\\n\\nFinal Summary Statistics:\\n\")\ncat(\"=\", strrep(\"=\", 48), \"\\n\", sep = \"\")\ncat(paste(\"Total states in dataset:\", nrow(sdis_final), \"\\n\"))\ncat(paste(\"States with n_total_estimated:\", sum(!is.na(sdis_final$n_total_estimated)), \"\\n\"))\ncat(paste(\"States with n_total_reported:\", sum(!is.na(sdis_final$n_total_reported)), \"\\n\"))\ncat(paste(\"Total profiles (estimated):\", format(sum(sdis_final$n_total_estimated, na.rm = TRUE), big.mark = \",\"), \"\\n\"))\n\n# Prepare the data for the interactive table\nsummary_table &lt;- sdis_data %&gt;%\n  select(state, n_total_estimated, n_total_reported, n_offenders, n_arrestees, \n         n_forensic, n_total_estimated_comment) %&gt;%\n  mutate(across(where(is.numeric), ~ifelse(is.na(.), NA, format(., big.mark = \",\", scientific = FALSE))))\n\n# Create interactive table\ndatatable(\n  summary_table,\n  extensions = c('Buttons', 'ColReorder', 'Scroller'),\n  options = list(\n    dom = 'Bfrtip',\n    buttons = c('copy', 'csv', 'excel', 'colvis'),\n    scrollX = TRUE,\n    scrollY = \"600px\",\n    scroller = TRUE,\n    pageLength = 20,\n    columnDefs = list(\n      list(className = 'dt-right', targets = 1:5),  # Right-align numeric columns\n      list(className = 'dt-left', targets = c(0, 6))  # Left-align state and comment columns\n    )\n  ),\n  rownames = FALSE,\n  filter = 'top',\n  caption = \"Enhanced Data with Estimated Totals\"\n)\n\n# Create final frozen version (v1.0)\nfrozen_dir &lt;- here(\"data\", \"v1.0\")\ndir.create(frozen_dir, recursive = TRUE, showWarnings = FALSE)\n\nfrozen_path &lt;- here(frozen_dir, \"SDIS_cross_section.csv\")\nwrite_csv(sdis_final, frozen_path)\ncat(paste(\"âœ“ Created frozen version 1.0 at:\", frozen_path, \"\\n\"))\n\n\nExported enhanced SDIS dataset to: C:/Users/Donadio/Documents/PODFRIDGE_Databases/data/sdis/final/SDIS_cross_section.csv \n\n\nFinal Summary Statistics:\n=================================================\nTotal states in dataset: 50 \nStates with n_total_estimated: 28 \nStates with n_total_reported: 27 \nTotal profiles (estimated): 12,814,222 \n\n\n\n\n\n\nâœ“ Created frozen version 1.0 at: C:/Users/Donadio/Documents/PODFRIDGE_Databases/data/v1.0/SDIS_cross_section.csv"
  },
  {
    "objectID": "qmd_root/ndis_analysis.html",
    "href": "qmd_root/ndis_analysis.html",
    "title": "NDIS Database",
    "section": "",
    "text": "Analyze patterns of state and federal participation in NDIS\n\nTrack which jurisdictions are actively contributing data and identify any geographic disparities in participation levels.\n\nIdentify periods of rapid growth or stagnation in DNA profile submissions\n\nDetect acceleration points and plateaus in the expansion of the national DNA database to understand adoption trends.\n\nDocument the expansion of DNA profiles (offender, arrestee, forensic) over time\n\nMonitor the growth trajectory of different profile categories to assess program effectiveness and resource allocation."
  },
  {
    "objectID": "qmd_root/ndis_analysis.html#setup-configuration",
    "href": "qmd_root/ndis_analysis.html#setup-configuration",
    "title": "NDIS Database",
    "section": "Setup and Configuration",
    "text": "Setup and Configuration\nThis section prepares the environment for analysis by:\n\nEnsuring all required R packages are available\nLoading the NDIS dataset with proper type specifications\nProviding basic data validation checks\n\n\n\nShow setup code\n# List of required packages\nrequired_packages &lt;- c(\n  \"tidyverse\",    # Data manipulation and visualization\n  \"lubridate\",    # Date-time manipulation\n  \"DT\",           # Interactive tables\n  \"plotly\",       # Interactive visualizations\n  \"leaflet\",      # Geospatial mapping\n  \"kableExtra\",   # Enhanced table formatting\n  \"scales\",       # Axis scaling and formatting\n  \"dlookr\",       # Data validation and diagnostics\n  \"gt\",           # Table generation\n  \"assertr\",      # Data validation and assertions\n  \"flextable\",    # Enhanced table visualization\n  \"ggridges\",     # Ridge plots\n  \"here\",         # File path management\n  \"patchwork\",    # Data visualization  \n  \"scales\",       # Plot aesthetics\n  \"viridis\",      # Color pallete for plots\n  \"ggrepel\"       # Adjust legend location\n  )\n\n# Function to install missing packages\ninstall_missing &lt;- function(packages) {\n  for (pkg in packages) {\n    if (!requireNamespace(pkg, quietly = TRUE)) {\n      message(paste(\"Installing missing package:\", pkg))\n      install.packages(pkg, dependencies = TRUE)\n    }\n  }\n}\n\n# Install any missing packages\ninstall_missing(required_packages)\n\n# Load all packages\nsuppressPackageStartupMessages({\n  library(tidyverse)\n  library(lubridate)\n  library(DT)\n  library(plotly)\n  library(leaflet)\n  library(kableExtra)\n  library(scales)\n  library(dlookr)\n  library(gt)\n  library(assertr)\n  library(flextable)\n  library(ggridges)\n  library(here)\n  library(patchwork)\n  library(scales)\n  library(viridis)\n  library(ggrepel)\n})\n\n# Verify all packages loaded successfully\nloaded_packages &lt;- sapply(required_packages, require, character.only = TRUE)\n\nif (all(loaded_packages)) {\n  message(\"ðŸ“š All packages loaded successfully!\")\n} else {\n  warning(\"The following packages failed to load: \", \n          paste(names(loaded_packages)[!loaded_packages], collapse = \", \"))\n}\n\n\n\nData Import and Validation\n\n\nShow data import code\n# Define expected column structure\nexpected_cols &lt;- cols(\n  timestamp = col_character(),\n  report_month = col_character(),\n  report_year = col_character(),\n  jurisdiction = col_character(),\n  offender_profiles = col_double(),\n  arrestee = col_double(),\n  forensic_profiles = col_double(),\n  ndis_labs = col_double(),\n  investigations_aided = col_double()\n)\n\n# Read data with validation\nndis_data &lt;- read_csv(\n  here::here(\"data\", \"ndis\", \"raw\", \"ndis_data_raw.csv\"),\n  col_types = expected_cols\n)"
  },
  {
    "objectID": "qmd_root/ndis_analysis.html#datacleaning",
    "href": "qmd_root/ndis_analysis.html#datacleaning",
    "title": "NDIS Database",
    "section": "Dataset Cleaning",
    "text": "Dataset Cleaning\nThis section outlines the preprocessing steps applied to the raw NDIS data before analysis.\nFirst of all, to ensure consistency for analysis, we fixed jurisdiction names that were not correctly scraped in the dataset as we see here:\n\n\nAlabama, Alabama Alabama, Alabama Stats Alabama, Alaska, Alaska Alaska, Alaska Stats Alaska, and Legal profiles at NDIS. Statistics as of April 2025 Alabama, and Legal profiles at NDIS. Statistics as of June 2025 Alabama, and Legal profiles at NDIS. Statistics as of March 2025 Alabama, Arizona, Arizona Arizona, Arizona Stats Arizona, Arkansas, Arkansas Arkansas, Arkansas Stats Arkansas, Army, California, California California, California Stats California, Colorado, Colorado Colorado, Colorado Stats Colorado, Connecticut, Connecticut Connecticut, Connecticut Stats Connecticut, DC, DC map pin). Statistics as of November 2022 Alabama, DC map pin.) Statistics as of August 2023 Alabama, DC map pin.) Statistics as of February 2024 Alabama, DC map pin.) Statistics as of January 2025 Alabama, DC map pin.) Statistics as of November 2024 Alabama, DC map pin.) Statistics as of October 2024 Alabama, DC/FBI Lab, DC/Metro PD, Delaware, Delaware Delaware, Delaware Stats Delaware, Florida, Florida Florida, Florida Stats Florida, Georgia, Georgia Georgia, Georgia Stats Georgia, Hawaii, Hawaii Hawaii, Hawaii Stats Hawaii, Idaho, Idaho Idaho, Illinois, Illinois Illinois, Illinois Stats Illinois, Indiana, Indiana Indiana, Indiana Stats Indiana, Iowa, Iowa Iowa, Iowa Stats Iowa, Kansas, Kansas Kansas, Kansas Stats Kansas, Kentucky, Kentucky Kentucky, Kentucky Stats Kentucky, Lab, Louisiana, Louisiana Louisiana, Louisiana Stats Louisiana, Maine, Maine Maine, Maine Stats Maine, Maryland, Maryland Maryland, Maryland Stats Maryland, Massachusetts, Massachusetts Massachusetts, Massachusetts Stats Massachusetts, Mexico Stats New Mexico, Michigan, Michigan Michigan, Michigan Stats Idaho, Michigan Stats Michigan, Michigan Stats Utah, Minnesota, Minnesota Minnesota, Minnesota Stats Minnesota, Mississippi, Mississippi Mississippi, Mississippi Stats Mississippi, Missouri, Missouri Missouri, Missouri Stats Missouri, Montana, Montana Montana, Montana Stats Montana, Nebraska, Nebraska Nebraska, Nebraska Stats Nebraska, Nevada, Nevada Nevada, Nevada Stats Nevada, New Hampshire, New Hampshire New Hampshire, New Hampshire Stats New Hampshire, New Jersey, New Jersey New Jersey, New Jersey Stats New Jersey, New Mexico, New Mexico New Mexico, New York, New York New York, New York Stats New York, North Carolina, North Carolina North Carolina, North Carolina Stats North Carolina, North Dakota, North Dakota North Dakota, North Dakota Stats North Dakota, Ohio, Ohio Ohio, Ohio Stats Ohio, Oklahoma, Oklahoma Oklahoma, Oklahoma Stats Oklahoma, Oregon, Oregon Oregon, Oregon Stats Oregon, Participant Alabama, Pennsylvania, Pennsylvania Pennsylvania, Pennsylvania Stats Pennsylvania, PR, Puerto Rico, Rhode Island, Rhode Island Rhode Island, Rhode Island Stats Rhode Island, South Carolina, South Carolina South Carolina, South Carolina Stats South Carolina, South Dakota, South Dakota South Dakota, South Dakota Stats South Dakota, Tables by NDIS Participant Alabama, Tennessee, Tennessee Stats Tennessee, Tennessee Tennessee, Texas, Texas Stats Texas, Texas Texas, U.S. Army, Utah, Utah Utah, Vermont, Vermont Stats Vermont, Vermont Vermont, Virginia, Virginia Stats Virginia, Virginia Virginia, Washington, Washington State Stats Washington, Washington State Washington, West Virginia, West Virginia Stats West Virginia, West Virginia Stats Wyoming, West Virginia West Virginia, Wisconsin, Wisconsin Stats Wisconsin, Wisconsin Wisconsin, Wyoming, Wyoming Wyoming\n\n\n\n\nShow cleaning code (jurisdiction)\n# Clean jurisdiction names with Alabama-specific patterns\nndis_data_jurisdiction &lt;- ndis_data %&gt;%\n  mutate(\n    jurisdiction = case_when(\n      # Standard state names\n      str_detect(jurisdiction, \"Alabama$|Alabama Stats\") ~ \"Alabama\",\n      str_detect(jurisdiction, \"Alaska$|Alaska Stats\") ~ \"Alaska\",\n      str_detect(jurisdiction, \"Arizona$|Arizona Stats\") ~ \"Arizona\",\n      str_detect(jurisdiction, \"Arkansas$|Arkansas Stats\") ~ \"Arkansas\",\n      str_detect(jurisdiction, \"California$|California Stats\") ~ \"California\",\n      str_detect(jurisdiction, \"Colorado$|Colorado Stats\") ~ \"Colorado\",\n      str_detect(jurisdiction, \"Connecticut$|Connecticut Stats\") ~ \"Connecticut\",\n      str_detect(jurisdiction, \"Delaware$|Delaware Stats\") ~ \"Delaware\",\n      str_detect(jurisdiction, \"Florida$|Florida Stats\") ~ \"Florida\",\n      str_detect(jurisdiction, \"Georgia$|Georgia Stats\") ~ \"Georgia\",\n      str_detect(jurisdiction, \"Hawaii$|Hawaii Stats\") ~ \"Hawaii\",\n      str_detect(jurisdiction, \"Idaho$|Idaho Stats\") ~ \"Idaho\",\n      str_detect(jurisdiction, \"Illinois$|Illinois Stats\") ~ \"Illinois\",\n      str_detect(jurisdiction, \"Indiana$|Indiana Stats\") ~ \"Indiana\",\n      str_detect(jurisdiction, \"Iowa$|Iowa Stats\") ~ \"Iowa\",\n      str_detect(jurisdiction, \"Kansas$|Kansas Stats\") ~ \"Kansas\",\n      str_detect(jurisdiction, \"Kentucky$|Kentucky Stats\") ~ \"Kentucky\",\n      str_detect(jurisdiction, \"Louisiana$|Louisiana Stats\") ~ \"Louisiana\",\n      str_detect(jurisdiction, \"Maine$|Maine Stats\") ~ \"Maine\",\n      str_detect(jurisdiction, \"Maryland$|Maryland Stats\") ~ \"Maryland\",\n      str_detect(jurisdiction, \"Massachusetts$|Massachusetts Stats\") ~ \"Massachusetts\",\n      str_detect(jurisdiction, \"Michigan$|Michigan Stats\") ~ \"Michigan\",\n      str_detect(jurisdiction, \"Minnesota$|Minnesota Stats\") ~ \"Minnesota\",\n      str_detect(jurisdiction, \"Mississippi$|Mississippi Stats\") ~ \"Mississippi\",\n      str_detect(jurisdiction, \"Missouri$|Missouri Stats\") ~ \"Missouri\",\n      str_detect(jurisdiction, \"Montana$|Montana Stats\") ~ \"Montana\",\n      str_detect(jurisdiction, \"Nebraska$|Nebraska Stats\") ~ \"Nebraska\",\n      str_detect(jurisdiction, \"Nevada$|Nevada Stats\") ~ \"Nevada\",\n      str_detect(jurisdiction, \"New Hampshire$|New Hampshire Stats\") ~ \"New Hampshire\",\n      str_detect(jurisdiction, \"New Jersey$|New Jersey Stats\") ~ \"New Jersey\",\n      str_detect(jurisdiction, \"New Mexico$|New Mexico Stats|Mexico Stats\") ~ \"New Mexico\",\n      str_detect(jurisdiction, \"New York$|New York Stats\") ~ \"New York\",\n      str_detect(jurisdiction, \"North Carolina$|North Carolina Stats\") ~ \"North Carolina\",\n      str_detect(jurisdiction, \"North Dakota$|North Dakota Stats\") ~ \"North Dakota\",\n      str_detect(jurisdiction, \"Ohio$|Ohio Stats\") ~ \"Ohio\",\n      str_detect(jurisdiction, \"Oklahoma$|Oklahoma Stats\") ~ \"Oklahoma\",\n      str_detect(jurisdiction, \"Oregon$|Oregon Stats\") ~ \"Oregon\",\n      str_detect(jurisdiction, \"Pennsylvania$|Pennsylvania Stats\") ~ \"Pennsylvania\",\n      str_detect(jurisdiction, \"Rhode Island$|Rhode Island Stats\") ~ \"Rhode Island\",\n      str_detect(jurisdiction, \"South Carolina$|South Carolina Stats\") ~ \"South Carolina\",\n      str_detect(jurisdiction, \"South Dakota$|South Dakota Stats\") ~ \"South Dakota\",\n      str_detect(jurisdiction, \"Tennessee$|Tennessee Stats\") ~ \"Tennessee\",\n      str_detect(jurisdiction, \"Texas$|Texas Stats\") ~ \"Texas\",\n      str_detect(jurisdiction, \"Utah$|Utah Stats\") ~ \"Utah\",\n      str_detect(jurisdiction, \"Vermont$|Vermont Stats\") ~ \"Vermont\",\n      str_detect(jurisdiction, \"West Virginia$|West Virginia Stats\") ~ \"West Virginia\",\n      str_detect(jurisdiction, \"Virginia$|Virginia Stats\") ~ \"Virginia\",\n      str_detect(jurisdiction, \"Washington$|Washington State Stats\") ~ \"Washington\",\n      str_detect(jurisdiction, \"Wisconsin$|Wisconsin Stats\") ~ \"Wisconsin\",\n      str_detect(jurisdiction, \"Wyoming$|Wyoming Stats\") ~ \"Wyoming\",\n      \n      # Special jurisdictions\n      str_detect(jurisdiction, \"DC/FBI|Washington DC Stats|Lab\") ~ \"DC/FBI Lab\",\n      str_detect(jurisdiction, \"DC/Metro|DC\") ~ \"DC/Metro PD\",\n      str_detect(jurisdiction, \"U.S. Army$|U.S. Army Stats\") ~ \"U.S. Army\",\n            str_detect(jurisdiction, \"Puerto Rico$|Puerto Rico Stats\") ~ \"Puerto Rico\",\n          \n      str_detect(jurisdiction, \"Tables by NDIS Participant\") ~ \"Alabama\", # Default to Alabama\n      \n      TRUE ~ jurisdiction\n    ),\n    \n    # Clean up any remaining whitespace\n    jurisdiction = str_trim(jurisdiction)\n     )  %&gt;%\n  \n  # Convert to factor with the 54 levels you want\n  mutate(\n    jurisdiction = factor(jurisdiction,\n                         levels = c(sort(state.name), \"Puerto Rico\", \"DC/FBI Lab\", \"DC/Metro PD\", \"U.S. Army\"))) %&gt;%\n  \n  # Filter out NA jurisdictions\n  filter(!is.na(jurisdiction))\n\n\nUpdated Jurisdiction names:\n\n\nAlabama, Alaska, Arizona, Arkansas, California, Colorado, Connecticut, Delaware, Florida, Georgia, Hawaii, Idaho, Illinois, Indiana, Iowa, Kansas, Kentucky, Louisiana, Maine, Maryland, Massachusetts, Michigan, Minnesota, Mississippi, Missouri, Montana, Nebraska, Nevada, New Hampshire, New Jersey, New Mexico, New York, North Carolina, North Dakota, Ohio, Oklahoma, Oregon, Pennsylvania, Rhode Island, South Carolina, South Dakota, Tennessee, Texas, Utah, Vermont, Virginia, Washington, West Virginia, Wisconsin, Wyoming, Puerto Rico, DC/FBI Lab, DC/Metro PD, U.S. Army\n\n\nVariables were reformatted into consistent date and time structures.\nKey profile counts were combined into a total_profiles measure, and missing reporting periods were filled using available capture information.\nFinally, year and month variables were standardized, and the dataset was reordered to ensure a clean, consistent structure for validation and analysis.\n\n\nShow cleaning code (general)\nndis_data &lt;- ndis_data_jurisdiction %&gt;%\n  mutate(\n    capture_datetime = as_datetime(timestamp, format = \"%Y%m%d%H%M%S\"),\n    total_profiles = offender_profiles + arrestee + forensic_profiles,\n    asof_date = make_date(report_year, report_month, 1)\n  )\n\nndis_intermediate &lt;- ndis_data %&gt;%\n  select(\n    capture_datetime, asof_date, jurisdiction,\n    offender_profiles, arrestee, forensic_profiles,\n    total_profiles, ndis_labs, investigations_aided\n  ) %&gt;%\n  arrange(jurisdiction, capture_datetime)\n\n\n\nSaving Intermediate Cleaned Data\nThe cleaned dataset preserves the core NDIS metrics while standardizing temporal and jurisdictional dimensions for consistent analysis. Key structural improvements include:\nÂ· Temporal Standardization: Unified date handling with capture_datetime for data extraction timing and asof_month/asof_year for reported periods\nÂ· Jurisdictional Harmonization: Normalized 54 jurisdiction names (50 states + Puerto Rico, DC/FBI Lab, DC/Metro PD, U.S. Army) using consistent naming conventions\nÂ· Derived Metrics: Added total_profiles as the sum of offender, arrestee, and forensic profiles for comprehensive trend analysis\nÂ· Data Integrity: Removed ambiguous records and ensured proper typing for analytical operations\n\n\nShow intermediate dataset saving code\n# Glimpse\n\nenhanced_glimpse &lt;- function(df) {\n  glimpse_data &lt;- data.frame(\n    Column = names(df),\n    Type = sapply(df, function(x) paste(class(x), collapse = \", \")),\n    Rows = nrow(df),\n    Missing = sapply(df, function(x) sum(is.na(x))),\n    Unique = sapply(df, function(x) length(unique(x))),\n    First_Values = sapply(df, function(x) {\n      if(is.numeric(x)) {\n        paste(round(head(x, 3), 2), collapse = \", \")\n      } else {\n        paste(encodeString(head(as.character(x), 3)), collapse = \", \")\n      }\n    })\n  )\n  \n  ft &lt;- flextable(glimpse_data) %&gt;%\n    theme_zebra() %&gt;%\n    set_caption(paste(\"Enhanced Data Glimpse:\", deparse(substitute(df)))) %&gt;%\n    autofit() %&gt;%\n    align(align = \"left\", part = \"all\") %&gt;%\n    colformat_num(j = c(\"Rows\", \"Missing\", \"Unique\"), big.mark = \"\") %&gt;%\n    bg(j = \"Missing\", bg = function(x) ifelse(x &gt; 0, \"#FFF3CD\", \"transparent\")) %&gt;%\n    bg(j = \"Unique\", bg = function(x) ifelse(x == 1, \"#FFF3CD\", \"transparent\")) %&gt;%\n    add_footer_lines(paste(\"Data frame dimensions:\", nrow(df), \"rows Ã—\", ncol(df), \"columns\")) %&gt;%\n    fontsize(size = 10, part = \"all\") %&gt;%\n    set_table_properties(layout = \"autofit\", width = 1)\n  \n  return(ft)\n}\n\nenhanced_glimpse(ndis_intermediate)\n\n\nColumnTypeRowsMissingUniqueFirst_Valuescapture_datetimePOSIXct, POSIXt318730112692001-07-15 04:15:59, 2001-08-22 11:55:31, 2001-09-13 00:17:54asof_dateDate3187310919123&lt;NA&gt;, &lt;NA&gt;, &lt;NA&gt;jurisdictionfactor31873054Alabama, Alabama, Alabamaoffender_profilesnumeric31873076080, 0, 0arresteenumeric31873025150, 0, 0forensic_profilesnumeric31873063250, 0, 0total_profilesnumeric31873083600, 0, 0ndis_labsnumeric318730234, 4, 4investigations_aidednumeric318730430688, 88, 88Data frame dimensions: 31873 rows Ã— 9 columns\n\n\nShow intermediate dataset saving code\n# Save cleaned data to CSV\nwrite_csv(ndis_intermediate, here::here(\"data\", \"ndis\", \"intermediate\", \"ndis_intermediate.csv\"))\n\nmessage(\"âœ… Intermediate dataset saved to 'data/ndis/intermediate' folder\")\n\n\n\n\nRaw Data Distribution\n\n\nShow raw data visualization code\n# Heatmap\ntemporal_coverage_intermediate &lt;-  ndis_intermediate %&gt;%\n  mutate(year = year(capture_datetime)) %&gt;%\n  count(jurisdiction, year) %&gt;%\n  complete(jurisdiction, year = 2001:2025, fill = list(n = 0)) %&gt;%\n  filter(!is.na(jurisdiction)) %&gt;%\n  mutate(jurisdiction = factor(jurisdiction, levels = rev(sort(unique(jurisdiction)))))\n\nheatmap_raw &lt;- ggplot(temporal_coverage_intermediate, aes(x = year, y = jurisdiction, fill = n)) +\n  geom_tile(color = \"white\", linewidth = 0.3) +\n  scale_fill_viridis(\n    name = \"Snapshots\\nper Year\",\n    option = \"plasma\",\n    direction = -1,\n    breaks = c(0, 12, 24, 48),\n    labels = c(\"0\", \"12\", \"24\", \"48+\")\n  ) +\n  scale_x_continuous(\n    breaks = seq(2001, 2025, by = 1),\n    expand = expansion(mult = 0.01)\n  ) +\n  labs(\n    x = \"Year\",\n    y = \"Jurisdiction\",\n    title = \" \"\n  ) +\n  theme_minimal(base_size = 10) +\n  theme(\n    panel.grid = element_blank(),\n    axis.text.x = element_text(angle = 45, hjust = 1, size = 15),\n    axis.text.y = element_text(size = 15),\n    plot.title = element_text(face = \"bold\", size = 20, hjust = 0),\n    plot.subtitle = element_text(size = 18, hjust = 0),\n    legend.position = \"right\",\n    legend.key.height = unit(0.6, \"cm\"), \n    legend.key.width = unit(0.2, \"cm\"), \n    legend.text = element_text(size = 15),\n    legend.title = element_text(size = 18), \n    axis.title.x = element_text(size = 18, margin = margin(t = 12)),\n    axis.title.y = element_text(size = 18, margin = margin(r = 26))\n  )\n\nheatmap_raw"
  },
  {
    "objectID": "qmd_root/ndis_analysis.html#summarystats",
    "href": "qmd_root/ndis_analysis.html#summarystats",
    "title": "NDIS Database",
    "section": "Summary Statistics",
    "text": "Summary Statistics\nBasic descriptive statistics to understand the scope and characteristics of the NDIS data.\n\n\nShow summary statistics code\n# Summary statistics table\nndis_summary &lt;- ndis_clean %&gt;%\n  mutate(\n    year = year(capture_datetime),\n    offender_profiles = ifelse(is.na(offender_profiles), 0, offender_profiles),\n    arrestee = ifelse(is.na(arrestee), 0, arrestee),\n    forensic_profiles = ifelse(is.na(forensic_profiles), 0, forensic_profiles)\n  ) %&gt;%\n  group_by(year, jurisdiction) %&gt;%\n  summarise(\n    offender = max(offender_profiles, na.rm = TRUE),\n    arrestee = max(arrestee, na.rm = TRUE),\n    forensic = max(forensic_profiles, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  group_by(year) %&gt;%\n  summarise(\n    jurisdictions = n(),\n    offender = sum(offender, na.rm = TRUE),\n    arrestee = sum(arrestee, na.rm = TRUE),\n    forensic = sum(forensic, na.rm = TRUE),\n    total_profiles = sum(offender + arrestee + forensic, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(year)\n\n# Print summary table\nkable(ndis_summary, caption = \"Annual Summary Statistics\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nAnnual Summary Statistics\n\n\nyear\njurisdictions\noffender\narrestee\nforensic\ntotal_profiles\n\n\n\n\n2001\n29\n602848\n0\n21414\n624262\n\n\n2002\n34\n922908\n0\n36070\n958978\n\n\n2003\n41\n1406708\n0\n65446\n1472154\n\n\n2004\n45\n1859371\n0\n91460\n1950831\n\n\n2005\n48\n2743367\n0\n123780\n2867147\n\n\n2006\n49\n3714152\n0\n153738\n3867890\n\n\n2007\n51\n5051763\n0\n195056\n5246819\n\n\n2008\n50\n6229082\n0\n240466\n6469548\n\n\n2009\n51\n7000317\n0\n275850\n7276167\n\n\n2010\n51\n8213788\n0\n331258\n8545046\n\n\n2011\n52\n9298626\n0\n397257\n9695883\n\n\n2012\n52\n9993151\n1239280\n449642\n11682073\n\n\n2013\n52\n10667112\n1694305\n524823\n12886240\n\n\n2014\n52\n11192422\n2037068\n587026\n13816516\n\n\n2015\n52\n11934281\n2287369\n654047\n14875697\n\n\n2016\n52\n12227209\n2254961\n687439\n15169609\n\n\n2017\n54\n13332249\n2891857\n811242\n17035348\n\n\n2018\n54\n13566718\n3324282\n895228\n17786228\n\n\n2019\n54\n14013946\n3760236\n979841\n18754023\n\n\n2020\n54\n14377412\n4181569\n1069155\n19628136\n\n\n2021\n54\n14836566\n4513962\n1144266\n20494794\n\n\n2022\n54\n14836490\n4513955\n1144255\n20494700\n\n\n2023\n54\n16532335\n5190629\n1282432\n23005396\n\n\n2024\n54\n17026171\n5382544\n1321790\n23730505\n\n\n2025\n54\n18648655\n5954756\n1421751\n26025162"
  },
  {
    "objectID": "qmd_root/ndis_analysis.html#visualiz",
    "href": "qmd_root/ndis_analysis.html#visualiz",
    "title": "NDIS Database",
    "section": "Data Visualization",
    "text": "Data Visualization\n\nGeospatial Mapping of Jurisdiction Participation\n\n\nShow jurisdiction mapping analysis code\njurisdiction_coords &lt;- tibble::tribble(\n  ~jurisdiction_std,        ~lat,     ~lng,\n  \"Alabama\",         32.8067,  -86.7911,\n  \"Alaska\",         66.1605, -153.3691,\n  \"Arizona\",        33.7298, -111.4312,\n  \"Arkansas\",       34.9697,  -92.3731,\n  \"California\",     36.1162, -119.6816,\n  \"Colorado\",       39.0598, -105.3111,\n  \"Connecticut\",    41.5978,  -72.7554,\n  \"Delaware\",       39.3185,  -75.5071,\n  \"DC/FBI Lab\",     38.9072,  -77.0369,\n  \"DC/Metro PD\",    39.9072,  -77.0369,\n  \"Florida\",        27.7663,  -81.6868,\n  \"Georgia\",        33.0406,  -83.6431,\n  \"Hawaii\",         21.3068, -157.7912,\n  \"Idaho\",          44.2405, -114.4788,\n  \"Illinois\",       40.3495,  -88.9861,\n  \"Indiana\",        39.8494,  -86.2583,\n  \"Iowa\",           42.0115,  -93.2105,\n  \"Kansas\",         38.5266,  -96.7265,\n  \"Kentucky\",       37.6681,  -84.6701,\n  \"Louisiana\",      31.1695,  -91.8678,\n  \"Maine\",          44.6939,  -69.3819,\n  \"Maryland\",       39.0639,  -76.8021,\n  \"Massachusetts\",  42.2302,  -71.5301,\n  \"Michigan\",       43.3266,  -84.5361,\n  \"Minnesota\",      45.6945,  -93.9002,\n  \"Mississippi\",    32.7416,  -89.6787,\n  \"Missouri\",       38.4561,  -92.2884,\n  \"Montana\",        46.9219, -110.4544,\n  \"Nebraska\",       41.1254,  -98.2681,\n  \"Nevada\",         38.3135, -117.0554,\n  \"New Hampshire\",  43.4525,  -71.5639,\n  \"New Jersey\",     40.2989,  -74.5210,\n  \"New Mexico\",     34.8405, -106.2485,\n  \"New York\",       42.1657,  -74.9481,\n  \"North Carolina\", 35.6301,  -79.8064,\n  \"North Dakota\",   47.5289,  -99.7840,\n  \"Ohio\",           40.3888,  -82.7649,\n  \"Oklahoma\",       35.5653,  -96.9289,\n  \"Oregon\",         44.5720, -122.0709,\n  \"Pennsylvania\",   40.5908,  -77.2098,\n  \"Rhode Island\",   41.6809,  -71.5118,\n  \"South Carolina\", 33.8569,  -80.9450,\n  \"South Dakota\",   44.2998,  -99.4388,\n  \"Tennessee\",      35.7478,  -86.6923,\n  \"Texas\",          31.0545,  -97.5635,\n  \"Utah\",           40.1500, -111.8624,\n  \"Vermont\",        44.0459,  -72.7107,\n  \"Virginia\",       37.7693,  -78.1700,\n  \"Washington\",     47.4009, -121.4905,\n  \"West Virginia\",  38.4912,  -80.9545,\n  \"Wisconsin\",      44.2685,  -89.6165,\n  \"Wyoming\",        42.7560, -107.3025,\n  \"US Army\",        33.7443,  -86.4733,\n  \"Puerto Rico\",    18.2208,  -66.5901\n)\n\nstate_abbs &lt;- tibble::tibble(\n  state = tolower(c(\n    \"Alabama\",\"Alaska\",\"Arizona\",\"Arkansas\",\"California\",\"Colorado\",\"Connecticut\",\n    \"Delaware\", \"Florida\",\"Georgia\",\"Hawaii\",\"Idaho\",\"Illinois\",\n    \"Indiana\",\"Iowa\",\"Kansas\",\"Kentucky\",\"Louisiana\",\"Maine\",\"Maryland\",\"Massachusetts\",\n    \"Michigan\",\"Minnesota\",\"Mississippi\",\"Missouri\",\"Montana\",\"Nebraska\",\"Nevada\",\n    \"New Hampshire\",\"New Jersey\",\"New Mexico\",\"New York\",\"North Carolina\",\"North Dakota\",\n    \"Ohio\",\"Oklahoma\",\"Oregon\",\"Pennsylvania\",\"Rhode Island\",\"South Carolina\",\n    \"South Dakota\",\"Tennessee\",\"Texas\",\"Utah\",\"Vermont\",\"Virginia\",\"Washington\",\n    \"West Virginia\",\"Wisconsin\",\"Wyoming\",\n    \"DC/Metro PD\",\n    \"DC/FBI Lab\",\n    \"Puerto Rico\",\n    \"US Army\"\n  )),\n  abb = c(\n    \"AL\",\"AK\",\"AZ\",\"AR\",\"CA\",\"CO\",\"CT\",\n    \"DE\",\"FL\",\"GA\",\"HI\",\"ID\",\"IL\",\n    \"IN\",\"IA\",\"KS\",\"KY\",\"LA\",\"ME\",\"MD\",\"MA\",\n    \"MI\",\"MN\",\"MS\",\"MO\",\"MT\",\"NE\",\"NV\",\n    \"NH\",\"NJ\",\"NM\",\"NY\",\"NC\",\"ND\",\n    \"OH\",\"OK\",\"OR\",\"PA\",\"RI\",\"SC\",\n    \"SD\",\"TN\",\"TX\",\"UT\",\"VT\",\"VA\",\"WA\",\n    \"WV\",\"WI\",\"WY\",\n    \"DC\",\n    \"FBI\",\n    \"PR\",\n    \"US\"\n  )\n)\n\nmap_data &lt;- ndis_clean %&gt;%\n  left_join(jurisdiction_coords, by = c(\"jurisdiction\" = \"jurisdiction_std\"))\n\nset.seed(123) # for reproducibility\n\nmap_data &lt;- map_data %&gt;%\n  group_by(lat, lng) %&gt;%\n  mutate(\n    n = n(),\n    offset_needed = n &gt; 1,\n    lat_offset = ifelse(offset_needed, runif(1, -0.5, 0.5), 0),\n    lng_offset = ifelse(offset_needed, runif(1, -0.5, 0.5), 0),\n    lat_adj = lat + lat_offset,\n    lng_adj = lng + lng_offset\n  ) %&gt;%\n  ungroup()\n\njurisdiction_summary &lt;- map_data %&gt;%\n  group_by(jurisdiction) %&gt;%\n  filter(capture_datetime == max(capture_datetime, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(jurisdiction = tolower(trimws(jurisdiction))) %&gt;% \n  group_by(jurisdiction) %&gt;%\n  summarise(\n    capture_datetime = max(capture_datetime, na.rm = TRUE),\n    offender = if (all(is.na(offender_profiles))) 0 else max(offender_profiles, na.rm = TRUE),\n    arrestee = if (all(is.na(arrestee))) 0 else max(arrestee, na.rm = TRUE),\n    forensic = if (all(is.na(forensic_profiles))) 0 else max(forensic_profiles, na.rm = TRUE),\n    total_profiles = sum(c(offender, arrestee, forensic), na.rm = TRUE),\n    lat_adj = first(lat_adj),\n    lng_adj = first(lng_adj)\n  ) %&gt;%\n  left_join(state_abbs, by = c(\"jurisdiction\" = \"state\")) %&gt;%\n  filter(!is.na(lat_adj) & !is.na(lng_adj))\n\npal &lt;- colorNumeric(palette = \"Blues\", domain = jurisdiction_summary$total_profiles)\n\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  addLabelOnlyMarkers(\n    data = jurisdiction_summary,\n    lng = ~lng_adj,\n    lat = ~lat_adj,\n    label = ~abb,\n    labelOptions = labelOptions(\n      noHide = TRUE,\n      direction = \"center\",\n      textOnly = FALSE,\n      style = list(\n        \"background\" = \"white\",\n        \"border\" = \"2px solid #1a5276\",\n        \"border-radius\" = \"3px\",\n        \"padding\" = \"2px 4px\",\n        \"font-weight\" = \"bold\",\n        \"font-size\" = \"10px\",\n        \"color\" = \"#1a5276\",\n        \"box-shadow\" = \"2px 2px 4px rgba(0,0,0,0.3)\"\n      )\n    )\n  ) %&gt;%\n  addCircleMarkers(\n    data = jurisdiction_summary,\n    lng = ~lng_adj,\n    lat = ~lat_adj,\n    stroke = TRUE,\n    weight = 1,\n    popup = ~paste0(\n      \"&lt;div style='font-size:12px'&gt;\",\n      \"&lt;b&gt;\", tools::toTitleCase(jurisdiction), \" (\", abb, \")&lt;/b&gt;&lt;br&gt;\",\n      \"Date: \",  format(capture_datetime, \"%Y-%m\"), \"&lt;br&gt;\",\n      \"Total: \", format(total_profiles, big.mark = \",\"), \"&lt;br&gt;\",\n      \"Offender: \", format(offender, big.mark = \",\"), \"&lt;br&gt;\",\n      \"Arrestee: \", format(arrestee, big.mark = \",\"), \"&lt;br&gt;\",\n      \"Forensic: \", format(forensic, big.mark = \",\"),\n      \"&lt;/div&gt;\"\n    )\n  ) %&gt;%\n  addControl(\n    html = \"&lt;div style='background:white;padding:5px;border:2px solid #1a5276;border-radius:3px;font-weight:bold;'&gt;NDIS 2025 State Participation&lt;/div&gt;\",\n    position = \"topright\"\n  ) %&gt;%\n  setView(lng = -98.5833, lat = 39.8333, zoom = 4)\n\n\n\n\n\n\n\n\nNDIS Time Series Dataset\nThe NDIS_time_series.csv dataset retains key temporal, jurisdictional, and operational metrics that can be used for further analysis and visualization.\n\n\n\n\n\n\n\n\nColumn\nType\nDescription\n\n\n\n\ncapture_datetime\nPOSIXct\nFull timestamp of data capture, parsed from the raw timestamp field (YYYY-MM-DD HH:MM:SS).\n\n\nasof_date\nDate\nStandardized date representing the reporting period (asof_year + asof_month, first day of month).\n\n\njurisdiction\nCharacter\nName or code of the reporting jurisdiction (e.g., â€œCaliforniaâ€, â€œTexasâ€).\n\n\noffender_profiles\nNumeric\nNumber of DNA profiles from known offenders in the jurisdiction.\n\n\narrestee\nNumeric\nNumber of DNA profiles collected from arrestees.\n\n\nforensic_profiles\nNumeric\nNumber of DNA profiles developed from forensic (crime scene) samples.\n\n\ntotal_profiles\nNumeric\nSum of offender, arrestee, and forensic profiles for each record.\n\n\nndis_labs\nInteger\nCount of laboratories actively participating in NDIS for the given jurisdiction and month.\n\n\ninvestigations_aided\nNumeric\nNumber of investigations aided by NDIS matches in the reporting period.\n\n\n\n\n\nShow interactive table code\nsummary_table &lt;- ndis_clean %&gt;%\n  group_by(jurisdiction, capture_datetime) %&gt;%\n  arrange(jurisdiction, capture_datetime) %&gt;%\n  select(\n    capture_datetime, asof_date, jurisdiction,\n    offender_profiles, arrestee, forensic_profiles, total_profiles,\n    ndis_labs, investigations_aided\n  )\n\n# Count the number of numeric columns (excluding the first 2 grouping columns)\nnumeric_cols_start &lt;- 3\nnumeric_cols_end &lt;- ncol(summary_table)  \n\n# Interactive table\ndatatable(\n  summary_table,\n  extensions = c('Buttons', 'ColReorder', 'Scroller'),\n  options = list(\n    dom = 'Bfrtip',\n    buttons = c('copy', 'csv', 'excel', 'colvis'),\n    scrollX = TRUE,\n    scrollY = \"600px\",\n    scroller = TRUE,\n    pageLength = 20,\n    columnDefs = list(\n      list(className = 'dt-right', targets = (numeric_cols_start-1):(numeric_cols_end-1))\n    )\n  ),\n  rownames = FALSE,\n  filter = 'top'\n)\n\n\n\n\n\n\n\n\nExport Cleaned Dataset\nAfter cleaning and processing the NDIS data, the final dataset is exported as a CSV file for further analysis or sharing. The file is saved to the data/v1.0/ directory to maintain an organized workflow.\nOutput: NDIS_time_series.csv\n\n\nColumnTypeRowsMissingUniqueFirst_Valuescapture_datetimePOSIXct, POSIXt29512092002003-06-26 00:35:13, 2004-02-05 18:34:48, 2005-01-26 15:39:56asof_dateDate295128827123&lt;NA&gt;, &lt;NA&gt;, &lt;NA&gt;jurisdictionfactor29512054Alabama, Alabama, Alabamaoffender_profilesnumeric29512073572507, 9231, 31542arresteenumeric29512024940, 0, 0forensic_profilesnumeric295120621822, 24, 676total_profilesnumeric29512080172529, 9255, 32218ndis_labsnumeric295120234, 4, 4investigations_aidednumeric2951204274299, 299, 434Data frame dimensions: 29512 rows Ã— 9 columns\n\n\n\n\nShow dataset exportation code\nndis_clean &lt;- ndis_clean %&gt;%\n  select(\n    capture_datetime, asof_date, jurisdiction,\n    offender_profiles, arrestee, forensic_profiles, total_profiles,\n    ndis_labs, investigations_aided\n  )\n\n# Create final frozen version (v1.0)\nfinal_dir &lt;- here(\"data\", \"ndis\", \"final\")\ndir.create(final_dir, recursive = TRUE, showWarnings = FALSE)\nfinal_path &lt;- here(final_dir, \"NDIS_time_series.csv\")\n\nwrite_csv(ndis_clean, final_path)\ncat(paste(\"âœ“ Created final version at:\", final_path, \"\\n\"))\n\n\nâœ“ Created final version at: C:/Users/Donadio/Documents/PODFRIDGE-3/PODFRIDGE-Databases/data/ndis/final/NDIS_time_series.csv \n\n\nShow dataset exportation code\nfrozen_dir &lt;- here(\"data\", \"v1.0\")\ndir.create(frozen_dir, recursive = TRUE, showWarnings = FALSE)\nfrozen_path &lt;- here(frozen_dir, \"NDIS_time_series.csv\")\n\nwrite_csv(ndis_clean, frozen_path)\ncat(paste(\"âœ“ Created frozen version 1.0 at:\", frozen_path, \"\\n\"))\n\n\nâœ“ Created frozen version 1.0 at: C:/Users/Donadio/Documents/PODFRIDGE-3/PODFRIDGE-Databases/data/v1.0/NDIS_time_series.csv"
  },
  {
    "objectID": "qmd_root/foia_processing.html",
    "href": "qmd_root/foia_processing.html",
    "title": "FOIA Document OCR Processing",
    "section": "",
    "text": "This document details the processing of Freedom of Information Act (FOIA) responses from seven U.S. states regarding the demographic composition of their State DNA Index System (SDIS) databases. These responses were obtained by Professor Erin Murphy (NYU Law) in 2018 as part of research on racial disparities in DNA databases."
  },
  {
    "objectID": "qmd_root/foia_processing.html#data-sources",
    "href": "qmd_root/foia_processing.html#data-sources",
    "title": "FOIA Document OCR Processing",
    "section": "2.1 Data Sources",
    "text": "2.1 Data Sources\n\n2.1.1 Raw FOIA Responses\nThe original FOIA responses are stored in two formats:\n\nPDFs: raw/foia_pdfs/ - Original scanned documents\nHTML: raw/foia_html/ - OCRâ€™d versions for easier extraction\n\n\n\nShow setup code\n# List of required packages\nrequired_packages &lt;- c(\n  \"tidyverse\",    # Data manipulation and visualization\n  \"here\",         # File path management\n  \"knitr\",        # Dynamic report generation\n  \"kableExtra\",   # Enhanced table formatting\n  \"ggplot2\",      # Data visualization\n  \"patchwork\",    # Plot composition and layout\n  \"scales\",       # Axis scaling and formatting\n  \"tidyr\",        # Data tidying and reshaping\n  \"tibble\",       # Modern data frames\n  \"flextable\",    # Advanced table formatting\n  \"DT\",           # Interactive tables\n  \"cowplot\",      # Plotting composition\n  \"sf\",           # Simple Features for spatial data\n  \"usmap\"        # Mapping US states\n)\n  \n\n# Function to install missing packages\ninstall_missing &lt;- function(packages) {\n  for (pkg in packages) {\n    if (!requireNamespace(pkg, quietly = TRUE)) {\n      message(paste(\"Installing missing package:\", pkg))\n      install.packages(pkg, dependencies = TRUE)\n    }\n  }\n}\n\n# Install any missing packages\ninstall_missing(required_packages)\n\n# Load all packages\nsuppressPackageStartupMessages({\n  library(tidyverse)\n  library(here)\n  library(knitr)\n  library(kableExtra)\n  library(ggplot2)\n  library(patchwork)\n  library(scales)\n  library(tidyr)\n  library(tibble)\n  library(flextable)\n  library(cowplot)\n  library(sf)\n  library(usmap)\n})\n\n# Verify all packages loaded successfully\nloaded_packages &lt;- sapply(required_packages, require, character.only = TRUE)\nif (all(loaded_packages)) {\n  message(\"All packages loaded successfully!\")\n} else {\n  warning(\"The following packages failed to load: \", \n          paste(names(loaded_packages)[!loaded_packages], collapse = \", \"))\n}\n\n# Display options\noptions(tibble.width = Inf)\noptions(dplyr.summarise.inform = FALSE)\n\n# Path to per-state files (run notebook from analysis/)\nbase_dir &lt;- here(\"..\")\nper_state &lt;- here(\"data\", \"foia\", \"intermediate\")\n\n# ------------------------------------------------------------------\n# 1. Discover available per-state CSV files\n# ------------------------------------------------------------------\nstate_files &lt;- list.files(per_state, pattern = \"*_foia_data\\\\.csv$\", full.names = TRUE)\n\nif (length(state_files) == 0) {\n  stop(paste(\"No per-state FOIA files found in\", per_state, \". Check the folder path.\"))\n}\n\nstem_to_state &lt;- function(stem) {\n  toks &lt;- str_split(stem, \"_\")[[1]]\n  if (\"foia\" %in% toks) {\n    toks &lt;- toks[1:(which(toks == \"foia\") - 1)]\n  }\n  paste(tools::toTitleCase(toks), collapse = \" \")\n}\n\nstates_available &lt;- map_chr(basename(state_files), ~ stem_to_state(str_remove(.x, \"_foia_data\\\\.csv\")))\n\ncat(paste(\"âœ“ Found\", length(state_files), \"per-state files:\\n\"))\nfor (s in states_available) {\n  cat(paste(\"  â€¢\", s, \"\\n\"))\n}\n\n# ------------------------------------------------------------------\n# 2. Initialize empty containers for the loop that follows\n# ------------------------------------------------------------------\nfoia_combined &lt;- tibble()\nfoia_state_metadata &lt;- list()\n\n\nâœ“ Found 7 per-state files:\n  â€¢ California \n  â€¢ Florida \n  â€¢ Indiana \n  â€¢ Maine \n  â€¢ Nevada \n  â€¢ South Dakota \n  â€¢ Texas"
  },
  {
    "objectID": "qmd_root/foia_processing.html#processing-workflow",
    "href": "qmd_root/foia_processing.html#processing-workflow",
    "title": "FOIA Document OCR Processing",
    "section": "2.2 Processing workflow",
    "text": "2.2 Processing workflow\nFor transparency, each state file is processed independently then merged into a single combined longâ€‘format table (foia_combined):\n\nLoad one file per state from data/foia/intermediate/.\nAppend its rows to foia_combined. A parallel dataframe, foia_state_metadata, records what each state reported (counts, percentages, which categories) and any state-specific characteristics (e.g.Â Nevadaâ€™s â€œflagsâ€ terminology).\nQualityâ€‘check each state:\n\nverify that race and gender percentages sum to â‰ˆ 100 % when provided,\nconfirm that demographic counts sum to the stateâ€™s reported total profiles,\ncalculate any missing counts or percentages and tag those rows value_source = \"calculated\".\n\nSave outputs\n\ndata/foia/final/foia_data_clean.csv â€” the fully combined tidy table with both reported and calculated values,\ndata/foia/intermediate/foia_state_metadata.csv â€” one row per state summarising coverage and caveats. After QC passes, freeze foia_data_clean.csv to data/v1.0/FOIA_demographics.csv."
  },
  {
    "objectID": "qmd_root/foia_processing.html#helper-functions",
    "href": "qmd_root/foia_processing.html#helper-functions",
    "title": "FOIA Document OCR Processing",
    "section": "2.3 Helper Functions",
    "text": "2.3 Helper Functions\nThe functions below perform each transformation required for harmonizing the stateâ€‘level FOIA tables.\n\n2.3.1 Data Processing Helper Functions Reference\n\n\n\n\n\n\n\n\nFunction\nDefinition\nParameters\n\n\n\n\nload_state()\nLoads and preprocesses state FOIA data files, handling numeric conversion and validation\npath: File path to state CSV\n\n\nenhanced_glimpse()\nProvides an enhanced data overview with column types, missing values, unique counts, and unique values\ndf: Input dataframe\n\n\nfill_demographic_gaps()\nFills missing gender counts and adds Unknown race category when totals permit calculation\ndf: Input dataframe\n\n\nadd_combined()\nCreates Combined offender type by summing Convicted Offender and Arrestee counts when missing\ndf: Input dataframe\n\n\nadd_percentages()\nDerives percentage values from counts for all demographic categories\ndf: Input dataframe\n\n\ncounts_consistent()\nVerifies that demographic counts sum to total_profiles for each offender type\ndf: Input dataframe\n\n\npercentages_consistent()\nVerifies that percentages sum to 100 Â± 0.5% for each category\ndf: Input dataframe\n\n\nreport_status()\nReports what data types (counts/percentages/both) are available for a category\ndf: Input dataframe, category: race or gender\n\n\nverify_category_totals()\nCompares demographic sums against reported totals and shows differences\ndf: Input dataframe\n\n\nverify_percentage_consistency()\nCompares reported vs calculated percentages for consistency\ndf_combined: Combined dataframe, state_name: State name\n\n\ncalculate_combined_totals()\nCalculates Combined totals by summing across offender types\ndf: Input dataframe, state_name: State name\n\n\ncalculate_percentages()\nCalculates percentages from counts for demographic categories\ndf_combined: Combined dataframe, state_name: State name\n\n\ncalculate_counts_from_percentages()\nCalculates counts from percentages for demographic categories\ndf_combined: Combined dataframe, state_name: State name\n\n\nstandardize_offender_types()\nStandardizes offender type names to consistent terminology\ndf: Input dataframe\n\n\nprepare_state_for_combined()\nPrepares state data for inclusion in combined dataset with proper columns\ndf: Input dataframe, state_name: State name\n\n\nformat_compact()\nFormats large numbers with K/M suffixes for readability\nx: Numeric value\n\n\ncreate_pie_chart()\nCreates pie charts for specific demographic categories\ndata: Input data, offender_type, category, value_type, title, show_values\n\n\ncreate_state_visualizations()\nCreates comprehensive pie chart visualizations for all metrics\ndf_combined: Combined dataframe, state_name: State name\n\n\ncreate_demographic_bar_charts()\nCreates side-by-side bar charts for gender and race distributions\ndf_combined: Combined dataframe, state_name: State name\n\n\nadd_state_metadata()\nCreates and appends a metadata record capturing state data characteristics including available offender types, demographic categories, data formats, and special features\ndf: Input dataframe, state_name: State name\n\n\nupdate_state_metadata()\nModifies existing state metadata to update QC results (count/percentage consistency) and append validation notes\nstate_name: State name, counts_ok: Count consistency flag, percentages_ok: Percentage consistency flag, notes_text: Additional notes\n\n\n\n\n\nShow helper functions setup\n# Columns retained from every raw table\nCOLS_NEEDED &lt;- c(\"state\", \"offender_type\", \"variable_category\",\n                 \"variable_detailed\", \"value\", \"value_type\")\n\n# ------------------------------------------------------------------\n# 1. Load and preprocess state files\n# ------------------------------------------------------------------\nload_state &lt;- function(path) {\n  \"\n  Read a *_foia_data.csv* file, enforce column order,\n  and convert &lt;1 to 0.5 so that trace counts are retained.\n  Execution halts if non-numeric values remain.\n  \"\n  df &lt;- read_csv(path, show_col_types = FALSE)\n  if (!\"state\" %in% colnames(df)) {\n    df &lt;- df %&gt;%\n      mutate(state = str_remove(basename(path), \"_foia_data\\\\.csv\") %&gt;%\n        str_replace_all(\"_\", \" \") %&gt;%\n        tools::toTitleCase())\n  }\n  df &lt;- df %&gt;% select(all_of(COLS_NEEDED))\n  df$value_source &lt;- \"reported\"\n\n  df &lt;- df %&gt;%\n    mutate(value = ifelse(value == \"&lt;1\", 0.5, value),\n           value = as.numeric(value))\n  \n  nonnumeric &lt;- df %&gt;% filter(is.na(value))\n  if (nrow(nonnumeric) &gt; 0) {\n    cat(paste(\"**Non-numeric rows in\", basename(path), \"; please amend**\\n\"))\n    print(nonnumeric)\n    stop(\"Numeric coercion failure\")\n  }\n  return(df)\n}\n\n\n# ------------------------------------------------------------------\n# 2. Enhanced glimpse\n# ------------------------------------------------------------------\n# Display data types for each column with unique values\nenhanced_glimpse &lt;- function(df) {\n  glimpse_data &lt;- data.frame(\n    Column = names(df),\n    Type = sapply(df, function(x) paste(class(x), collapse = \", \")),\n    Rows = nrow(df),\n    Missing = sapply(df, function(x) sum(is.na(x))),\n    Unique = sapply(df, function(x) length(unique(x))),\n    Unique_Values = sapply(df, function(x) {\n      unique_vals &lt;- unique(x)\n      if (length(unique_vals) &gt; 10) {\n        paste(encodeString(as.character(unique_vals[1:10])), collapse = \", \", \"...\")\n      } else {\n        paste(encodeString(as.character(unique_vals)), collapse = \", \")\n      }\n    })\n  )\n  \n  ft &lt;- flextable(glimpse_data) %&gt;%\n    theme_zebra() %&gt;%\n    set_caption(paste(\"Enhanced Data Glimpse:\", deparse(substitute(df)))) %&gt;%\n    autofit() %&gt;%\n    align(align = \"left\", part = \"all\") %&gt;%\n    colformat_num(j = c(\"Rows\", \"Missing\", \"Unique\"), big.mark = \"\") %&gt;%\n    bg(j = \"Missing\", bg = function(x) ifelse(x &gt; 0, \"#FFF3CD\", \"transparent\")) %&gt;%\n    bg(j = \"Unique\", bg = function(x) ifelse(x == 1, \"#FFF3CD\", \"transparent\")) %&gt;%\n    add_footer_lines(paste(\"Data frame dimensions:\", nrow(df), \"rows Ã—\", ncol(df), \"columns\")) %&gt;%\n    fontsize(size = 10, part = \"all\") %&gt;%\n    set_table_properties(layout = \"autofit\", width = 1)\n  \n  return(ft)\n}\n\n# ------------------------------------------------------------------\n# 3. Fill missing Male counts and Unknown race counts\n# ------------------------------------------------------------------\nfill_demographic_gaps &lt;- function(df) {\n  \"If exactly one gender or the Unknown race category is absent and\n  totals permit a residual, calculate and insert the missing count.\n  \"\n  inserts &lt;- list()\n  \n  for (ot in unique(df$offender_type)) {\n    tot &lt;- df %&gt;%\n      filter(offender_type == ot,\n             variable_category == \"total\",\n             variable_detailed == \"total_profiles\",\n             value_type == \"count\")\n    \n    if (nrow(tot) == 0) next\n    \n    total &lt;- tot$value[1]\n    \n    # gender residual ----------------------------------------------\n    g &lt;- df %&gt;%\n      filter(offender_type == ot,\n             variable_category == \"gender\",\n             value_type == \"count\")\n    \n    missing_gender &lt;- setdiff(c(\"Male\", \"Female\"), unique(g$variable_detailed))\n    if (nrow(g) == 1 && length(missing_gender) == 1) {\n      inserts[[length(inserts) + 1]] &lt;- tibble(\n        state = df$state[1],\n        offender_type = ot,\n        variable_category = \"gender\",\n        variable_detailed = missing_gender,\n        value = total - sum(g$value),\n        value_type = \"count\",\n        value_source = \"calculated\"\n      )\n    }\n    \n    # race residual -----------------------------------------------\n    r &lt;- df %&gt;%\n      filter(offender_type == ot,\n             variable_category == \"race\",\n             value_type == \"count\")\n    \n    if (nrow(r) &gt; 0 && !\"Unknown\" %in% r$variable_detailed) {\n      gap &lt;- total - sum(r$value)\n      if (gap &gt; 0) {\n        inserts[[length(inserts) + 1]] &lt;- tibble(\n          state = df$state[1],\n          offender_type = ot,\n          variable_category = \"race\",\n          variable_detailed = \"Unknown\",\n          value = gap,\n          value_type = \"count\",\n          value_source = \"calculated\"\n        )\n      }\n    }\n  }\n  \n  if (length(inserts) &gt; 0) {\n    df &lt;- bind_rows(df, bind_rows(inserts))\n  }\n  return(df)\n}\n\n# ------------------------------------------------------------------\n# 4. Construct Combined offender type if absent (add_combined)\n# ------------------------------------------------------------------\nadd_combined &lt;- function(df) {\n  \"\n  When a state reports Convicted Offender and Arrestee counts but\n  omits Combined, create a Combined block by summing the two.\n  \"\n  if (\"Combined\" %in% df$offender_type) return(df)\n  \n  required &lt;- c(\"Convicted Offender\", \"Arrestee\")\n  if (!all(required %in% df$offender_type)) return(df)  # cannot construct\n  \n  summed &lt;- df %&gt;%\n    filter(value_type == \"count\") %&gt;%\n    group_by(variable_category, variable_detailed, value_type) %&gt;%\n    summarise(value = sum(value), .groups = \"drop\") %&gt;%\n    mutate(state = df$state[1],\n           offender_type = \"Combined\",\n           value_source = \"calculated\")\n  \n  return(bind_rows(df, summed))\n}\n\n# ------------------------------------------------------------------\n# 5. Derive percentages wherever only counts exist (add_percentages)\n# ------------------------------------------------------------------\nadd_percentages &lt;- function(df) {\n  \"\n  Ensure that every gender and race row has both count and percentage\n  values, derived from the offender-type total if necessary.\n  \"\n  totals &lt;- df %&gt;%\n    filter(variable_category == \"total\",\n           variable_detailed == \"total_profiles\",\n           value_type == \"count\") %&gt;%\n    select(offender_type, value) %&gt;%\n    deframe()\n  \n  need_pct &lt;- df %&gt;%\n    filter(value_type == \"count\",\n           variable_category != \"total\")\n  \n  new_pct_rows &lt;- need_pct %&gt;%\n    rowwise() %&gt;%\n    mutate(has_percentage = nrow(df %&gt;%\n           filter(state == state,\n                  offender_type == offender_type,\n                  variable_category == variable_category,\n                  variable_detailed == variable_detailed,\n                  value_type == \"percentage\"))) %&gt;%\n    filter(has_percentage == 0) %&gt;%\n    mutate(value = round(value / totals[offender_type] * 100, 2),\n           value_type = \"percentage\",\n           value_source = \"calculated\") %&gt;%\n    select(-has_percentage)\n  \n  if (nrow(new_pct_rows) &gt; 0) {\n    df &lt;- bind_rows(df, new_pct_rows)\n  }\n  return(df)\n}\n\n# ------------------------------------------------------------------\n# 6. Counts consistency checks\n# ------------------------------------------------------------------\ncounts_consistent &lt;- function(df) {\n  \"\n  Verifies that demographic counts sum to total_profiles for each\n  offender type and category.\n  \"\n  demo_sum &lt;- df %&gt;%\n    filter(value_type == \"count\",\n           variable_category != \"total\") %&gt;%\n    group_by(offender_type, variable_category) %&gt;%\n    summarise(sum_value = sum(value), .groups = \"drop\")\n  \n  totals &lt;- df %&gt;%\n    filter(variable_category == \"total\",\n           variable_detailed == \"total_profiles\",\n           value_type == \"count\") %&gt;%\n    select(offender_type, value)\n  \n  merged &lt;- demo_sum %&gt;%\n    left_join(totals, by = \"offender_type\") %&gt;%\n    mutate(diff = abs(sum_value - value))\n  \n  all(merged$diff &lt; 1e-6)\n}\n\n# ------------------------------------------------------------------\n# 7. Percentage consistency checks\n# ------------------------------------------------------------------\n\npercentages_consistent &lt;- function(df) {\n  \"\n  Verifies that derived or reported percentages sum to 100 Â± 0.5 %.\n  \"\n  result &lt;- df %&gt;%\n    filter(value_type == \"percentage\") %&gt;%\n    group_by(offender_type, variable_category) %&gt;%\n    summarise(sum_value = sum(value), .groups = \"drop\") %&gt;%\n    mutate(consistent = abs(sum_value - 100) &lt;= 0.5)\n  \n  all(result$consistent)\n}\n\n\n# ------------------------------------------------------------------\n# 8. Report status for each category\n# ------------------------------------------------------------------\n\n# Define columns needed for foia_combined\nreport_status &lt;- function(df, category) {\n  values &lt;- unique(df$value_type[df$variable_category == category])\n  \n  if (all(c(\"count\", \"percentage\") %in% values)) {\n    return(\"both\")\n  } else if (\"count\" %in% values) {\n    return(\"counts\")\n  } else if (\"percentage\" %in% values) {\n    return(\"percentages\")\n  } else {\n    return(\"neither\")\n  }\n}\n\n# ------------------------------------------------------------------\n# 9. Verify category totals\n# ------------------------------------------------------------------\n\nverify_category_totals &lt;- function(df) {\n  # 1 pull total_profiles per offender_type\n  total_map &lt;- df %&gt;%\n    filter(variable_category == \"total\", \n           variable_detailed == \"total_profiles\") %&gt;%\n    select(offender_type, value) %&gt;%\n    deframe() %&gt;%\n    as.list()\n  \n  # 2 sum counts by offender_type and variable_category\n  demo_sum &lt;- df %&gt;%\n    filter(value_type == \"count\",\n           variable_category != \"total\") %&gt;%\n    group_by(offender_type, variable_category) %&gt;%\n    summarise(sum_counts = sum(value, na.rm = TRUE), .groups = \"drop\")\n  \n  # 3 attach total_profiles and compute difference\n  demo_sum &lt;- demo_sum %&gt;%\n    mutate(total_profiles = map_dbl(offender_type, ~total_map[[.x]]),\n           difference = total_profiles - sum_counts)\n  \n  # tidy columns order\n  demo_sum %&gt;%\n    select(offender_type, variable_category, total_profiles, \n           sum_counts, difference)\n}\n\n# ------------------------------------------------------------------\n# 10. Calculate Combined totals\n# ------------------------------------------------------------------\n\ncalculate_combined_totals &lt;- function(df, state_name) {\n  # Get all counts\n  counts_df &lt;- df %&gt;%\n    filter(value_type == 'count') %&gt;%\n    mutate(value_source = 'calculated')\n  \n  # Group by variable_category and variable_detailed, sum values\n  combined_sums &lt;- counts_df %&gt;%\n    group_by(variable_category, variable_detailed) %&gt;%\n    summarise(value = sum(value, na.rm = TRUE), .groups = \"drop\")\n  \n  # Create Combined rows\n  combined_rows &lt;- combined_sums %&gt;%\n    mutate(state = state_name,\n           offender_type = 'Combined',\n           value_type = 'count',\n           value_source = 'calculated') %&gt;%\n    select(all_of(COLS_NEEDED), value_source)\n  \n  return(combined_rows)\n}\n\n# ------------------------------------------------------------------\n# 11. Calculate percentages from counts\n# ------------------------------------------------------------------\n\ncalculate_percentages &lt;- function(df_combined, state_name) {\n  # Get total profiles for each offender type\n  totals_map &lt;- df_combined %&gt;%\n    filter(state == state_name,\n           variable_category == 'total',\n           variable_detailed == 'total_profiles') %&gt;%\n    select(offender_type, value) %&gt;%\n    deframe() %&gt;%\n    as.list()\n  \n  percentage_rows &lt;- list()\n  \n  for (offender_type in names(totals_map)) {\n    total &lt;- totals_map[[offender_type]]\n    \n    # Get all demographic counts\n    demo_data &lt;- df_combined %&gt;%\n      filter(state == state_name,\n             offender_type == !!offender_type,\n             variable_category %in% c('gender', 'race'),\n             value_type == 'count')\n    \n    if (nrow(demo_data) &gt; 0) {\n      # Calculate percentage for each\n      demo_percentages &lt;- demo_data %&gt;%\n        mutate(value = round((value / total) * 100, 2),\n               value_type = 'percentage',\n               value_source = 'calculated') %&gt;%\n        select(all_of(COLS_NEEDED), value_source)\n      \n      percentage_rows &lt;- c(percentage_rows, list(demo_percentages))\n    }\n  }\n  \n  bind_rows(percentage_rows)\n}\n\n# ------------------------------------------------------------------\n# 12. Calculate counts from percentages\n# ------------------------------------------------------------------\n\ncalculate_counts_from_percentages &lt;- function(df_combined, state_name) {\n  # Get total profiles for each offender type\n  totals_map &lt;- df_combined %&gt;%\n    filter(state == state_name,\n           variable_category == 'total',\n           variable_detailed == 'total_profiles') %&gt;%\n    select(offender_type, value) %&gt;%\n    deframe() %&gt;%\n    as.list()\n  \n  count_rows &lt;- list()\n  \n  for (offender_type in names(totals_map)) {\n    total &lt;- totals_map[[offender_type]]\n    \n    # Get all demographic percentages\n    demo_data &lt;- df_combined %&gt;%\n      filter(state == state_name,\n             offender_type == !!offender_type,\n             variable_category %in% c('gender', 'race'),\n             value_type == 'percentage')\n    \n    if (nrow(demo_data) &gt; 0) {\n      # Calculate count for each\n      demo_counts &lt;- demo_data %&gt;%\n        mutate(value = as.integer(round(total * (value / 100))),\n               value_type = 'count',\n               value_source = 'calculated') %&gt;%\n        select(all_of(COLS_NEEDED), value_source)\n      \n      count_rows &lt;- c(count_rows, list(demo_counts))\n    }\n  }\n  \n  bind_rows(count_rows)\n}\n\n# ------------------------------------------------------------------\n# 13. Standardize offender types\n# ------------------------------------------------------------------\n\nstandardize_offender_types &lt;- function(df) {\n  replacements &lt;- c(\n    'Offenders' = 'Convicted Offender',\n    'Convicted offenders' = 'Convicted Offender',\n    'Arrested offender' = 'Arrestee',\n    'All' = 'Combined'\n  )\n  \n  df %&gt;%\n    mutate(offender_type = recode(offender_type, !!!replacements))\n}\n\n# ------------------------------------------------------------------\n# 14. Prepare state data for combined dataset\n# ------------------------------------------------------------------\n\nprepare_state_for_combined &lt;- function(df, state_name) {\n  \n  df_prepared &lt;- df %&gt;%\n    select(any_of(COLS_NEEDED), value_source)\n  \n  df_prepared &lt;- df_prepared %&gt;%\n      mutate(value_source = case_when(\n        is.na(value_source) ~ \"calculated\",\n        value_source == \"\" ~ \"calculated\",\n        TRUE ~ value_source\n      ))\n\n  \n  df_prepared\n}\n\n# ------------------------------------------------------------------\n# 15. Compare reported vs calculated percentages\n# ------------------------------------------------------------------\n\nverify_percentage_consistency &lt;- function(df_combined, state_name) {\n  state_data &lt;- df_combined %&gt;%\n    filter(state == state_name)\n  \n  # Get all offender types that have both counts and percentages\n  offender_types &lt;- unique(state_data$offender_type)\n  \n  consistency_results &lt;- list()\n  \n  for (offender_type in offender_types) {\n    offender_data &lt;- state_data %&gt;%\n      filter(offender_type == !!offender_type)\n    \n    # Check if we have both reported and calculated percentages\n    for (category in c('gender', 'race')) {\n      reported_pcts &lt;- offender_data %&gt;%\n        filter(variable_category == !!category,\n               value_type == 'percentage',\n               value_source == 'reported')\n      \n      calculated_pcts &lt;- offender_data %&gt;%\n        filter(variable_category == !!category,\n               value_type == 'percentage',\n               value_source == 'calculated')\n      \n      if (nrow(reported_pcts) &gt; 0 && nrow(calculated_pcts) &gt; 0) {\n        # Compare each demographic value\n        for (i in 1:nrow(reported_pcts)) {\n          rep_row &lt;- reported_pcts[i, ]\n          calc_match &lt;- calculated_pcts %&gt;%\n            filter(variable_detailed == rep_row$variable_detailed)\n          \n          if (nrow(calc_match) &gt; 0) {\n            diff &lt;- abs(rep_row$value - calc_match$value[1])\n            consistency_results &lt;- c(consistency_results, list(data.frame(\n              offender_type = offender_type,\n              category = category,\n              variable = rep_row$variable_detailed,\n              reported = rep_row$value,\n              calculated = calc_match$value[1],\n              difference = diff,\n              consistent = diff &lt; 0.5\n            )))\n          }\n        }\n      }\n    }\n  }\n  \n  if (length(consistency_results) &gt; 0) {\n    consistency_df &lt;- bind_rows(consistency_results)\n    cat(paste0(\"\\nPercentage consistency check for \", state_name, \":\\n\"))\n    cat(paste0(\"All values consistent: \", all(consistency_df$consistent), \"\\n\"))\n    \n    if (!all(consistency_df$consistent)) {\n      cat(\"\\nInconsistent values:\\n\")\n      print(consistency_df %&gt;% filter(!consistent))\n    }\n    \n    return(all(consistency_df$consistent))\n  } else {\n    # No comparison possible - state only has one type of data\n    return(TRUE)\n  }\n}\n# ------------------------------------------------------------------\n# 16. Add compact formatting for large numbers\n# ------------------------------------------------------------------\n\nformat_compact &lt;- function(x) {\n  sapply(x, function(single_x) {\n    if (single_x &gt;= 1000000) {\n      if (single_x/1000000 == as.integer(single_x/1000000)) {\n        return(paste0(as.integer(single_x/1000000), \"M\"))\n      } else {\n        return(paste0(round(single_x/1000000, 1), \"M\"))\n      }\n    } else if (single_x &gt;= 1000) {\n      return(paste0(as.integer(single_x/1000), \"k\"))\n    } else {\n      return(paste0(as.integer(single_x)))\n    }\n  })\n}\n\n# ------------------------------------------------------------------\n# 17. Pie chart creation function\n# ------------------------------------------------------------------\n\ncreate_pie_chart &lt;- function(data, offender_type, category, value_type, title, show_values = FALSE) {\n  chart_data &lt;- data %&gt;%\n    filter(offender_type == !!offender_type,\n           variable_category == !!category,\n           value_type == !!value_type)\n  \n  # Check if we have data after filtering\n  if (nrow(chart_data) == 0) {\n    plot.new()\n    title(main = title, cex.main = 0.9)\n    text(0.5, 0.5, \"No data\", cex = 0.8)\n    return()\n  }\n  \n  # AGGREGATE DATA TO REMOVE DUPLICATES - KEY FIX\n  chart_data &lt;- chart_data %&gt;%\n    group_by(variable_detailed) %&gt;%\n    summarise(value = sum(value, na.rm = TRUE)) %&gt;%\n    ungroup()\n  \n  # Ensure consistent categories\n  if (category == 'gender') {\n    all_genders &lt;- data.frame(variable_detailed = c('Male', 'Female', 'Unknown'))\n    chart_data &lt;- chart_data %&gt;%\n      right_join(all_genders, by = \"variable_detailed\") %&gt;%\n      mutate(value = ifelse(is.na(value), 0, value)) %&gt;%\n      arrange(factor(variable_detailed, levels = c('Male', 'Female', 'Unknown')))\n  } else if (category == 'race') {\n    all_races &lt;- data.frame(variable_detailed = c('White', 'Black', 'Hispanic', \n                                                 'Asian', 'Native American', 'Other', 'Unknown'))\n    chart_data &lt;- chart_data %&gt;%\n      right_join(all_races, by = \"variable_detailed\") %&gt;%\n      mutate(value = ifelse(is.na(value), 0, value)) %&gt;%\n      arrange(factor(variable_detailed, levels = c('White', 'Black', 'Hispanic', \n                                                  'Asian', 'Native American', 'Other', 'Unknown')))\n  }\n  \n  # Filter out zero values and ensure we have data\n  chart_data &lt;- chart_data %&gt;% filter(value &gt; 0)\n  \n  if (nrow(chart_data) == 0) {\n    plot.new()\n    title(main = title, cex.main = 0.9)\n    text(0.5, 0.5, \"No data\", cex = 0.8)\n    return()\n  }\n  \n  # Define colors\n  if (category == 'gender') {\n    colors &lt;- c('Male' = '#4E79A7', 'Female' = '#E15759', 'Unknown' = '#BAB0AC')\n  } else {\n    colors &lt;- c('White' = '#4E79A7', 'Black' = '#F25E2B', 'Hispanic' = '#E14759',\n               'Asian' = '#76B7B2', 'Native American' = '#59A14F',\n               'Other' = '#9C755F', 'Unknown' = '#BAB0AC')\n  }\n  \n  # Filter colors to only include categories present in data\n  pie_colors &lt;- colors[names(colors) %in% chart_data$variable_detailed]\n  \n  # Calculate percentages\n  total_value &lt;- sum(chart_data$value)\n  chart_data &lt;- chart_data %&gt;%\n    mutate(pct = value / total_value * 100)\n  \n  # Create labels based on value_type and show_values\n  if (show_values && value_type == 'count') {\n    chart_data &lt;- chart_data %&gt;%\n      mutate(base_label = paste0(variable_detailed, \"\\n(\", format(value, big.mark = \",\"), \")\"))\n  } else if (value_type == 'percentage') {\n    chart_data &lt;- chart_data %&gt;%\n      mutate(base_label = paste0(variable_detailed, \"\\n(\", round(value, 1), \"%)\"))\n  } else {\n    chart_data &lt;- chart_data %&gt;%\n      mutate(base_label = variable_detailed)\n  }\n  \n  # Only show labels for slices &gt;= 3%, otherwise empty string\n  chart_data &lt;- chart_data %&gt;%\n    mutate(label = ifelse(pct &gt;= 3, base_label, \"\"))\n  \n  # Create the pie chart\n  pie(chart_data$value, \n      labels = chart_data$label,\n      main = title,\n      col = pie_colors,\n      cex.main = 0.9,\n      cex = 0.8)\n  \n  # Add legend for small slices\n  small_slices &lt;- chart_data %&gt;% filter(pct &lt; 3)\n  if (nrow(small_slices) &gt; 0) {\n    legend_labels &lt;- paste0(small_slices$variable_detailed, \" (\", round(small_slices$pct, 1), \"%)\")\n    legend_colors &lt;- pie_colors[small_slices$variable_detailed]\n    \n    legend(\"bottomright\", \n           legend = legend_labels,\n           fill = legend_colors,\n           cex = 0.7,\n           bty = \"n\")\n  }\n}\n\n# ------------------------------------------------------------------\n# 18. State visualizations with 2 pies per row\n# ------------------------------------------------------------------\n\ncreate_state_visualizations &lt;- function(df_combined, state_name) {\n  state_data &lt;- df_combined %&gt;% filter(state == state_name)\n  \n  offender_types &lt;- sort(unique(state_data$offender_type))\n  plots &lt;- list()\n  \n  for (offender_type in offender_types) {\n    plots &lt;- c(plots, list(\n      create_pie_chart(state_data, offender_type, 'gender', 'count',\n                       paste(offender_type, \"Gender Counts\"), TRUE),\n      create_pie_chart(state_data, offender_type, 'gender', 'percentage',\n                       paste(offender_type, \"Gender Percentages\")),\n      create_pie_chart(state_data, offender_type, 'race', 'count',\n                       paste(offender_type, \"Race Counts\"), TRUE),\n      create_pie_chart(state_data, offender_type, 'race', 'percentage',\n                       paste(offender_type, \"Race Percentages\"))\n    ))\n  }\n}\n\n# ------------------------------------------------------------------\n# 19. Demographic bar chart function\n# ------------------------------------------------------------------\n\ncreate_demographic_bar_charts &lt;- function(df_combined, state_name) {\n  state_data &lt;- df_combined %&gt;%\n    filter(state == state_name)\n  \n  # Get offender types and ensure Combined is last\n  offender_types &lt;- state_data %&gt;%\n    filter(value_type == 'count') %&gt;%\n    pull(offender_type) %&gt;%\n    unique() %&gt;%\n    sort()\n  \n  if ('Combined' %in% offender_types) {\n    offender_types &lt;- c(setdiff(offender_types, 'Combined'), 'Combined')\n  }\n  \n  # Color palettes\n  gender_colors &lt;- c('Male' = '#4E79A7', 'Female' = '#E15759', 'Unknown' = '#BAB0AC')\n  race_colors &lt;- c(\n    'White' = '#4E79A7', \n    'Black' = '#F25E2B', \n    'Hispanic' = '#E14759',\n    'Asian' = '#76B7B2',\n    'Native American' = '#59A14F',\n    'Other' = '#9C755F',\n    'Unknown' = '#BAB0AC'\n  )\n  \n  # Gender data - ensure no duplicates by summing values\n  gender_data &lt;- state_data %&gt;%\n    filter(variable_category == 'gender',\n           value_type == 'count') %&gt;%\n    group_by(offender_type, variable_detailed) %&gt;%\n    summarize(value = sum(value, na.rm = TRUE), .groups = 'drop') %&gt;%\n    complete(offender_type, variable_detailed = c('Male', 'Female', 'Unknown'), \n             fill = list(value = 0))\n  \n  # Race data - ensure no duplicates by summing values\n  race_data &lt;- state_data %&gt;%\n    filter(variable_category == 'race',\n           value_type == 'count') %&gt;%\n    group_by(offender_type, variable_detailed) %&gt;%\n    summarize(value = sum(value, na.rm = TRUE), .groups = 'drop') %&gt;%\n    complete(offender_type, \n             variable_detailed = c('White', 'Black', 'Hispanic', \n                                  'Asian', 'Native American', 'Other', 'Unknown'), \n             fill = list(value = 0))\n  \n  # Create separate plots - one per row\n  par(mfrow = c(2, 1), mar = c(5, 9, 4, 9), oma = c(0, 0, 2, 0)) # Increased right margin for legend\n  \n  # Gender plot - ordered by total volume\n  gender_plot_data &lt;- gender_data %&gt;%\n    filter(variable_detailed %in% c('Male', 'Female', 'Unknown')) %&gt;%\n    mutate(offender_type = factor(offender_type, levels = rev(offender_types)))\n  \n  # Order gender categories by total volume (largest at bottom)\n  gender_order &lt;- gender_plot_data %&gt;%\n    group_by(variable_detailed) %&gt;%\n    summarize(total = sum(value)) %&gt;%\n    arrange(total) %&gt;%\n    pull(variable_detailed)\n  \n  gender_plot_data &lt;- gender_plot_data %&gt;%\n    mutate(variable_detailed = factor(variable_detailed, levels = gender_order))\n  \n  # Reshape for barplot\n  gender_matrix &lt;- gender_plot_data %&gt;%\n    pivot_wider(names_from = variable_detailed, values_from = value) %&gt;%\n    as.data.frame() %&gt;%\n    column_to_rownames(\"offender_type\") %&gt;%\n    as.matrix()\n  \n  # Ensure all columns exist\n  for (gender in gender_order) {\n    if (!gender %in% colnames(gender_matrix)) {\n      gender_matrix &lt;- cbind(gender_matrix, temp = 0)\n      colnames(gender_matrix)[ncol(gender_matrix)] &lt;- gender\n    }\n  }\n  \n  # Reorder columns by volume\n  gender_matrix &lt;- gender_matrix[, as.character(gender_order), drop = FALSE]\n  \n  # Format x-axis labels with \"k\" for thousands\n  max_x &lt;- max(rowSums(gender_matrix))\n  x_breaks &lt;- pretty(c(0, max_x))\n  x_labels &lt;- ifelse(x_breaks &gt;= 1000, \n                    paste0(x_breaks/1000, \"k\"), \n                    as.character(x_breaks))\n  \n  barplot(t(gender_matrix), \n          horiz = TRUE,\n          las = 1,\n          col = gender_colors[colnames(gender_matrix)],\n          main = 'Gender Distribution',\n          xlab = 'Number of Profiles',\n          xaxt = 'n',  # Remove default x-axis\n          legend.text = FALSE,  # Don't show legend in plot area\n          args.legend = list(x = \"right\", bty = \"n\", inset = c(-0.2, 0)))\n  \n  # Add custom x-axis with formatted labels\n  axis(1, at = x_breaks, labels = x_labels)\n  \n  # Add legend outside the plot area\n  legend(\"topright\", \n         legend = colnames(gender_matrix), \n         fill = gender_colors[colnames(gender_matrix)],\n         bty = \"n\", \n         xpd = TRUE,  # Allow plotting outside main area\n         inset = c(-0.25, 0),  # Move legend to the right\n         cex = 0.8)\n  \n  # Race plot - ordered by total volume\n  race_plot_data &lt;- race_data %&gt;%\n    mutate(offender_type = factor(offender_type, levels = rev(offender_types)))\n  \n  # Order race categories by total volume (largest at bottom)\n  race_order &lt;- race_plot_data %&gt;%\n    group_by(variable_detailed) %&gt;%\n    summarize(total = sum(value)) %&gt;%\n    arrange(total) %&gt;%\n    pull(variable_detailed)\n  \n  race_plot_data &lt;- race_plot_data %&gt;%\n    mutate(variable_detailed = factor(variable_detailed, levels = race_order))\n  \n  # Reshape for barplot\n  race_matrix &lt;- race_plot_data %&gt;%\n    pivot_wider(names_from = variable_detailed, values_from = value) %&gt;%\n    as.data.frame() %&gt;%\n    column_to_rownames(\"offender_type\") %&gt;%\n    as.matrix()\n  \n  # Ensure all columns exist\n  for (race in race_order) {\n    if (!race %in% colnames(race_matrix)) {\n      race_matrix &lt;- cbind(race_matrix, temp = 0)\n      colnames(race_matrix)[ncol(race_matrix)] &lt;- race\n    }\n  }\n  \n  # Reorder columns by volume\n  race_matrix &lt;- race_matrix[, as.character(race_order), drop = FALSE]\n  \n  # Format x-axis labels with \"k\" for thousands\n  max_x_race &lt;- max(rowSums(race_matrix))\n  x_breaks_race &lt;- pretty(c(0, max_x_race))\n  x_labels_race &lt;- ifelse(x_breaks_race &gt;= 1000, \n                         paste0(x_breaks_race/1000, \"k\"), \n                         as.character(x_breaks_race))\n  \n  barplot(t(race_matrix), \n          horiz = TRUE,\n          las = 1,\n          col = race_colors[colnames(race_matrix)],\n          main = 'Race Distribution',\n          xlab = 'Number of Profiles',\n          xaxt = 'n',  # Remove default x-axis\n          legend.text = FALSE)  # Don't show legend in plot area\n  \n  # Add custom x-axis with formatted labels\n  axis(1, at = x_breaks_race, labels = x_labels_race)\n  \n  # Add legend outside the plot area\n  legend(\"topright\", \n         legend = colnames(race_matrix), \n         fill = race_colors[colnames(race_matrix)],\n         bty = \"n\", \n         xpd = TRUE,  # Allow plotting outside main area\n         inset = c(-0.25, 0),  # Move legend to the right\n         cex = 0.8)\n  \n  title(paste(state_name, \"Demographic Distribution\"), outer = TRUE, cex.main = 1.5)\n}\n\n# ------------------------------------------------------------------\n# 20. Add state's metadata\n# ------------------------------------------------------------------\n\nadd_state_metadata &lt;- function(state_name, state_df) {\n  \n  raw_data &lt;- state_df %&gt;% filter(value_source == \"reported\")\n  offender_types_reported &lt;- unique(raw_data$offender_type)\n  \n  has_unknown &lt;- any(raw_data$variable_detailed == \"Unknown\", na.rm = TRUE)\n  has_other &lt;- any(raw_data$variable_detailed == \"Other\", na.rm = TRUE)\n  has_crosstab &lt;- any(raw_data$variable_category == \"gender_race\", na.rm = TRUE)\n  \n  nonstandard_terms &lt;- any(\n    grepl(\"All|Offenders\", raw_data$offender_type, ignore.case = TRUE),\n    grepl(\"Caucasian|African American| American Indian\", raw_data$variable_detailed, ignore.case = TRUE),\n    grepl(\"flag\", raw_data$variable_detailed, ignore.case = TRUE))\n  \n  new_row &lt;- tibble(\n    state = state_name,\n    race_data_provided = report_status(raw_data, \"race\"),\n    gender_data_provided = report_status(raw_data, \"gender\"),\n    total_profiles_provided = report_status(\n      raw_data %&gt;% filter(variable_category == \"total\"), \"total\"\n    ),\n    convicted_offender_reported = \"Convicted Offender\" %in% offender_types_reported,\n    arrestee_reported = \"Arrestee\" %in% offender_types_reported,\n    combined_reported = \"Combined\" %in% offender_types_reported,\n    has_unknown_category = has_unknown,\n    has_other_category = has_other,\n    uses_nonstandard_terminology = nonstandard_terms,\n    provides_crosstabulation = has_crosstab,\n    counts_sum_to_total = NA,\n    percentages_sum_to_100 = NA,\n    total_calculated_combined = !(\"Combined\" %in% offender_types_reported),\n    notes = \"\"\n  )\n  \n  foia_state_metadata &lt;&lt;- bind_rows(foia_state_metadata, new_row)\n  \n  cat(\"âœ“ Metadata added for:\", state_name, \"\\n\")\n  return(invisible(TRUE))\n}\n\n# ------------------------------------------------------------------\n# 21. Function to update a state's metadata after QC checks\n# ------------------------------------------------------------------\nupdate_state_metadata &lt;- function(state_name, \n                                  counts_ok = NA, \n                                  percentages_ok = NA, \n                                  notes_text = NULL) {\n  \n  row_index &lt;- which(foia_state_metadata$state == state_name)\n  \n  if (length(row_index) == 0) {\n    warning(\"State not found in metadata: \", state_name)\n    return(FALSE)\n  }\n  \n  if (!is.na(counts_ok)) {\n    foia_state_metadata$counts_sum_to_total[row_index] &lt;&lt;- counts_ok\n  }\n  if (!is.na(percentages_ok)) {\n    foia_state_metadata$percentages_sum_to_100[row_index] &lt;&lt;- percentages_ok\n  }\n  if (!is.null(notes_text)) {\n    current_notes &lt;- foia_state_metadata$notes[row_index]\n    if (current_notes == \"\") {\n      foia_state_metadata$notes[row_index] &lt;&lt;- notes_text\n    } else {\n      foia_state_metadata$notes[row_index] &lt;&lt;- paste(current_notes, notes_text, sep = \"; \")\n    }\n  }\n  \n  cat(\"âœ“ Metadata updated for:\", state_name, \"\\n\")\n}"
  },
  {
    "objectID": "qmd_root/foia_processing.html#file-structure-and-contents",
    "href": "qmd_root/foia_processing.html#file-structure-and-contents",
    "title": "FOIA Document OCR Processing",
    "section": "2.4 File Structure and Contents",
    "text": "2.4 File Structure and Contents\n\n2.4.1 State-Specific Files: data/foia/intermediate/[state]_foia_data.csv\nPurpose: Individual files for each state containing only their reported data.\nStructure: Long format with columns:\n\nstate: State name\noffender_type: Category of individuals (Convicted Offender, Arrestee, Combined, etc.)\nvariable_category: Type of data (total, gender, race, gender_race)\nvariable_detailed: Specific value (e.g., Male, Female, African American)\nvalue: The reported number or percentage\nvalue_type: Whether value is a â€œcountâ€ or â€œpercentageâ€\ndate: Date of data snapshot, if reported\n\n\n\nShow per-state files loading code\nca_raw &lt;- load_state(here(per_state, \"california_foia_data.csv\"))\nfl_raw &lt;- load_state(here(per_state, \"florida_foia_data.csv\"))\nin_raw &lt;- load_state(here(per_state, \"indiana_foia_data.csv\"))\nme_raw &lt;- load_state(here(per_state, \"maine_foia_data.csv\"))\nnv_raw &lt;- load_state(here(per_state, \"nevada_foia_data.csv\"))\nsd_raw &lt;- load_state(here(per_state, \"south_dakota_foia_data.csv\"))\ntx_raw &lt;- load_state(here(per_state, \"texas_foia_data.csv\"))\n\n\n\n\n2.4.2 Raw Data Characteristics\nThe following table summarizes the structure and content of the data as originally received from each state prior to any standardization, calculation, or processing.\n\n\n\n\n\n\n\n\n\n\n\nState\nOffender Types\nValue Types\nTotal Profiles\nAction Needed\nKey Reporting Notes\n\n\n\n\nCalifornia\nCO, A\nCounts only\nReported per offender type\nAdd Unknown Race, Calculate % & Combined, Standardize Terminology\nDiscrepancy in Race: counts &lt; total profiles; Non-standard terminology (Caucasian and African American)\n\n\nFlorida\nCOMB\nCounts + %\nReported\nStandardize Terminology\nNon-standard terminology (Caucasian and African American)\n\n\nIndiana\nCO, A, COMB\nPercentage (Counts for totals only)\nReported per offender type\nCalculate Counts & Total Profiles Combined, Fix % inconsistency, Standardize Terminology\nDemographics only for Combined; Other race category as â€œ&lt;1â€; Non-standard terminology (Caucasian)\n\n\nMaine\nCOMB\nCounts + %\nReported\nSolve counts and Percentage inconsistency\n\n\n\nNevada\nCO, A, COMB\nCounts + %\nReported for all types\nStandardize Terminology\nNon-standard terminology (All, total_flags and American Indian)\n\n\nSouth Dakota\nCOMB\nCounts + %\nReported\nStandardize Terminology, Solve counts and % inconsistency\nIncludes genderÃ—race cross-tabulation; Non-standard terminology\n\n\nTexas\nCO, A\nCounts only\nReported per offender type\nCalculate Male counts, Solve counts inconsistency, Calculate % & Combined, Standardize Terminology\nOnly female gender was reported; Non-standard term (Offenders, Caucasian, and African American)\n\n\n\nLegend:\n\nCO: Convicted Offender\nAR: Arrestee\nCOMB: Combined Total (all profiles)\nCounts + %: Both raw numbers and percentages were provided"
  },
  {
    "objectID": "qmd_root/foia_processing.html#prepare-combined-dataset",
    "href": "qmd_root/foia_processing.html#prepare-combined-dataset",
    "title": "FOIA Document OCR Processing",
    "section": "2.5 Prepare Combined Dataset",
    "text": "2.5 Prepare Combined Dataset\nThe goal of this step is to transform each stateâ€™s raw data into a standardized format before appending it to the master foia_combined DataFrame. This ensures consistency and enables seamless analysis across all seven states.\nThe ideal, standardized state dataset ready for combination must have the following columns:\n\n\n\n\n\n\n\n\nColumn Name\nDescription\nExample Values\n\n\n\n\nstate\nThe name of the state.\n\"California\", \"Florida\"\n\n\noffender_type\nThe category of offender profile.\n\"Convicted Offender\", \"Arrestee\", \"Combined\"\n\n\nvariable_category\nThe broad demographic category.\n\"race\", \"gender\", \"total\", \"gender_race\"\n\n\nvariable_detailed\nThe specific value within the category.\n\"White\", \"Male\", \"total_profiles\", \"Male_White\"\n\n\nvalue\nThe numerical value for the metric.\n150000, 25.8\n\n\nvalue_type\nThe type of metric the value represents.\n\"count\", \"percentage\"\n\n\nvalue_source\nWhether the data was provided or derived.\n\"reported\", \"calculated\"\n\n\n\n\n\nShow the master foia_combined dataframe elaboration code\n# ------------------------------------------------------------------\n# Initialize the master foia_combined dataframe with correct schema\n# This empty structure ensures all state data is appended consistently\n# ------------------------------------------------------------------\n\nfoia_combined &lt;- tibble( state = character(),\noffender_type = character(),\nvariable_category = character(),\nvariable_detailed = character(), \nvalue = numeric(),\nvalue_type = character(),\nvalue_source = character()\n)\n\n# Create a data dictionary for foia_combined\nschema_dict &lt;- tribble(\n  ~Column,             ~Type,        ~Description, \n  \"state\",             \"character\",  \"'California', 'Florida'\",\n  \"offender_type\",     \"character\",  \"'Convicted Offender', 'Arrestee', 'Combined'\",\n  \"variable_category\", \"character\",  \"'race', 'gender', 'total', 'gender_race'\",\n  \"variable_detailed\", \"character\",  \"'White', 'Male', 'total_profiles', 'Male_White'\",\n  \"value\",             \"numeric\",    \"150000, 25.8\",\n  \"value_type\",        \"character\",  \"'count', 'percentage'\",\n  \"value_source\",      \"character\",  \"'reported', 'calculated'\"\n)\n\n# Turn into a nice flextable\nflextable(schema_dict) %&gt;%\n  autofit() %&gt;%\n  theme_booktabs() %&gt;%\n  set_header_labels(\n    Column = \"Column Name\",\n    Type = \"Data Type\",\n    Description = \"Example Values to be added\"\n  )\n\n\nColumn NameData TypeExample Values to be addedstatecharacter'California', 'Florida'offender_typecharacter'Convicted Offender', 'Arrestee', 'Combined'variable_categorycharacter'race', 'gender', 'total', 'gender_race'variable_detailedcharacter'White', 'Male', 'total_profiles', 'Male_White'valuenumeric150000, 25.8value_typecharacter'count', 'percentage'value_sourcecharacter'reported', 'calculated'"
  },
  {
    "objectID": "qmd_root/foia_processing.html#prepare-metadata-documentation-table",
    "href": "qmd_root/foia_processing.html#prepare-metadata-documentation-table",
    "title": "FOIA Document OCR Processing",
    "section": "2.6 Prepare Metadata Documentation Table",
    "text": "2.6 Prepare Metadata Documentation Table\nThis section creates a comprehensive metadata table (foia_state_metadata) to document the original content and structure of each stateâ€™s FOIA response before any processing or cleaning was applied.\nThis serves as a permanent record of data provenance, ensuring transparency and reproducibility by clearly distinguishing between what was provided by the states and what was calculated during analysis.\nKey Documentation Captured:\n\nData Types Provided: Whether each state reported counts, percentages, or both for race, gender, and total profiles.\nOffender Categories Reported: Which offender types (Convicted Offender, Arrestee, Combined) were originally included.\nDemographic Granularity: Presence of â€˜Unknownâ€™ or â€˜Otherâ€™ categories and gender-race cross-tabulations.\nTerminology & Anomalies: Use of non-standard terms (e.g., â€œflags,â€ â€œoffendersâ€) and other state-specific reporting notes.\nQC Results: Flags for whether cleaned data passes consistency checks (counts sum to totals, percentages sum to ~100%).\n\n\n\nShow the foia_state_metadata table elaboration code\n# ------------------------------------------------------------------\n# Initialize the foia_state_metadata as a tibble (not a list of lists)\n# This makes it easier to add rows and ensures consistent structure.\n# ------------------------------------------------------------------\n\n# Define the full schema for our metadata table\nfoia_state_metadata &lt;- tibble(\n  state = character(),\n  race_data_provided = character(),\n  gender_data_provided = character(),\n  total_profiles_provided = character(), \n  convicted_offender_reported = logical(),\n  arrestee_reported = logical(),\n  combined_reported = logical(),\n  has_unknown_category = logical(),\n  has_other_category = logical(),\n  uses_nonstandard_terminology = logical(),\n  provides_crosstabulation = logical(),\n  counts_sum_to_total = logical(),\n  percentages_sum_to_100 = logical(),\n  total_calculated_combined = logical(),\n  notes = character()\n)\n\n# Build data dictionary for foia_state_metadata\nschema_dict_meta &lt;- tribble(\n  ~Column,                        ~Type,       ~Description,\n  \"state\",                        \"character\", \"State name (e.g., 'California', 'Florida')\",\n  \"race_data_provided\",           \"character\", \"Race data availability: 'counts', 'percentages', 'both', 'none'\",\n  \"gender_data_provided\",         \"character\", \"Gender data availability: 'counts', 'percentages', 'both', 'none'\",\n  \"total_profiles_provided\",      \"character\", \"Total profiles availability: 'counts', 'percentages', 'both', 'none'\",\n  \"convicted_offender_reported\",  \"logical\",   \"Was convicted offender data reported?\",\n  \"arrestee_reported\",            \"logical\",   \"Was arrestee data reported?\",\n  \"combined_reported\",            \"logical\",   \"Was combined category reported?\",\n  \"has_unknown_category\",         \"logical\",   \"Does the state include 'Unknown' category?\",\n  \"has_other_category\",           \"logical\",   \"Does the state include 'Other' category?\",\n  \"uses_nonstandard_terminology\", \"logical\",   \"Does the state use non-standard terms?\",\n  \"provides_crosstabulation\",     \"logical\",   \"Does the state provide crosstabs (e.g., gender x race)?\",\n  \"counts_sum_to_total\",          \"logical\",   \"Do reported counts sum to the total?\",\n  \"percentages_sum_to_100\",       \"logical\",   \"Do reported percentages sum to ~100%?\",\n  \"total_calculated_combined\",    \"logical\",   \"Did we calculate combined total manually?\",\n  \"notes\",                        \"character\", \"Free-text notes for state-specific caveats\"\n)\n\n# Render with flextable\nflextable(schema_dict_meta) %&gt;%\n  autofit() %&gt;%\n  theme_booktabs() %&gt;%\n  set_header_labels(\n    Column = \"Column Name\",\n    Type = \"Data Type\",\n    Description = \"Meaning\"\n  )\n\n\nColumn NameData TypeMeaningstatecharacterState name (e.g., 'California', 'Florida')race_data_providedcharacterRace data availability: 'counts', 'percentages', 'both', 'none'gender_data_providedcharacterGender data availability: 'counts', 'percentages', 'both', 'none'total_profiles_providedcharacterTotal profiles availability: 'counts', 'percentages', 'both', 'none'convicted_offender_reportedlogicalWas convicted offender data reported?arrestee_reportedlogicalWas arrestee data reported?combined_reportedlogicalWas combined category reported?has_unknown_categorylogicalDoes the state include 'Unknown' category?has_other_categorylogicalDoes the state include 'Other' category?uses_nonstandard_terminologylogicalDoes the state use non-standard terms?provides_crosstabulationlogicalDoes the state provide crosstabs (e.g., gender x race)?counts_sum_to_totallogicalDo reported counts sum to the total?percentages_sum_to_100logicalDo reported percentages sum to ~100%?total_calculated_combinedlogicalDid we calculate combined total manually?notescharacterFree-text notes for state-specific caveats"
  },
  {
    "objectID": "qmd_root/foia_processing.html#california-ca",
    "href": "qmd_root/foia_processing.html#california-ca",
    "title": "FOIA Document OCR Processing",
    "section": "3.1 California (CA)",
    "text": "3.1 California (CA)\nOverview: California supplies counts only for gender and race plus a separate total for each offender type; no percentages are reported.\n\n3.1.1 Examine Raw Data\nEstablish a baseline understanding of the data exactly as it was received.\n\n\nColumnTypeRowsMissingUniqueUnique_Valuesstatecharacter1601Californiaoffender_typecharacter1602Convicted Offender, Arresteevariable_categorycharacter1603total, gender, racevariable_detailedcharacter1608total_profiles, Female, Male, Unknown, African American, Caucasian, Hispanic, Asianvaluenumeric160162019899 ..., 751822 ..., 309827 ..., 1603222 ..., 106850 ..., 208225 ..., 524231 ..., 19366 ..., 368952 ..., 588555 ...value_typecharacter1601countvalue_sourcecharacter1601reportedData frame dimensions: 16 rows Ã— 7 columns\n\n\n\n\n3.1.2 Verify Data Consistency\nRuns the first quality check using the verify_category_totals() and counts_consistent() functions.\nThis identifies any immediate discrepancies, such as the sum of demographic counts not matching the reported total profiles, which flags data issues that need to be resolved.\n\n\nVerifying that demographic counts match reported totals:\n\n\n\n\n\noffender_type\nvariable_category\ntotal_profiles\nsum_counts\ndifference\n\n\n\n\nArrestee\ngender\n751822\n751822\n0\n\n\nArrestee\nrace\n751822\n655695\n96127\n\n\nConvicted Offender\ngender\n2019899\n2019899\n0\n\n\nConvicted Offender\nrace\n2019899\n1626012\n393887\n\n\n\n\n\n\nCounts consistency check on raw data:\n\n\nAll counts consistent: FALSE \n\n\n\n\n3.1.3 Address Data Gaps\n\n3.1.3.1 Create Unknown Category\n\nâ€œRacial classification is not considered a required field on the collection card; thus, an unknown number of offenders may have no racial classification listed.â€ â€” California DOJ FOIA letter, July 10 2018 (raw/foia_pdfs/FOIA_RacialComp_California.pdf)\n\nThe 393,887 Convicted Offender profiles and 96,127 Arrestee profiles that do not appear in any of the four reported race categories must belong to an unreported â€œUnknownâ€ category.\nThe calculated values are added with a value_source = \"calculated\" tag to maintain transparency about what was provided versus what was derived.\n\n\nShow unknown addition code\n# Start with the raw data\nca_clean &lt;- ca_raw\n\n# Add Unknown race category to reconcile totals\nca_clean &lt;- fill_demographic_gaps(ca_clean)\n\n# Verify the fix\ncat(\"Category totals after adding Unknown race category:\\n\")\nverify_category_totals(ca_clean) %&gt;% kable() %&gt;% kable_styling()\n\ncat(\"\\nCounts consistency after adding Unknown:\\n\")\ncat(paste(\"All counts consistent:\", counts_consistent(ca_clean), \"\\n\"))\n\n\nCategory totals after adding Unknown race category:\n\n\n\n\n\noffender_type\nvariable_category\ntotal_profiles\nsum_counts\ndifference\n\n\n\n\nArrestee\ngender\n751822\n751822\n0\n\n\nArrestee\nrace\n751822\n751822\n0\n\n\nConvicted Offender\ngender\n2019899\n2019899\n0\n\n\nConvicted Offender\nrace\n2019899\n2019899\n0\n\n\n\n\n\n\nCounts consistency after adding Unknown:\nAll counts consistent: TRUE \n\n\n\n\n3.1.3.2 Create Combined Totals\nSince California only reported data for â€œConvicted Offenderâ€ and â€œArresteeâ€ separately.\nThis step uses the add_combined() helper function to calculate a new â€œCombinedâ€ offender type by summing the counts from the other two categories.\n\n\nShow combined addition code\n# Calculate Combined totals using helper function\nca_clean &lt;- add_combined(ca_clean)\n\ncat(\"âœ“ Created Combined totals for California\\n\")\n\n# Show the Combined total\ncombined_total &lt;- ca_clean %&gt;%\n  filter(offender_type == \"Combined\",\n         variable_category == \"total\",\n         variable_detailed == \"total_profiles\") %&gt;%\n  pull(value)\n\ncat(paste(\"Combined total profiles:\", format(combined_total, big.mark = \",\"), \"\\n\"))\n\n\nâœ“ Created Combined totals for California\nCombined total profiles: 2,771,721 \n\n\n\n\n3.1.3.3 Calculate Percentages\nTransforms the data from counts into percentages for comparative analysis.\nThe add_percentages() helper function calculates each demographic groupâ€™s proportion relative to its offender typeâ€™s total.\nA final consistency check ensures all percentages logically sum to approximately 100%.\n\n\nShow percentage calculation code\n# Derive percentages from counts\nca_clean &lt;- add_percentages(ca_clean)\n\ncat(\"âœ“ Added percentages for all demographic categories\\n\")\n\n# Check percentage consistency\ncat(\"Percentage consistency check:\\n\")\ncat(paste(\"All percentages sum to ~100%:\", percentages_consistent(ca_clean), \"\\n\\n\"))\n\n# Show current data availability\ncat(\"Final data availability:\\n\")\ncat(paste(\"Race data:\", report_status(ca_clean, \"race\"), \"\\n\"))\ncat(paste(\"Gender data:\", report_status(ca_clean, \"gender\"), \"\\n\"))\n\n\nâœ“ Added percentages for all demographic categories\nPercentage consistency check:\nAll percentages sum to ~100%: TRUE \n\nFinal data availability:\nRace data: both \nGender data: both \n\n\n\n\n3.1.3.4 Standardize Terminology\nCalifornia uses â€œAfrican Americanâ€ instead of â€œBlackâ€ and â€œCaucasianâ€ instead of â€œWhiteâ€.\n\n\nShow terminology standardization code\n# Standardize racial terminology\nca_clean &lt;- ca_clean %&gt;%\n  mutate(variable_detailed = case_when(\n    variable_detailed == \"African American\" ~ \"Black\",\n    TRUE ~ variable_detailed\n  ))\n\ncat(\"âœ“ Standardized terminology: 'African American' â†’ 'Black'\\n\")\n\nca_clean &lt;- ca_clean %&gt;%\n  mutate(variable_detailed = case_when(\n    variable_detailed == \"Caucasian\" ~ \"White\",\n    TRUE ~ variable_detailed\n  ))\n\ncat(\"âœ“ Standardized terminology: 'Caucasian' â†’ 'White'\\n\")\n\n\nâœ“ Standardized terminology: 'African American' â†’ 'Black'\nâœ“ Standardized terminology: 'Caucasian' â†’ 'White'\n\n\n\n\n\n3.1.4 Prepare for Combined Dataset\nThe cleaned data is formatted to match the master schema and appended to the foia_combined dataframe.\n\n\nShow California data preparation to combined dataset\n# Prepare the cleaned data for the combined dataset\nca_prepared &lt;- prepare_state_for_combined(ca_clean, \"California\")\n\n# Append to the master combined dataframe\nfoia_combined &lt;- bind_rows(foia_combined, ca_prepared)\n\ncat(paste0(\"âœ“ Appended \", nrow(ca_prepared), \" California rows to foia_combined\\n\"))\ncat(paste0(\"âœ“ Total rows in foia_combined: \", nrow(foia_combined), \"\\n\"))\n\n\nâœ“ Appended 51 California rows to foia_combined\nâœ“ Total rows in foia_combined: 51\n\n\n\n\n3.1.5 Document Metadata\nThe metadata is added with the raw information and updated with the results of the quality checks and a note on the processing steps taken.\n\n\nShow California data preparation and addition to metadata table\n# Add California to the metadata table using the helper function\nadd_state_metadata(\"California\", ca_raw)\n\n# Update metadata with QC results\nupdate_state_metadata(\"California\", \n                      counts_ok = counts_consistent(ca_clean),\n                      percentages_ok = percentages_consistent(ca_clean),\n                      notes_text = \"Added Unknown race category to reconcile totals; calculated Combined totals and all percentages\")\n\n\nâœ“ Metadata added for: California \nâœ“ Metadata updated for: California \n\n\n\n\n3.1.6 Visualizations\n\n\n\n\n\nCalifornia DNA Database Demographic Distributions\n\n\n\n\n\n\n\nCalifornia DNA Database Demographic Distributions\n\n\n\n\n\n\n\nCalifornia DNA Database Demographic Distributions\n\n\n\n\n\n\n\nCalifornia DNA Database Demographic Distributions\n\n\n\n\n\n\n\nCalifornia DNA Database Demographic Distributions\n\n\n\n\n\n\n\nCalifornia DNA Database Demographic Distributions\n\n\n\n\n\n\n\nCalifornia DNA Database Demographic Distributions\n\n\n\n\n\n\n\nCalifornia DNA Database Demographic Distributions\n\n\n\n\n\n\n\nCalifornia DNA Database Demographic Distributions\n\n\n\n\n\n\n\nCalifornia DNA Database Demographic Distributions\n\n\n\n\n\n\n\nCalifornia DNA Database Demographic Distributions\n\n\n\n\n\n\n\nCalifornia DNA Database Demographic Distributions\n\n\n\n\n\n\n\n\n\nCalifornia Demographic Distributions by Offender Type\n\n\n\n\n\n\n3.1.7 Summary Statistics\n\n\nShow the summary statistics code\ncat(\"California DNA Database Summary:\\n\")\ncat(\"=\", strrep(\"=\", 40), \"\\n\")\n\n# Total profiles by offender type\ntotals &lt;- foia_combined %&gt;%\n  filter(state == \"California\",\n         variable_category == \"total\",\n         variable_detailed == \"total_profiles\",\n         value_type == \"count\") %&gt;%\n  select(offender_type, value) %&gt;%\n  mutate(value_formatted = format(value, big.mark = \",\"))\n\nprint(totals)\n\n# Data completeness\ncat(\"\\nData completeness:\\n\")\ncompleteness &lt;- foia_combined %&gt;%\n  filter(state == \"California\") %&gt;%\n  group_by(offender_type, value_source) %&gt;%\n  summarise(n_values = n(), .groups = \"drop\")\n\nprint(completeness)\n\n# Final verification\ncat(\"\\nFinal verification:\\n\")\ncat(paste(\"Counts consistent:\", counts_consistent(foia_combined %&gt;% filter(state == \"California\")), \"\\n\"))\ncat(paste(\"Percentages consistent:\", percentages_consistent(foia_combined %&gt;% filter(state == \"California\")), \"\\n\"))\n\n\nCalifornia DNA Database Summary:\n= ======================================== \n# A tibble: 3 Ã— 3\n  offender_type        value value_formatted\n  &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;          \n1 Convicted Offender 2019899 \"2,019,899\"    \n2 Arrestee            751822 \"  751,822\"    \n3 Combined           2771721 \"2,771,721\"    \n\nData completeness:\n# A tibble: 5 Ã— 3\n  offender_type      value_source n_values\n  &lt;chr&gt;              &lt;chr&gt;           &lt;int&gt;\n1 Arrestee           calculated          9\n2 Arrestee           reported            8\n3 Combined           calculated         17\n4 Convicted Offender calculated          9\n5 Convicted Offender reported            8\n\nFinal verification:\nCounts consistent: TRUE \nPercentages consistent: TRUE \n\n\n\n\n3.1.8 Summary of California Processing\nCalifornia data processing complete. The dataset now includes:\n\nâœ… Reported data: Counts for Convicted Offender and Arrestee\nâœ… Calculated additions:\n\nUnknown race category to reconcile reported totals\nCombined totals across all offender types\nPercentage values for all demographic categories\nâ€œCaucasianâ€ and â€œAfrican Americanâ€ converted to â€œWhiteâ€ and â€œBlackâ€.\n\nâœ… Quality checks: All counts and percentages pass consistency validation\nâœ… Provenance tracking: All values include appropriate value_source indicators\n\nThe California data is now standardized and ready for cross-state analysis."
  },
  {
    "objectID": "qmd_root/foia_processing.html#florida-fl",
    "href": "qmd_root/foia_processing.html#florida-fl",
    "title": "FOIA Document OCR Processing",
    "section": "3.2 Florida (FL)",
    "text": "3.2 Florida (FL)\nOverview: Florida provides both counts and percentages for gender and race categories and already includes a â€œCombinedâ€ total for all offender types, making it one of the most complete and straightforward datasets.\nOnly requires to standardize terminology for gender and race categories to match the common data model.\n\n3.2.1 Examine Raw Data\nEstablish a baseline understanding of the data exactly as it was received.\n\n\nColumnTypeRowsMissingUniqueUnique_Valuesstatecharacter2201Floridaoffender_typecharacter2201Combinedvariable_categorycharacter2203total, gender, racevariable_detailedcharacter22010total_profiles, Female, Male, Unknown, African American, Asian, Caucasian, Hispanic, Native American, Othervaluenumeric220221175391 ..., 100 ..., 260885 ..., 22.2 ..., 901126 ..., 76.67 ..., 13380 ..., 1.14 ..., 413733 ..., 35.2 ...value_typecharacter2202count, percentagevalue_sourcecharacter2201reportedData frame dimensions: 22 rows Ã— 7 columns\n\n\n\n\n3.2.2 Verify Data Consistency\nRuns the first quality check using the Verify_category_totals() and counts_consistent() functions.\n\n\nVerifying that demographic counts match reported totals:\n\n\n\n\n\noffender_type\nvariable_category\ntotal_profiles\nsum_counts\ndifference\n\n\n\n\nCombined\ngender\n1175391\n1175391\n0\n\n\nCombined\nrace\n1175391\n1175391\n0\n\n\n\n\n\n\nCounts consistency check on raw data:\n\n\nAll counts consistent: TRUE \n\n\n\nPercentage consistency check on raw data:\n\n\nAll percentages sum to ~100%: TRUE \n\n\n\n\n3.2.3 Address Data Gaps\n\n3.2.3.1 Standardize Terminology\nFlorida uses â€œAfrican Americanâ€ instead of â€œBlackâ€ and â€œCaucasianâ€ instead of â€œWhiteâ€.\n\n\nShow terminology standardization code\nfl_clean &lt;- fl_raw\n\n# Standardize racial terminology\nfl_clean &lt;- fl_clean %&gt;%\n  mutate(variable_detailed = case_when(\n    variable_detailed == \"African American\" ~ \"Black\",\n    TRUE ~ variable_detailed\n  ))\n\ncat(\"âœ“ Standardized terminology: 'African American' â†’ 'Black'\\n\")\n\nfl_clean &lt;- fl_clean %&gt;%\n  mutate(variable_detailed = case_when(\n    variable_detailed == \"Caucasian\" ~ \"White\",\n    TRUE ~ variable_detailed\n  ))\n\ncat(\"âœ“ Standardized terminology: 'Caucasian' â†’ 'White'\\n\")\n\n\nâœ“ Standardized terminology: 'African American' â†’ 'Black'\nâœ“ Standardized terminology: 'Caucasian' â†’ 'White'\n\n\n\n\n\n3.2.4 Prepare for Combined Dataset\nThe Florida data is already complete and consistent. It is formatted to match the master schema and appended to the foia_combined dataframe.\n\n\nShow Florida data preparation to combined dataset\n# Prepare the data for the combined dataset\nfl_prepared &lt;- prepare_state_for_combined(fl_clean, \"Florida\")\n\n# Append to the master combined dataframe\nfoia_combined &lt;- bind_rows(foia_combined, fl_prepared)\n\ncat(paste0(\"âœ“ Appended \", nrow(fl_prepared), \" Florida rows to foia_combined\\n\"))\ncat(paste0(\"âœ“ Total rows in foia_combined: \", nrow(foia_combined), \"\\n\"))\n\n\nâœ“ Appended 22 Florida rows to foia_combined\nâœ“ Total rows in foia_combined: 73\n\n\n\n\n3.2.5 Document Metadata\nThe metadata is added with a note that the data was complete and required no processing.\n\n\nShow Florida data preparation and addition to metadata table\n# Add Florida to the metadata table using the helper function\nadd_state_metadata(\"Florida\", fl_raw)\n\n# Update metadata with QC results\nupdate_state_metadata(\"Florida\", \n                      counts_ok = counts_consistent(fl_clean),\n                      percentages_ok = percentages_consistent(fl_clean),\n                      notes_text = \"Complete dataset provided. No processing or calculations required. All values are reported.\")\n\n\nâœ“ Metadata added for: Florida \nâœ“ Metadata updated for: Florida \n\n\n\n\n3.2.6 Visualizations\n\n\n\n\n\nFlorida DNA Database Demographic Distributions\n\n\n\n\n\n\n\nFlorida DNA Database Demographic Distributions\n\n\n\n\n\n\n\nFlorida DNA Database Demographic Distributions\n\n\n\n\n\n\n\nFlorida DNA Database Demographic Distributions\n\n\n\n\n\n\n3.2.7 Summary Statistics\n\n\nShow the summary statistics code\ncat(\"Florida DNA Database Summary:\\n\")\ncat(\"=\", strrep(\"=\", 40), \"\\n\")\n\n# Total profiles by offender type\ntotals &lt;- foia_combined %&gt;%\n  filter(state == \"Florida\",\n         variable_category == \"total\",\n         variable_detailed == \"total_profiles\",\n         value_type == \"count\") %&gt;%\n  select(offender_type, value) %&gt;%\n  mutate(value_formatted = format(value, big.mark = \",\"))\n\nprint(totals)\n\n# Data completeness\ncat(\"\\nData completeness:\\n\")\ncompleteness &lt;- foia_combined %&gt;%\n  filter(state == \"Florida\") %&gt;%\n  group_by(offender_type, value_source) %&gt;%\n  summarise(n_values = n(), .groups = \"drop\")\n\nprint(completeness)\n\n# Final verification\ncat(\"\\nFinal verification:\\n\")\ncat(paste(\"Counts consistent:\", counts_consistent(foia_combined %&gt;% filter(state == \"Florida\")), \"\\n\"))\ncat(paste(\"Percentages consistent:\", percentages_consistent(foia_combined %&gt;% filter(state == \"Florida\")), \"\\n\"))\n\n\nFlorida DNA Database Summary:\n= ======================================== \n# A tibble: 1 Ã— 3\n  offender_type   value value_formatted\n  &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;          \n1 Combined      1175391 1,175,391      \n\nData completeness:\n# A tibble: 1 Ã— 3\n  offender_type value_source n_values\n  &lt;chr&gt;         &lt;chr&gt;           &lt;int&gt;\n1 Combined      reported           22\n\nFinal verification:\nCounts consistent: TRUE \nPercentages consistent: TRUE \n\n\n\n\n3.2.8 Summary of Florida Processing\nFlorida data processing complete. The dataset is exemplary and required no adjustments:\n\nâœ… Reported data: Both counts and percentages for all Convicted Offender, Arrestee, and Combined categories.\nâœ… Terminology standardization: â€œCaucasianâ€ and â€œAfrican Americanâ€ converted to â€œWhiteâ€ and â€œBlackâ€.\nâœ… No calculated additions needed: All values are sourced directly from the state report (value_source = \"reported\").\nâœ… Quality checks: All counts and percentages pass consistency validation.\nâœ… Provenance tracking: All values maintain their original value_source as â€œreportedâ€.\n\nThe Florida data is now standardized and ready for cross-state analysis."
  },
  {
    "objectID": "qmd_root/foia_processing.html#indiana-in",
    "href": "qmd_root/foia_processing.html#indiana-in",
    "title": "FOIA Document OCR Processing",
    "section": "3.3 Indiana (IN)",
    "text": "3.3 Indiana (IN)\nOverview: Indiana presents a unique reporting pattern where total counts are provided by offender type, but demographic breakdowns are given only as percentages for the Combined total.\nValues were provided as strings, including a â€œ&lt;1â€ notation, requiring conversion.\n\n3.3.1 Examine Raw Data\nEstablish a baseline understanding of the data exactly as it was received.\n\n\nColumnTypeRowsMissingUniqueUnique_Valuesstatecharacter801Indianaoffender_typecharacter803Convicted Offender, Arrestee, Combinedvariable_categorycharacter803total, gender, racevariable_detailedcharacter807total_profiles, Female, Male, Caucasian, Black, Hispanic, Othervaluenumeric808279654, 21087, 20, 80, 70, 26, 4, 0.5value_typecharacter802count, percentagevalue_sourcecharacter801reportedData frame dimensions: 8 rows Ã— 7 columns\n\n\n\n\n3.3.2 Verify Data Consistency\nInitial checks reveal Indianaâ€™s unique structure: counts for totals, percentages only for Combined demographics.\n\n\nInitial data availability:\n\n\nRace data: percentages \n\n\nGender data: percentages \n\n\n\nValue types in raw data:\n\n\ncount, percentage\n\n\n\n\n3.3.3 Address Data Gaps\n\n3.3.3.1 Convert String Values to Numeric\nThe raw data contains string values including â€œ&lt;1â€ which we convert to 0.5.\n\n\nShow value conversion code\n# Start with raw data\nin_clean &lt;- in_raw\n\n# Convert string values to numeric, handling \"&lt;1\" as 1\nin_clean$value &lt;- sapply(in_clean$value, function(x) {\n  if (x == \"&lt;1\") {\n    0.5\n  } else {\n    as.numeric(x)\n  }\n})\n\n# Update value_type for converted percentages\nin_clean &lt;- in_clean %&gt;%\n  mutate(value_type = ifelse(value_type == \"percentage\", \"percentage\", value_type))\n\ncat(\"âœ“ Converted Indiana values from String to numeric\\n\")\ncat(paste(\"Unique values after conversion:\", paste(unique(in_clean$value), collapse = \", \"), \"\\n\"))\n\n\nâœ“ Converted Indiana values from String to numeric\nUnique values after conversion: 279654, 21087, 20, 80, 70, 26, 4, 0.5 \n\n\n\n\n3.3.3.2 Solve Percentages Inconsistency\nRacial percentages summed to 100.5% instead of 100%\nProportional scaling was applied and value_source was updated to â€œcalculatedâ€ for all adjusted values.\n\n\nShow percentage recalculation code\n# Adjust percentages to ensure they sum to 100% and mark as calculated\nin_clean &lt;- in_clean %&gt;%\n  group_by(value_type, variable_category) %&gt;%\n  mutate(\n    value = ifelse(\n      value_type == \"percentage\" & variable_category == \"race\",\n      value * (100 / sum(value, na.rm = TRUE)),\n      value\n    ),\n    value_source = ifelse(\n      value_type == \"percentage\" & variable_category == \"race\",\n      \"calculated\",\n      value_source\n    )\n  ) %&gt;%\n  ungroup()\n\n# Verify the new sum\npercentage_sum &lt;- in_clean %&gt;%\n  filter(value_type == \"percentage\" & variable_category == \"race\") %&gt;%\n  summarise(total = sum(value, na.rm = TRUE))\n\ncat(\"âœ“ Recalculated percentages for Indiana - New sum:\", percentage_sum$total, \"%\\n\")\n\n\nâœ“ Recalculated percentages for Indiana - New sum: 100 %\n\n\n\n\n3.3.3.3 Standardize Terminology\nIndiana uses â€œCaucasianâ€ instead of â€œWhiteâ€.\n\n\nShow terminology standardization code\n# Standardize racial terminology\nin_clean &lt;- in_clean %&gt;%\n  mutate(variable_detailed = case_when(\n    variable_detailed == \"Caucasian\" ~ \"White\",\n    TRUE ~ variable_detailed\n  ))\n\ncat(\"âœ“ Standardized terminology: 'Caucasian' â†’ 'White'\\n\")\n\n\nâœ“ Standardized terminology: 'Caucasian' â†’ 'White'\n\n\n\n\n3.3.3.4 Create Combined Total Profiles\nIndiana provides separate totals for Convicted Offenders and Arrestees, but we need a Combined total to match the demographic percentages.\n\n\nShow combined total calculation code\n# Calculate Combined total from separate offender type totals\nconvicted_total &lt;- in_clean %&gt;%\n  filter(offender_type == \"Convicted Offender\",\n         variable_category == \"total\",\n         variable_detailed == \"total_profiles\") %&gt;%\n  pull(value)\n\narrestee_total &lt;- in_clean %&gt;%\n  filter(offender_type == \"Arrestee\",\n         variable_category == \"total\",\n         variable_detailed == \"total_profiles\") %&gt;%\n  pull(value)\n\ncombined_total &lt;- convicted_total + arrestee_total\n\n# Add Combined total to the data\ncombined_row &lt;- data.frame(\n  state = \"Indiana\",\n  offender_type = \"Combined\",\n  variable_category = \"total\",\n  variable_detailed = \"total_profiles\",\n  value = combined_total,\n  value_type = \"count\",\n  value_source = \"calculated\"\n)\n\nin_clean &lt;- bind_rows(in_clean, combined_row)\n\ncat(paste(\"Combined total profiles:\", format(combined_total, big.mark = \",\"), \"\\n\"))\ncat(\"âœ“ Added Combined total profiles\\n\")\n\n\nCombined total profiles: 300,741 \nâœ“ Added Combined total profiles\n\n\n\n\n3.3.3.5 Calculate Counts from Percentages\nIndiana only provides percentages for demographic categories. We calculate the actual counts using the Combined total.\n\n\nShow count calculation code\n# Calculate counts from percentages for Combined offender type\nin_clean &lt;- bind_rows(in_clean, calculate_counts_from_percentages(in_clean, \"Indiana\"))\n\ncat(\"âœ“ Calculated demographic counts from percentages\\n\")\n\n# Verify the calculations\ncat(\"Category totals after calculating counts:\\n\")\nverify_category_totals(in_clean) %&gt;% kable() %&gt;% kable_styling()\n\n\nâœ“ Calculated demographic counts from percentages\nCategory totals after calculating counts:\n\n\n\n\n\noffender_type\nvariable_category\ntotal_profiles\nsum_counts\ndifference\n\n\n\n\nCombined\ngender\n300741\n300741\n0\n\n\nCombined\nrace\n300741\n300741\n0\n\n\n\n\n\n\n\n\n3.3.4 Verify Data Consistency\nFinal checks to ensure all data is now consistent and complete.\n\n\nFinal data consistency checks:\n\n\nCounts consistent: TRUE \n\n\nPercentages consistent: TRUE \n\n\n\nFinal data availability:\n\n\nRace data: both \n\n\nGender data: both \n\n\n\n\n3.3.5 Prepare for Combined Dataset\nThe cleaned data is formatted to match the master schema and appended to the foia_combined dataframe.\n\n\nShow Indiana data preparation to combined dataset\n# Prepare the cleaned data for the combined dataset\nin_prepared &lt;- prepare_state_for_combined(in_clean, \"Indiana\")\n\n# Append to the master combined dataframe\nfoia_combined &lt;- bind_rows(foia_combined, in_prepared)\n\ncat(paste0(\"âœ“ Appended \", nrow(in_prepared), \" Indiana rows to foia_combined\\n\"))\ncat(paste0(\"âœ“ Total rows in foia_combined: \", nrow(foia_combined), \"\\n\"))\n\n\nâœ“ Appended 15 Indiana rows to foia_combined\nâœ“ Total rows in foia_combined: 88\n\n\n\n\n3.3.6 Document Metadata\nThe metadata is added with details on all processing steps performed.\n\n\nShow Indiana data preparation and addition to metadata table\n# Add Indiana to the metadata table using the helper function\nadd_state_metadata(\"Indiana\", in_raw)\n\n# Update metadata with QC results and processing notes\nupdate_state_metadata(\"Indiana\", \n                      counts_ok = counts_consistent(in_clean),\n                      percentages_ok = percentages_consistent(in_clean),\n                      notes_text = \"Converted string values to numeric; standardized 'Black' to 'African American'; calculated Combined total profiles; derived all demographic counts from reported percentages\")\n\n\nâœ“ Metadata added for: Indiana \nâœ“ Metadata updated for: Indiana \n\n\n\n\n3.3.7 Visualizations\n\n\n\n\n\nIndiana DNA Database Demographic Distributions\n\n\n\n\n\n\n\nIndiana DNA Database Demographic Distributions\n\n\n\n\n\n\n\nIndiana DNA Database Demographic Distributions\n\n\n\n\n\n\n\nIndiana DNA Database Demographic Distributions\n\n\n\n\n\n\n3.3.8 Summary Statistics\n\n\nShow the summary statistics code\ncat(\"Indiana DNA Database Summary:\\n\")\ncat(\"=\", strrep(\"=\", 40), \"\\n\")\n\n# Total profiles by offender type\ntotals &lt;- foia_combined %&gt;%\n  filter(state == \"Indiana\",\n         variable_category == \"total\",\n         variable_detailed == \"total_profiles\",\n         value_type == \"count\") %&gt;%\n  select(offender_type, value, value_source) %&gt;%\n  mutate(value_formatted = format(value, big.mark = \",\"))\n\nprint(totals)\n\n# Data completeness by value source\ncat(\"\\nData completeness by source:\\n\")\ncompleteness &lt;- foia_combined %&gt;%\n  filter(state == \"Indiana\") %&gt;%\n  group_by(value_source) %&gt;%\n  summarise(n_values = n(), .groups = \"drop\")\n\nprint(completeness)\n\n# Final verification\ncat(\"\\nFinal verification:\\n\")\ncat(paste(\"Counts consistent:\", counts_consistent(foia_combined %&gt;% filter(state == \"Indiana\")), \"\\n\"))\ncat(paste(\"Percentages consistent:\", percentages_consistent(foia_combined %&gt;% filter(state == \"Indiana\")), \"\\n\"))\n\n\nIndiana DNA Database Summary:\n= ======================================== \n# A tibble: 3 Ã— 4\n  offender_type       value value_source value_formatted\n  &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;          \n1 Convicted Offender 279654 reported     \"279,654\"      \n2 Arrestee            21087 reported     \" 21,087\"      \n3 Combined           300741 calculated   \"300,741\"      \n\nData completeness by source:\n# A tibble: 2 Ã— 2\n  value_source n_values\n  &lt;chr&gt;           &lt;int&gt;\n1 calculated         11\n2 reported            4\n\nFinal verification:\nCounts consistent: TRUE \nPercentages consistent: TRUE \n\n\n\n\n3.3.9 Summary of Indiana Processing\nIndiana data processing complete. The unique dataset required:\n\nâœ… Data conversion: String values converted to numeric, handling â€œ&lt;1â€ as 0.5\nâœ… Terminology standardization: â€œCaucasianâ€ converted to â€œWhiteâ€\nâœ… Calculated additions:\n\nCombined total profiles across offender types\nAll demographic counts derived from reported percentages\n\nâœ… Quality checks: All counts and percentages pass consistency validation\nâœ… Provenance tracking: Clear distinction between reported and calculated values\n\nThe Indiana data is now standardized and ready for cross-state analysis."
  },
  {
    "objectID": "qmd_root/foia_processing.html#maine-me",
    "href": "qmd_root/foia_processing.html#maine-me",
    "title": "FOIA Document OCR Processing",
    "section": "3.4 Maine (ME)",
    "text": "3.4 Maine (ME)\nOverview: Maine provides comprehensive reporting with both counts and percentages for all gender and race categories across all offender types, including pre-calculated Combined totals. The data is complete and requires no processing.\n\n3.4.1 Examine Raw Data\nEstablish a baseline understanding of the data exactly as it was received.\n\n\nColumnTypeRowsMissingUniqueUnique_Valuesstatecharacter1901Maineoffender_typecharacter1901Combinedvariable_categorycharacter1903total, gender, racevariable_detailedcharacter1909total_profiles, Male, Female, Unknown, White, Black, Native American, Hispanic, Asianvaluenumeric1901933711 ..., 27694 ..., 82.7 ..., 5734 ..., 17 ..., 83 ..., 0.2 ..., 31298 ..., 92.8 ..., 1299 ...value_typecharacter1902count, percentagevalue_sourcecharacter1901reportedData frame dimensions: 19 rows Ã— 7 columns\n\n\n\n\n3.4.2 Verify Data Consistency\nRuns quality checks using the verify_category_totals(), counts_consistent(), and percentages_consistent() functions.\n\n\nVerifying that demographic counts match reported totals:\n\n\n\n\n\noffender_type\nvariable_category\ntotal_profiles\nsum_counts\ndifference\n\n\n\n\nCombined\ngender\n33711\n33511\n200\n\n\nCombined\nrace\n33711\n33711\n0\n\n\n\n\n\n\nCounts consistency check on raw data:\n\n\nAll counts consistent: FALSE \n\n\n\nPercentage consistency check on raw data:\n\n\nAll percentages sum to ~100%: TRUE \n\n\n\n\n3.4.3 Address Data Gaps\n\n3.4.3.1 Solve Percentages Inconsistency\nRacial percentages summed to 99.9% instead of 100%\nProportional scaling was applied and value_source was updated to â€œcalculatedâ€ for all adjusted values.\n\n\nShow percentage recalculation code\n# Start with the raw data\nme_clean &lt;- me_raw\n\n# Adjust percentages to ensure they sum to 100% and mark as calculated\nme_clean &lt;- me_clean %&gt;%\n  group_by(value_type, variable_category) %&gt;%\n  mutate(\n    value = ifelse(\n      value_type == \"percentage\" & variable_category == \"gender\",\n      value * (100 / sum(value, na.rm = TRUE)),\n      value\n    ),\n    value_source = ifelse(\n      value_type == \"percentage\" & variable_category == \"gender\",\n      \"calculated\",\n      value_source\n    )\n  ) %&gt;%\n  ungroup()\n\n# Verify the new sum\npercentage_sum &lt;- me_clean %&gt;%\n  filter(value_type == \"percentage\" & variable_category == \"gender\") %&gt;%\n  summarise(total = sum(value, na.rm = TRUE))\n\ncat(\"âœ“ Recalculated percentages for Maine - New sum:\", round(percentage_sum$total, 2), \"%\\n\")\n\n\nâœ“ Recalculated percentages for Maine - New sum: 100 %\n\n\n\n\n3.4.3.2 Recalculate Counts from Percentages\nMaineâ€™s reported gender counts sum were inconsistent with the total_profiles.\nWe removed existing gender count data and recalculated counts using percentage values and combined totals.\nAll recalculated values flagged with value_source = \"calculated\"\n\n\nShow count recalculation code\n# Remove existing gender count rows to avoid duplication\nme_clean &lt;- me_clean %&gt;%\n  filter(!(variable_category == \"gender\" & value_type == \"count\"))\n\ncat(\"âœ“ Removed existing gender count data\\n\")\n\nme_gender &lt;- me_clean %&gt;%\n    filter(variable_category == \"gender\" | variable_category == \"total\")\n\n# Calculate counts from percentages for Combined offender type\nme_gender &lt;- calculate_counts_from_percentages(me_gender, \"Maine\")\n\n# Append recalculated gender counts to the main dataset\nme_clean &lt;- bind_rows(me_clean, me_gender)\n\ncat(\"âœ“ Calculated demographic counts from percentages\\n\")\n\n# Verify the calculations\ncat(\"Category totals after calculating counts:\\n\")\nverify_category_totals(me_clean) %&gt;% kable() %&gt;% kable_styling()\n\n\nâœ“ Removed existing gender count data\nâœ“ Calculated demographic counts from percentages\nCategory totals after calculating counts:\n\n\n\n\n\noffender_type\nvariable_category\ntotal_profiles\nsum_counts\ndifference\n\n\n\n\nCombined\ngender\n33711\n33711\n0\n\n\nCombined\nrace\n33711\n33711\n0\n\n\n\n\n\n\n\n\n3.4.4 Verify Data Consistency\nFinal checks to ensure all data is now consistent and complete.\n\n\nFinal data consistency checks:\n\n\nCounts consistent: TRUE \n\n\nPercentages consistent: TRUE \n\n\n\nFinal data availability:\n\n\nRace data: both \n\n\nGender data: both \n\n\n\n\n3.4.5 Prepare for Combined Dataset\nThe Maine data is already complete and consistent. It is formatted to match the master schema and appended to the foia_combined dataframe.\n\n\nShow Maine data preparation to combined dataset\n# Prepare the data for the combined dataset\nme_prepared &lt;- prepare_state_for_combined(me_clean, \"Maine\")\n\n# Append to the master combined dataframe\nfoia_combined &lt;- bind_rows(foia_combined, me_prepared)\n\ncat(paste0(\"âœ“ Appended \", nrow(me_prepared), \" Maine rows to foia_combined\\n\"))\ncat(paste0(\"âœ“ Total rows in foia_combined: \", nrow(foia_combined), \"\\n\"))\n\n\nâœ“ Appended 19 Maine rows to foia_combined\nâœ“ Total rows in foia_combined: 107\n\n\n\n\n3.4.6 Document Metadata\nThe metadata is added with a note that the data was complete and required no processing.\n\n\nShow Maine data preparation and addition to metadata table\n# Add Maine to the metadata table using the helper function\nadd_state_metadata(\"Maine\", me_raw)\n\n# Update metadata with QC results\nupdate_state_metadata(\"Maine\", \n                      counts_ok = counts_consistent(me_clean),\n                      percentages_ok = percentages_consistent(me_clean),\n                      notes_text = \"Complete dataset provided with both counts and percentages. No processing or calculations required. All values are reported.\")\n\n\nâœ“ Metadata added for: Maine \nâœ“ Metadata updated for: Maine \n\n\n\n\n3.4.7 Visualizations\n\n\n\n\n\nMaine DNA Database Demographic Distributions\n\n\n\n\n\n\n\nMaine DNA Database Demographic Distributions\n\n\n\n\n\n\n\nMaine DNA Database Demographic Distributions\n\n\n\n\n\n\n\nMaine DNA Database Demographic Distributions\n\n\n\n\n\n\n3.4.8 Summary Statistics\n\n\nShow the summary statistics code\ncat(\"Maine DNA Database Summary:\\n\")\ncat(\"=\", strrep(\"=\", 40), \"\\n\")\n\n# Total profiles by offender type\ntotals &lt;- foia_combined %&gt;%\n  filter(state == \"Maine\",\n         variable_category == \"total\",\n         variable_detailed == \"total_profiles\",\n         value_type == \"count\") %&gt;%\n  select(offender_type, value) %&gt;%\n  mutate(value_formatted = format(value, big.mark = \",\"))\n\nprint(totals)\n\n# Data completeness\ncat(\"\\nData completeness:\\n\")\ncompleteness &lt;- foia_combined %&gt;%\n  filter(state == \"Maine\") %&gt;%\n  group_by(offender_type, value_source) %&gt;%\n  summarise(n_values = n(), .groups = \"drop\")\n\nprint(completeness)\n\n# Final verification\ncat(\"\\nFinal verification:\\n\")\ncat(paste(\"Counts consistent:\", counts_consistent(foia_combined %&gt;% filter(state == \"Maine\")), \"\\n\"))\ncat(paste(\"Percentages consistent:\", percentages_consistent(foia_combined %&gt;% filter(state == \"Maine\")), \"\\n\"))\n\n\nMaine DNA Database Summary:\n= ======================================== \n# A tibble: 1 Ã— 3\n  offender_type value value_formatted\n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;          \n1 Combined      33711 33,711         \n\nData completeness:\n# A tibble: 2 Ã— 3\n  offender_type value_source n_values\n  &lt;chr&gt;         &lt;chr&gt;           &lt;int&gt;\n1 Combined      calculated          6\n2 Combined      reported           13\n\nFinal verification:\nCounts consistent: TRUE \nPercentages consistent: TRUE \n\n\n\n\n3.4.9 Summary of Maine Processing\nMaine data processing complete. The dataset is exemplary and required no adjustments:\n\nâœ… Reported data: Both counts and percentages for all Convicted Offender, Arrestee, and Combined categories\nâœ… No calculated additions needed: All values are sourced directly from the state report (value_source = \"reported\")\nâœ… Quality checks: All counts and percentages pass consistency validation\nâœ… Provenance tracking: All values maintain their original value_source as â€œreportedâ€\n\nThe Maine data is now standardized and ready for cross-state analysis."
  },
  {
    "objectID": "qmd_root/foia_processing.html#nevada-nv",
    "href": "qmd_root/foia_processing.html#nevada-nv",
    "title": "FOIA Document OCR Processing",
    "section": "3.5 Nevada (NV)",
    "text": "3.5 Nevada (NV)\nOverview: Nevada provides both counts and percentages for gender and race categories but uses non-standard terminology that requires conversion for consistency with our schema.\n\n3.5.1 Examine Raw Data\nEstablish a baseline understanding of the data exactly as it was received.\n\n\nColumnTypeRowsMissingUniqueUnique_Valuesstatecharacter2101Nevadaoffender_typecharacter2104All, Arrestee, Convicted Offender, Combinedvariable_categorycharacter2103total, gender, racevariable_detailedcharacter2109total_flags, total_profiles, Female, Male, Unknown, White, American Indian, Black, Asianvaluenumeric21021344097 ..., 185074 ..., 53.785 ..., 159023 ..., 46.215 ..., 63287 ..., 18.392 ..., 280738 ..., 81.587 ..., 72 ...value_typecharacter2102count, percentagevalue_sourcecharacter2101reportedData frame dimensions: 21 rows Ã— 7 columns\n\n\n\n\n3.5.2 Verify Data Consistency\nInitial check reveals Nevadaâ€™s non-standard terminology.\n\n\nInitial data availability:\n\n\nRace data: both \n\n\nGender data: both \n\n\n\nNon-standard terminology found:\n\n\nOffender types: All, Arrestee, Convicted Offender, Combined \n\n\n\n\n3.5.3 Address Data Gaps\n\n3.5.3.1 Standardize Terminology\nNevada uses â€œAllâ€ instead of â€œCombinedâ€, â€œtotal_flagsâ€ instead of â€œtotal_profilesâ€ and â€œAmerican Indianâ€ instead of â€œNative Americanâ€.\n\n\nShow terminology standardization code\n# Start with raw data\nnv_clean &lt;- nv_raw\n\n# Standardize offender types and racial terminology\nnv_clean &lt;- nv_clean %&gt;%\n  mutate(\n    offender_type = case_when(\n      offender_type == \"All\" ~ \"Combined\",\n      TRUE ~ offender_type\n    ),\n    variable_detailed = case_when(\n      variable_detailed == \"total_flags\" ~ \"total_profiles\",\n      TRUE ~ variable_detailed\n    ),\n    variable_detailed = case_when(\n      variable_detailed == \"American Indian\" ~ \"Native American\",\n      TRUE ~ variable_detailed\n    )\n  )\n\ncat(\"âœ“ Standardized terminology:\\n\")\ncat(\"  - 'All' â†’ 'Combined'\\n\")\ncat(\"  - 'total_flags' â†’ 'total_profiles'\\n\")\ncat(\"  - 'American Indian' â†’ 'Native American'\\n\")\n\n\nâœ“ Standardized terminology:\n  - 'All' â†’ 'Combined'\n  - 'total_flags' â†’ 'total_profiles'\n  - 'American Indian' â†’ 'Native American'\n\n\n\n\n3.5.3.2 Verify Consistency\nNow that the offender types are standardized, we can verify the counts and percentages.\n\n\nVerifying that demographic counts match reported totals:\n\n\n\n\n\noffender_type\nvariable_category\ntotal_profiles\nsum_counts\ndifference\n\n\n\n\nCombined\ngender\n344097\n344097\n0\n\n\nCombined\nrace\n344097\n344097\n0\n\n\n\n\n\n\nCounts consistency check on raw data:\n\n\nAll counts consistent: TRUE \n\n\n\nPercentage consistency check on raw data:\n\n\nAll percentages sum to ~100%: TRUE \n\n\nSum of 'race' percentages: 100 %\n\n\nSum of 'gender' percentages: 100 %\n\n\n\n\n\n3.5.4 Prepare for Combined Dataset\nThe cleaned data is formatted to match the master schema and appended to the foia_combined dataframe.\n\n\nShow Nevada data preparation to combined dataset\n# Prepare the cleaned data for the combined dataset\nnv_prepared &lt;- prepare_state_for_combined(nv_clean, \"Nevada\")\n\n# Append to the master combined dataframe\nfoia_combined &lt;- bind_rows(foia_combined, nv_prepared)\n\ncat(paste0(\"âœ“ Appended \", nrow(nv_prepared), \" Nevada rows to foia_combined\\n\"))\ncat(paste0(\"âœ“ Total rows in foia_combined: \", nrow(foia_combined), \"\\n\"))\n\n\nâœ“ Appended 21 Nevada rows to foia_combined\nâœ“ Total rows in foia_combined: 128\n\n\n\n\n3.5.5 Document Metadata\nThe metadata is added with details on the terminology standardization performed.\n\n\nShow Nevada data preparation and addition to metadata table\n# Add Nevada to the metadata table using the helper function\nadd_state_metadata(\"Nevada\", nv_raw)\n\n# Update metadata with QC results and processing notes\nupdate_state_metadata(\"Nevada\", \n                      counts_ok = counts_consistent(nv_clean),\n                      percentages_ok = percentages_consistent(nv_clean),\n                      notes_text = \"Standardized terminology: 'All' to 'Combined' and 'American Indian' to 'Native American'. All values remain reported.\")\n\n\nâœ“ Metadata added for: Nevada \nâœ“ Metadata updated for: Nevada \n\n\n\n\n3.5.6 Visualizations\n\n\n\n\n\nNevada DNA Database Demographic Distributions\n\n\n\n\n\n\n\nNevada DNA Database Demographic Distributions\n\n\n\n\n\n\n\nNevada DNA Database Demographic Distributions\n\n\n\n\n\n\n\nNevada DNA Database Demographic Distributions\n\n\n\n\n\n\n3.5.7 Summary Statistics\n\n\nShow the summary statistics code\ncat(\"Nevada DNA Database Summary:\\n\")\ncat(\"=\", strrep(\"=\", 40), \"\\n\")\n\n# Total profiles by offender type\ntotals &lt;- foia_combined %&gt;%\n  filter(state == \"Nevada\",\n         variable_category == \"total\",\n         variable_detailed == \"total_profiles\",\n         value_type == \"count\") %&gt;%\n  select(offender_type, value) %&gt;%\n  mutate(value_formatted = format(value, big.mark = \",\"))\n\nprint(totals)\n\n# Data completeness\ncat(\"\\nData completeness:\\n\")\ncompleteness &lt;- foia_combined %&gt;%\n  filter(state == \"Nevada\") %&gt;%\n  group_by(offender_type, value_source) %&gt;%\n  summarise(n_values = n(), .groups = \"drop\")\n\nprint(completeness)\n\n# Final verification\ncat(\"\\nFinal verification:\\n\")\ncat(paste(\"Counts consistent:\", counts_consistent(foia_combined %&gt;% filter(state == \"Nevada\")), \"\\n\"))\ncat(paste(\"Percentages consistent:\", percentages_consistent(foia_combined %&gt;% filter(state == \"Nevada\")), \"\\n\"))\n\n\nNevada DNA Database Summary:\n= ======================================== \n# A tibble: 3 Ã— 3\n  offender_type       value value_formatted\n  &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;          \n1 Combined           344097 344,097        \n2 Arrestee           185074 185,074        \n3 Convicted Offender 159023 159,023        \n\nData completeness:\n# A tibble: 3 Ã— 3\n  offender_type      value_source n_values\n  &lt;chr&gt;              &lt;chr&gt;           &lt;int&gt;\n1 Arrestee           reported            2\n2 Combined           reported           17\n3 Convicted Offender reported            2\n\nFinal verification:\nCounts consistent: TRUE \nPercentages consistent: FALSE \n\n\n\n\n3.5.8 Summary of Nevada Processing\nNevada data processing complete. The dataset required minimal adjustments:\n\nâœ… Terminology standardization:\n\nâ€œAllâ€ â†’ â€œCombinedâ€ (offender type)\nâ€œAmerican Indianâ€ â†’ â€œNative Americanâ€ (race category)\n\nâœ… Reported data: Both counts and percentages for all categories\nâœ… Quality checks: All counts and percentages pass consistency validation\nâœ… Provenance tracking: All values maintain value_source = \"reported\" as only terminology changes were made\n\nThe Nevada data is now standardized and ready for cross-state analysis."
  },
  {
    "objectID": "qmd_root/foia_processing.html#south-dakota-sd",
    "href": "qmd_root/foia_processing.html#south-dakota-sd",
    "title": "FOIA Document OCR Processing",
    "section": "3.6 South Dakota (SD)",
    "text": "3.6 South Dakota (SD)\nOverview: South Dakota provides the most comprehensive reporting with both counts and percentages for all standard categories plus unique intersectional genderÃ—race data. Minor terminology standardization is required for consistency.\n\n3.6.1 Examine Raw Data\nEstablish a baseline understanding of the data exactly as it was received.\n\n\nColumnTypeRowsMissingUniqueUnique_Valuesstatecharacter4101South Dakotaoffender_typecharacter4101Combinedvariable_categorycharacter4104total, gender, race, gender_racevariable_detailedcharacter41021total_profiles ..., Male ..., Female ..., Asian ..., Black ..., Hispanic ..., Native American ..., Other/Unknown ..., White/Caucasian ..., Male_Asian ...valuenumeric4103867753 ..., 51197 ..., 75.56 ..., 16556 ..., 24.44 ..., 5 ..., 0.08 ..., 4041 ..., 5.96 ..., 2949 ...value_typecharacter4102count, percentagevalue_sourcecharacter4101reportedData frame dimensions: 41 rows Ã— 7 columns\n\n\n\n\n3.6.2 Gender-race intersection analysis\nSince South Dakota is the only state that reported gender-race intersection data, we can analyze it in detail.\n\n\n\n\n\n\nSouth Dakota Intersectional Gender Ã— Race Analysis\n\n\n\n\n\n\n\n3.6.3 Verify Data Consistency\nInitial check reveals South Dakotaâ€™s comprehensive data structure with some non-standard terminology.\n\n\nInitial data availability:\n\n\nRace data: both \n\n\nGender data: both \n\n\n\nNon-standard terminology found:\n\n\nRace terms: Asian, Black, Hispanic, Native American, Other/Unknown, White/Caucasian \n\n\nVerifying that demographic counts match reported totals:\n\n\n\n\n\noffender_type\nvariable_category\ntotal_profiles\nsum_counts\ndifference\n\n\n\n\nCombined\ngender\n67753\n67753\n0\n\n\nCombined\nrace\n67753\n67702\n51\n\n\n\n\n\n\nCounts consistency check on raw data:\n\n\nAll counts consistent: FALSE \n\n\n\nPercentage consistency check on raw data:\n\n\nAll percentages sum to ~100%: TRUE \n\n\nSum of 'race' percentages: 100 %\n\n\nSum of 'gender' percentages: 100 %\n\n\n\n\n3.6.4 Address Data Gaps\n\n3.6.4.1 Standardize Terminology\nSouth Dakota uses â€œWhite/Caucasianâ€ and â€œOther/Unknownâ€ which need standardization.\n\n\nShow terminology standardization code\n# Standardize racial terminology\nsd_clean &lt;- sd_clean %&gt;%\n  mutate(\n    variable_detailed = case_when(\n      variable_detailed == \"White/Caucasian\" ~ \"White\",\n      variable_detailed == \"Other/Unknown\" ~ \"Unknown\",\n      TRUE ~ variable_detailed\n    )\n  )\n\ncat(\"âœ“ Standardized terminology:\\n\")\ncat(\"  - 'White/Caucasian' â†’ 'White'\\n\")\ncat(\"  - 'Other/Unknown' â†’ 'Unknown'\\n\")\n\n# Verify the changes\ncat(\"\\nRace categories after standardization:\\n\")\nsd_clean %&gt;%\n  filter(variable_category == \"race\") %&gt;%\n  distinct(variable_detailed) %&gt;%\n  pull() %&gt;%\n  paste(collapse = \", \") %&gt;%\n  cat()\n\n\nâœ“ Standardized terminology:\n  - 'White/Caucasian' â†’ 'White'\n  - 'Other/Unknown' â†’ 'Unknown'\n\nRace categories after standardization:\nAsian, Black, Hispanic, Native American, Unknown, White\n\n\n\n\n3.6.4.2 Recalculate Counts from Percentages\nSouth Dakotaâ€™s reported race counts sum were inconsistent with the total_profiles.\nWe removed existing gender count data and recalculated counts using percentage values and combined totals.\nAll recalculated values flagged with value_source = \"calculated\"\n\n\nShow count recalculation code\n# Remove existing gender count rows to avoid duplication\nsd_clean &lt;- sd_clean %&gt;%\n  filter(!(variable_category == \"race\" & value_type == \"count\"))\n\ncat(\"âœ“ Removed existing race count data\\n\")\n\nsd_race &lt;- sd_clean %&gt;%\n    filter(variable_category == \"race\" | variable_category == \"total\")\n\n# Calculate counts from percentages for Combined offender type\nsd_race &lt;- calculate_counts_from_percentages(sd_race, \"South Dakota\")\n\n# Append recalculated race counts to the main dataset\nsd_clean &lt;- bind_rows(sd_clean, sd_race)\n\ncat(\"âœ“ Calculated demographic counts from percentages\\n\")\n\n# Verify the calculations\ncat(\"Category totals after calculating counts:\\n\")\nverify_category_totals(sd_clean) %&gt;% kable() %&gt;% kable_styling()\n\n\nâœ“ Removed existing race count data\nâœ“ Calculated demographic counts from percentages\nCategory totals after calculating counts:\n\n\n\n\n\noffender_type\nvariable_category\ntotal_profiles\nsum_counts\ndifference\n\n\n\n\nCombined\ngender\n67753\n67753\n0\n\n\nCombined\nrace\n67753\n67752\n1\n\n\n\n\n\nWe handled this diffence of 1 by adding it to the most representative race (White).\n\n\nShow difference handle code\n# Handle the difference of 1 by adding it to the most representative race\nsd_clean &lt;- sd_clean %&gt;%\n  mutate(value = ifelse(variable_detailed == \"White\" & value_type == \"count\", value + 1, value))\n\n\n\n\n\n3.6.5 Verify Data Consistency\nFinal checks to ensure standardization didnâ€™t affect data integrity.\n\n\nFinal data consistency checks after standardization:\n\n\nVerifying that demographic counts match reported totals:\n\n\n\n\n\noffender_type\nvariable_category\ntotal_profiles\nsum_counts\ndifference\n\n\n\n\nCombined\ngender\n67753\n67753\n0\n\n\nCombined\nrace\n67753\n67753\n0\n\n\n\n\n\n\nCounts consistency check:\n\n\nAll counts consistent: TRUE \n\n\n\nPercentage consistency check:\n\n\nAll percentages sum to ~100%: TRUE \n\n\n\n\n3.6.6 Prepare for Combined Dataset\nThe cleaned data is formatted to match the master schema and appended to the foia_combined dataframe.\n\n\nShow South Dakota data preparation to combined dataset\n# Prepare the cleaned data for the combined dataset\nsd_prepared &lt;- prepare_state_for_combined(sd_clean, \"South Dakota\")\n\n# Append to the master combined dataframe\nfoia_combined &lt;- bind_rows(foia_combined, sd_prepared)\n\ncat(paste0(\"âœ“ Appended \", nrow(sd_prepared), \" South Dakota rows to foia_combined\\n\"))\ncat(paste0(\"âœ“ Total rows in foia_combined: \", nrow(foia_combined), \"\\n\"))\n\n# Show the comprehensive nature of South Dakota's data\ncat(\"\\nSouth Dakota's comprehensive data structure:\\n\")\nsd_prepared %&gt;%\n  group_by(variable_category) %&gt;%\n  summarise(n_rows = n(), .groups = \"drop\") %&gt;%\n  kable() %&gt;% kable_styling()\n\n\nâœ“ Appended 17 South Dakota rows to foia_combined\nâœ“ Total rows in foia_combined: 145\n\nSouth Dakota's comprehensive data structure:\n\n\n\n\n\nvariable_category\nn_rows\n\n\n\n\ngender\n4\n\n\nrace\n12\n\n\ntotal\n1\n\n\n\n\n\n\n\n3.6.7 Document Metadata\nThe metadata is added with details on South Dakotaâ€™s comprehensive reporting and the terminology standardization performed.\n\n\nShow South Dakota data preparation and addition to metadata table\n# Add South Dakota to the metadata table using the helper function\nadd_state_metadata(\"South Dakota\", sd_raw)\n\n# Update metadata with QC results and processing notes\nupdate_state_metadata(\"South Dakota\", \n                      counts_ok = counts_consistent(sd_clean),\n                      percentages_ok = percentages_consistent(sd_clean),\n                      notes_text = \"Standardized terminology: 'White/Caucasian' to 'White' and 'Other/Unknown' to 'Unknown'. Includes comprehensive gender_race intersectional data. All values remain reported.\")\n\n\nâœ“ Metadata added for: South Dakota \nâœ“ Metadata updated for: South Dakota \n\n\n\n\n3.6.8 Visualizations\n\n\n\n\n\nSouth Dakota DNA Database Demographic Distributions\n\n\n\n\n\n\n\nSouth Dakota DNA Database Demographic Distributions\n\n\n\n\n\n\n\nSouth Dakota DNA Database Demographic Distributions\n\n\n\n\n\n\n\nSouth Dakota DNA Database Demographic Distributions\n\n\n\n\n\n\n3.6.9 Summary Statistics\n\n\nShow the summary statistics code\ncat(\"South Dakota DNA Database Summary:\\n\")\ncat(\"=\", strrep(\"=\", 40), \"\\n\")\n\n# Total profiles by offender type\ntotals &lt;- foia_combined %&gt;%\n  filter(state == \"South Dakota\",\n         variable_category == \"total\",\n         variable_detailed == \"total_profiles\",\n         value_type == \"count\") %&gt;%\n  select(offender_type, value) %&gt;%\n  mutate(value_formatted = format(value, big.mark = \",\"))\n\nprint(totals)\n\n# Data completeness by category\ncat(\"\\nData completeness by category:\\n\")\ncompleteness &lt;- foia_combined %&gt;%\n  filter(state == \"South Dakota\") %&gt;%\n  group_by(variable_category) %&gt;%\n  summarise(n_values = n(), .groups = \"drop\")\n\nprint(completeness)\n\n# Final verification\ncat(\"\\nFinal verification:\\n\")\ncat(paste(\"Counts consistent:\", counts_consistent(foia_combined %&gt;% filter(state == \"South Dakota\")), \"\\n\"))\ncat(paste(\"Percentages consistent:\", percentages_consistent(foia_combined %&gt;% filter(state == \"South Dakota\")), \"\\n\"))\n\n\nSouth Dakota DNA Database Summary:\n= ======================================== \n# A tibble: 1 Ã— 3\n  offender_type value value_formatted\n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;          \n1 Combined      67753 67,753         \n\nData completeness by category:\n# A tibble: 3 Ã— 2\n  variable_category n_values\n  &lt;chr&gt;                &lt;int&gt;\n1 gender                   4\n2 race                    12\n3 total                    1\n\nFinal verification:\nCounts consistent: TRUE \nPercentages consistent: TRUE \n\n\n\n\n3.6.10 Summary of South Dakota Processing\nSouth Dakota data processing complete. The state provided exemplary data with minimal adjustments needed:\n\nâœ… Terminology standardization:\n\nâ€œWhite/Caucasianâ€ â†’ â€œWhiteâ€\nâ€œOther/Unknownâ€ â†’ â€œUnknownâ€\n\nâœ… Comprehensive reporting: Standard demographics plus unique genderÃ—race intersectional data\nâœ… Reported data: Both counts and percentages for all categories\nâœ… Quality checks: All counts and percentages pass consistency validation\nâœ… Provenance tracking: All values maintain value_source = \"reported\" as only terminology changes were made\n\nSouth Dakotaâ€™s data is now standardized and ready for cross-state analysis."
  },
  {
    "objectID": "qmd_root/foia_processing.html#texas-tx",
    "href": "qmd_root/foia_processing.html#texas-tx",
    "title": "FOIA Document OCR Processing",
    "section": "3.7 Texas (TX)",
    "text": "3.7 Texas (TX)\nOverview: Texas provides counts only for gender and race categories. The Male gender is missing in the dataset. The state uses non-standard terminology that requires conversion and needs Combined totals and percentages calculated.\n\n3.7.1 Examine Raw Data\nEstablish a baseline understanding of the data exactly as it was received.\n\n\nColumnTypeRowsMissingUniqueUnique_Valuesstatecharacter1601Texasoffender_typecharacter1602Offenders, Arresteevariable_categorycharacter1603total, gender, racevariable_detailedcharacter1608total_profiles, Female, Asian, African American, Caucasian, Hispanic, Native American, Othervaluenumeric16016845322 ..., 73631 ..., 121434 ..., 18721 ..., 3361 ..., 254366 ..., 309010 ..., 276245 ..., 138 ..., 2173 ...value_typecharacter1601countvalue_sourcecharacter1601reportedData frame dimensions: 16 rows Ã— 7 columns\n\n\n\n\n3.7.2 Verify Data Consistency\nInitial checks reveal Texasâ€™s reporting structure and terminology differences.\n\n\nInitial data availability:\n\n\nRace data: counts \n\n\nGender data: counts \n\n\n\nNon-standard terminology found:\n\n\nOffender types: Offenders, Arrestee \n\n\nRace terms: Asian, African American, Caucasian, Hispanic, Native American, Other \n\n\n\n\n3.7.3 Address Data Gaps\n\n3.7.3.1 Add Missing Male category\nTexas data reports only Female counts explicitly. We calculated Male counts by subtracting Female counts from total profiles, assuming binary gender classification in the dataset.\n\n\nShow male addition code\n# First, let's examine the current structure of gender data\ngender_data &lt;- tx_raw %&gt;%\n  filter(variable_category == \"gender\")\n\ncat(\"Current gender structure:\\n\")\nprint(unique(gender_data$variable_detailed))\n\n# Get total profiles for each offender type\ntotal_profiles &lt;- tx_raw %&gt;%\n  filter(variable_category == \"total\" & variable_detailed == \"total_profiles\") %&gt;%\n  select(offender_type, total_value = value)\n\n# Join total profiles with gender data\ngender_with_totals &lt;- gender_data %&gt;%\n  left_join(total_profiles, by = \"offender_type\")\n\n# Create Male entries for each offender type\nmale_entries &lt;- gender_with_totals %&gt;%\n  filter(variable_detailed == \"Female\") %&gt;%\n  mutate(\n    variable_detailed = \"Male\",\n    value = total_value - value, \n    value_source = \"calculated\",\n    total_value = NULL \n  )\n\n# Add these entries to the original dataset\ntx_raw_with_male &lt;- tx_raw %&gt;%\n  bind_rows(male_entries)\n\n# Update the tx_raw object\ntx_clean &lt;- tx_raw_with_male\n\n# Verify the addition\ncat(\"\\nAfter adding Male entries - gender categories:\\n\")\nprint(unique(tx_clean %&gt;% \n       filter(variable_category == \"gender\") %&gt;% \n       pull(variable_detailed)))\n\n\nCurrent gender structure:\n[1] \"Female\"\n\nAfter adding Male entries - gender categories:\n[1] \"Female\" \"Male\"  \n\n\n\n\n3.7.3.2 Standardize Terminology\nTexas uses â€œOffendersâ€ instead of â€œConvicted Offenderâ€ and â€œCaucasianâ€ instead of â€œWhiteâ€.\n\n\nShow terminology standardization code\n# Standardize offender types and racial terminology\ntx_clean &lt;- tx_clean %&gt;%\n  mutate(\n    offender_type = case_when(\n      offender_type == \"Offenders\" ~ \"Convicted Offender\",\n      TRUE ~ offender_type\n    ),\n    variable_detailed = case_when(\n      variable_detailed == \"Caucasian\" ~ \"White\",\n      variable_detailed == \"African American\" ~ \"Black\",\n      TRUE ~ variable_detailed\n    )\n  )\n\ncat(\"âœ“ Standardized terminology:\\n\")\ncat(\"  - 'Offenders' â†’ 'Convicted Offender'\\n\")\ncat(\"  - 'Caucasian' â†’ 'White'\\n\")\ncat(\"  - 'African American' â†’ 'Black'\\n\")\ncat(paste(\"Offender types after standardization:\", paste(sort(unique(tx_clean$offender_type)), collapse = \", \"), \"\\n\"))\n\n\nâœ“ Standardized terminology:\n  - 'Offenders' â†’ 'Convicted Offender'\n  - 'Caucasian' â†’ 'White'\n  - 'African American' â†’ 'Black'\nOffender types after standardization: Arrestee, Convicted Offender \n\n\n\n\n3.7.3.3 Create Unknown Category\nTexas race count is inconsistent, with a significant number of profiles not reported in any racial category.\nUnknown category was created to account for these missing profiles.\nThe calculated values are added with a value_source = \"calculated\" tag to maintain transparency about what was provided versus what was derived.\n\n\nShow unknown addition code\n# Add Unknown race category to reconcile totals\ntx_clean &lt;- fill_demographic_gaps(tx_clean)\n\n# Verify the fix\ncat(\"Category totals after adding Unknown race category:\\n\")\nverify_category_totals(tx_clean) %&gt;% kable() %&gt;% kable_styling()\n\ncat(\"\\nCounts consistency after adding Unknown:\\n\")\ncat(paste(\"All counts consistent:\", counts_consistent(tx_clean), \"\\n\"))\n\n\nCategory totals after adding Unknown race category:\n\n\n\n\n\noffender_type\nvariable_category\ntotal_profiles\nsum_counts\ndifference\n\n\n\n\nArrestee\ngender\n73631\n73631\n0\n\n\nArrestee\nrace\n73631\n73631\n0\n\n\nConvicted Offender\ngender\n845322\n845322\n0\n\n\nConvicted Offender\nrace\n845322\n845322\n0\n\n\n\n\n\n\nCounts consistency after adding Unknown:\nAll counts consistent: TRUE \n\n\n\n\n3.7.3.4 Create Combined Totals\nTexas only reported data for â€œConvicted Offenderâ€ and â€œArresteeâ€ separately. We calculate Combined totals.\n\n\nShow combined addition code\n# Calculate Combined totals using helper function\ntx_clean &lt;- add_combined(tx_clean)\n\ncat(\"âœ“ Created Combined totals for Texas\\n\")\n\n# Show the Combined total\ncombined_total &lt;- tx_clean %&gt;%\n  filter(offender_type == \"Combined\",\n         variable_category == \"total\",\n         variable_detailed == \"total_profiles\") %&gt;%\n  pull(value)\n\ncat(paste(\"Combined total profiles:\", format(combined_total, big.mark = \",\"), \"\\n\"))\n\n\nâœ“ Created Combined totals for Texas\nCombined total profiles: 918,953 \n\n\n\n\n3.7.3.5 Calculate Percentages\nTransforms the data from counts into percentages for comparative analysis.\n\n\nShow percentage calculation code\n# Derive percentages from counts\ntx_clean &lt;- add_percentages(tx_clean)\n\ncat(\"âœ“ Added percentages for all demographic categories\\n\")\n\n# Check percentage consistency\ncat(\"Percentage consistency check:\\n\")\ncat(paste(\"All percentages sum to ~100%:\", percentages_consistent(tx_clean), \"\\n\\n\"))\n\n# Show current data availability\ncat(\"Final data availability:\\n\")\ncat(paste(\"Race data:\", report_status(tx_clean, \"race\"), \"\\n\"))\ncat(paste(\"Gender data:\", report_status(tx_clean, \"gender\"), \"\\n\"))\n\n\nâœ“ Added percentages for all demographic categories\nPercentage consistency check:\nAll percentages sum to ~100%: TRUE \n\nFinal data availability:\nRace data: both \nGender data: both \n\n\n\n\n\n3.7.4 Verify Data Consistency\nFinal checks to ensure all processing maintained data integrity.\n\n\nFinal data consistency checks:\n\n\nVerifying that demographic counts match reported totals:\n\n\n\n\n\noffender_type\nvariable_category\ntotal_profiles\nsum_counts\ndifference\n\n\n\n\nArrestee\ngender\n73631\n73631\n0\n\n\nArrestee\nrace\n73631\n73631\n0\n\n\nCombined\ngender\n918953\n918953\n0\n\n\nCombined\nrace\n918953\n918953\n0\n\n\nConvicted Offender\ngender\n845322\n845322\n0\n\n\nConvicted Offender\nrace\n845322\n845322\n0\n\n\n\n\n\n\nCounts consistency check:\n\n\nAll counts consistent: TRUE \n\n\n\nPercentage consistency check:\n\n\nAll percentages sum to ~100%: TRUE \n\n\n\n\n3.7.5 Prepare for Combined Dataset\nThe cleaned data is formatted to match the master schema and appended to the foia_combined dataframe.\n\n\nShow Texas data preparation to combined dataset\n# Prepare the cleaned data for the combined dataset\ntx_prepared &lt;- prepare_state_for_combined(tx_clean, \"Texas\")\n\n# Append to the master combined dataframe\nfoia_combined &lt;- bind_rows(foia_combined, tx_prepared)\n\ncat(paste0(\"âœ“ Appended \", nrow(tx_prepared), \" Texas rows to foia_combined\\n\"))\ncat(paste0(\"âœ“ Total rows in foia_combined: \", nrow(foia_combined), \"\\n\"))\n\n\nâœ“ Appended 57 Texas rows to foia_combined\nâœ“ Total rows in foia_combined: 202\n\n\n\n\n3.7.6 Document Metadata\nThe metadata is added with details on all processing steps performed.\n\n\nShow Texas data preparation and addition to metadata table\n# Add Texas to the metadata table using the helper function\nadd_state_metadata(\"Texas\", tx_raw)\n\n# Update metadata with QC results and processing notes\nupdate_state_metadata(\"Texas\", \n                      counts_ok = counts_consistent(tx_clean),\n                      percentages_ok = percentages_consistent(tx_clean),\n                      notes_text = \"Standardized terminology: 'Offenders' to 'Convicted Offender', 'Caucasian' to 'White', 'African American' to 'Black'; calculated Combined totals and all percentages\")\n\n\nâœ“ Metadata added for: Texas \nâœ“ Metadata updated for: Texas \n\n\n\n\n3.7.7 Visualizations\n\n\n\n\n\nTexas DNA Database Demographic Distributions\n\n\n\n\n\n\n\nTexas DNA Database Demographic Distributions\n\n\n\n\n\n\n\nTexas DNA Database Demographic Distributions\n\n\n\n\n\n\n\nTexas DNA Database Demographic Distributions\n\n\n\n\n\n\n\nTexas DNA Database Demographic Distributions\n\n\n\n\n\n\n\nTexas DNA Database Demographic Distributions\n\n\n\n\n\n\n\nTexas DNA Database Demographic Distributions\n\n\n\n\n\n\n\nTexas DNA Database Demographic Distributions\n\n\n\n\n\n\n\nTexas DNA Database Demographic Distributions\n\n\n\n\n\n\n\nTexas DNA Database Demographic Distributions\n\n\n\n\n\n\n\nTexas DNA Database Demographic Distributions\n\n\n\n\n\n\n\nTexas DNA Database Demographic Distributions\n\n\n\n\n\n\n\n\n\nTexas Demographic Distributions by Offender Type\n\n\n\n\n\n\n3.7.8 Summary Statistics\n\n\nShow the summary statistics code\ncat(\"Texas DNA Database Summary:\\n\")\ncat(\"=\", strrep(\"=\", 40), \"\\n\")\n\n# Total profiles by offender type\ntotals &lt;- foia_combined %&gt;%\n  filter(state == \"Texas\",\n         variable_category == \"total\",\n         variable_detailed == \"total_profiles\",\n         value_type == \"count\") %&gt;%\n  select(offender_type, value, value_source) %&gt;%\n  mutate(value_formatted = format(value, big.mark = \",\"))\n\nprint(totals)\n\n# Data completeness by value source\ncat(\"\\nData completeness by source:\\n\")\ncompleteness &lt;- foia_combined %&gt;%\n  filter(state == \"Texas\") %&gt;%\n  group_by(value_source) %&gt;%\n  summarise(n_values = n(), .groups = \"drop\")\n\nprint(completeness)\n\n# Final verification\ncat(\"\\nFinal verification:\\n\")\ncat(paste(\"Counts consistent:\", counts_consistent(foia_combined %&gt;% filter(state == \"Texas\")), \"\\n\"))\ncat(paste(\"Percentages consistent:\", percentages_consistent(foia_combined %&gt;% filter(state == \"Texas\")), \"\\n\"))\n\n\nTexas DNA Database Summary:\n= ======================================== \n# A tibble: 3 Ã— 4\n  offender_type       value value_source value_formatted\n  &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;          \n1 Convicted Offender 845322 reported     \"845,322\"      \n2 Arrestee            73631 reported     \" 73,631\"      \n3 Combined           918953 calculated   \"918,953\"      \n\nData completeness by source:\n# A tibble: 2 Ã— 2\n  value_source n_values\n  &lt;chr&gt;           &lt;int&gt;\n1 calculated         41\n2 reported           16\n\nFinal verification:\nCounts consistent: TRUE \nPercentages consistent: TRUE \n\n\n\n\n3.7.9 Summary of Texas Processing\nTexas data processing complete. The dataset required several adjustments:\n\nâœ… Male Category Addition:\n\nâ€œMaleâ€ added to variable_detailed\n\nâœ… Terminology standardization:\n\nâ€œOffendersâ€ â†’ â€œConvicted Offenderâ€\nâ€œCaucasianâ€ â†’ â€œWhiteâ€\nâ€œAfrican Americanâ€ â†’ â€œBlackâ€\n\nâœ… Calculated additions:\n\nCombined totals across all offender types\nPercentage values for all demographic categories\n\nâœ… Quality checks: All counts and percentages pass consistency validation\nâœ… Provenance tracking: Clear distinction between reported and calculated values\n\nThe Texas data is now standardized and ready for cross-state analysis."
  },
  {
    "objectID": "qmd_root/foia_processing.html#combined-dataset",
    "href": "qmd_root/foia_processing.html#combined-dataset",
    "title": "FOIA Document OCR Processing",
    "section": "3.8 Combined Dataset",
    "text": "3.8 Combined Dataset"
  },
  {
    "objectID": "qmd_root/foia_processing.html#metadata-table",
    "href": "qmd_root/foia_processing.html#metadata-table",
    "title": "FOIA Document OCR Processing",
    "section": "3.9 Metadata table",
    "text": "3.9 Metadata table"
  },
  {
    "objectID": "qmd_root/appendix_analysis.html",
    "href": "qmd_root/appendix_analysis.html",
    "title": "Annual DNA collection from Murphy & Tong Study",
    "section": "",
    "text": "This document provides complete transparency regarding the data sources and methodology used to compile racial disparities in DNA collection across U.S. states. The original data was collected in Summer 2017, with most data points from 2013-2016.\n\n\n\nConsistent racial disparities: Black populations show the highest DNA collection rates relative to their population percentage in nearly all states\nData limitations: Many states lack comprehensive conviction data, requiring the use of prison admission proxies\nMethodological challenges: Hispanic/Latino populations often uncounted or miscategorized in state data"
  },
  {
    "objectID": "qmd_root/appendix_analysis.html#executive-summary",
    "href": "qmd_root/appendix_analysis.html#executive-summary",
    "title": "Annual DNA collection from Murphy & Tong Study",
    "section": "",
    "text": "This document provides complete transparency regarding the data sources and methodology used to compile racial disparities in DNA collection across U.S. states. The original data was collected in Summer 2017, with most data points from 2013-2016.\n\n\n\nConsistent racial disparities: Black populations show the highest DNA collection rates relative to their population percentage in nearly all states\nData limitations: Many states lack comprehensive conviction data, requiring the use of prison admission proxies\nMethodological challenges: Hispanic/Latino populations often uncounted or miscategorized in state data"
  },
  {
    "objectID": "qmd_root/appendix_analysis.html#methodology-overview",
    "href": "qmd_root/appendix_analysis.html#methodology-overview",
    "title": "Annual DNA collection from Murphy & Tong Study",
    "section": "Methodology Overview",
    "text": "Methodology Overview\n\nData Collection Period\n\nPrimary collection: Summer 2017\nData years used: Single year per state (2012-2016, varies by availability)\nCensus baseline: 2010 U.S. Census for demographic comparisons\n\n\n\nGeneral Challenges Encountered\n\nConviction Data Scarcity: Most states do not publicly disclose comprehensive felony conviction data\nPrison Admission Proxy: Prison admissions used as substitute for conviction data in majority of states\nRacial Classification Inconsistencies:\n\n-   Many states only report \"Black\" and \"White\" categories\n\n-   Hispanic/Latino often classified as ethnicity rather than race\n\n-   \"Other\" category frequently used without specification\n\nArrest Data Gaps: Racial makeup of arrests often unavailable or incomplete"
  },
  {
    "objectID": "qmd_root/appendix_analysis.html#national-summary-table",
    "href": "qmd_root/appendix_analysis.html#national-summary-table",
    "title": "Annual DNA collection from Murphy & Tong Study",
    "section": "National Summary Table",
    "text": "National Summary Table\n\nMethodology: Parsing DNA Collection Data\nThe national summary data was extracted from a structured text file (MurphyTong_Racial_Breakdown.txt) containing three distinct data sections for each state:\n\nDNA Collection Data: Percentage and absolute counts of DNA profiles collected by race (e.g., â€œ46% B (18,253)â€)\nPopulation Demographics: Percentage of state population by race from 2010 Census\n\nProcessing Steps:\n\nState Identification: The parser identifies state entries using standard two-letter abbreviations (AL, AK, AZ, etc.)\nSection Segmentation: For each state, the text is divided into three sections based on the pattern of â€œ% Bâ€ (Black percentage) occurrences\nData Extraction: Regular expressions extract both percentages and counts from the first section, and percentages only from the demographic and rate sections\nPattern Matching: The code uses regex patterns like ([0-9.]+)%\\s*B\\s*\\(([0-9,]+)\\) to capture both percentage (e.g., 46%) and count (e.g., 18,253) for DNA collection data\nRace Categories: Data is extracted for five racial categories: Black (B), Hispanic (H), Asian (A), Native American (N), and White (W)\n\nThis automated parsing ensures reproducibility and allows for systematic extraction of data from the original Murphy & Tong study format.\n\n\nShow summary code\n# Function to parse DNA collection data from text file\nparse_dna_data &lt;- function(file_path) {\n  \n  # Read the entire file\n  text_data &lt;- readLines(file_path, warn = FALSE)\n  \n  # Remove empty lines\n  text_data &lt;- text_data[text_data != \"\"]\n  \n  # Initialize list to store parsed data\n  parsed_data &lt;- list()\n  \n  # State abbreviations (for reference)\n  state_abbrevs &lt;- c(\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\", \n                     \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \n                     \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \n                     \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \n                     \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\", \"DC\")\n  \n  # Function to extract percentage and count from patterns like \"46% B (18,253)\"\n  extract_race_data &lt;- function(text, race_letter) {\n    pattern &lt;- paste0(\"([0-9.]+)%\\\\s*\", race_letter, \"\\\\s*\\\\(([0-9,]+)\\\\)\")\n    matches &lt;- str_match(text, pattern)\n    if (!is.na(matches[1])) {\n      pct &lt;- as.numeric(matches[2])\n      count &lt;- as.numeric(gsub(\",\", \"\", matches[3]))\n      return(list(pct = pct, count = count))\n    }\n    return(list(pct = NA, count = NA))\n  }\n  \n  # Function to extract just percentage (for demographics and collection rates)\n  extract_percentage &lt;- function(text, race_letter) {\n    pattern &lt;- paste0(\"([0-9.]+)%\\\\s*\", race_letter)\n    matches &lt;- str_match(text, pattern)\n    if (!is.na(matches[1])) {\n      return(as.numeric(matches[2]))\n    }\n    return(NA)\n  }\n  \n  # Process each line\n  i &lt;- 1\n  while (i &lt;= length(text_data)) {\n    line &lt;- text_data[i]\n    \n    # Check if line is a state abbreviation\n    if (line %in% state_abbrevs) {\n      state &lt;- line\n      \n      # Initialize data structure for this state\n      state_data &lt;- list(\n        State = state,\n        Black_DNA_Pct = NA, Black_DNA_N = NA,\n        Hispanic_DNA_Pct = NA, Hispanic_DNA_N = NA,\n        Asian_DNA_Pct = NA, Asian_DNA_N = NA,\n        Native_American_DNA_Pct = NA, Native_American_DNA_N = NA,\n        White_DNA_Pct = NA, White_DNA_N = NA,\n        Black_Pop_Pct = NA, Hispanic_Pop_Pct = NA, Asian_Pop_Pct = NA,\n        Native_American_Pop_Pct = NA, White_Pop_Pct = NA,\n        Black_Collection_Rate = NA, Hispanic_Collection_Rate = NA,\n        Asian_Collection_Rate = NA, Native_American_Collection_Rate = NA,\n        White_Collection_Rate = NA\n      )\n      \n      # Collect all lines for this state until we hit the next state or end of file\n      state_lines &lt;- c()\n      i &lt;- i + 1\n      while (i &lt;= length(text_data) && !(text_data[i] %in% state_abbrevs)) {\n        state_lines &lt;- c(state_lines, text_data[i])\n        i &lt;- i + 1\n      }\n      \n      # Combine all lines for this state into one text block\n      state_text &lt;- paste(state_lines, collapse = \" \")\n      \n      # Split into sections based on the pattern of % B occurrences\n      # We need to identify the three sections: DNA Collection, Demographics, Collection Rates\n      \n      # Find all \"% B\" patterns to help identify sections\n      b_patterns &lt;- str_locate_all(state_text, \"[0-9.]+%\\\\s*B\")[[1]]\n      \n      if (nrow(b_patterns) &gt;= 1) {\n        # First section: DNA Collection (has counts in parentheses)\n        dna_section_start &lt;- 1\n        dna_section_end &lt;- if(nrow(b_patterns) &gt;= 2) b_patterns[2,1] - 1 else nchar(state_text)\n        dna_section &lt;- substr(state_text, dna_section_start, dna_section_end)\n        \n        # Extract DNA collection data (with counts)\n        black_dna &lt;- extract_race_data(dna_section, \"B\")\n        hispanic_dna &lt;- extract_race_data(dna_section, \"H\")\n        asian_dna &lt;- extract_race_data(dna_section, \"A\")\n        native_dna &lt;- extract_race_data(dna_section, \"N\")\n        white_dna &lt;- extract_race_data(dna_section, \"W\")\n        \n        state_data$Black_DNA_Pct &lt;- black_dna$pct\n        state_data$Black_DNA_N &lt;- black_dna$count\n        state_data$Hispanic_DNA_Pct &lt;- hispanic_dna$pct\n        state_data$Hispanic_DNA_N &lt;- hispanic_dna$count\n        state_data$Asian_DNA_Pct &lt;- asian_dna$pct\n        state_data$Asian_DNA_N &lt;- asian_dna$count\n        state_data$Native_American_DNA_Pct &lt;- native_dna$pct\n        state_data$Native_American_DNA_N &lt;- native_dna$count\n        state_data$White_DNA_Pct &lt;- white_dna$pct\n        state_data$White_DNA_N &lt;- white_dna$count\n      }\n      \n      if (nrow(b_patterns) &gt;= 2) {\n        # Second section: Demographics (percentages only, no parentheses)\n        demo_section_start &lt;- b_patterns[2,1]\n        demo_section_end &lt;- if(nrow(b_patterns) &gt;= 3) b_patterns[3,1] - 1 else nchar(state_text)\n        demo_section &lt;- substr(state_text, demo_section_start, demo_section_end)\n        \n        # Extract demographic percentages\n        state_data$Black_Pop_Pct &lt;- extract_percentage(demo_section, \"B\")\n        state_data$Hispanic_Pop_Pct &lt;- extract_percentage(demo_section, \"H\")\n        state_data$Asian_Pop_Pct &lt;- extract_percentage(demo_section, \"A\")\n        state_data$Native_American_Pop_Pct &lt;- extract_percentage(demo_section, \"N\")\n        state_data$White_Pop_Pct &lt;- extract_percentage(demo_section, \"W\")\n      }\n      \n      if (nrow(b_patterns) &gt;= 3) {\n        # Third section: Collection Rates (percentages only, no parentheses)\n        rate_section_start &lt;- b_patterns[3,1]\n        rate_section &lt;- substr(state_text, rate_section_start, nchar(state_text))\n        \n        # Extract collection rate percentages\n        state_data$Black_Collection_Rate &lt;- extract_percentage(rate_section, \"B\")\n        state_data$Hispanic_Collection_Rate &lt;- extract_percentage(rate_section, \"H\")\n        state_data$Asian_Collection_Rate &lt;- extract_percentage(rate_section, \"A\")\n        state_data$Native_American_Collection_Rate &lt;- extract_percentage(rate_section, \"N\")\n        state_data$White_Collection_Rate &lt;- extract_percentage(rate_section, \"W\")\n      }\n      \n      # Add to parsed data\n      parsed_data[[length(parsed_data) + 1]] &lt;- state_data\n      \n      # Don't increment i here since we already did it in the while loop\n      i &lt;- i - 1\n    }\n    \n    i &lt;- i + 1\n  }\n  \n  # Convert to data frame\n  df &lt;- do.call(rbind, lapply(parsed_data, data.frame))\n  \n  return(df)\n}\n\nracial_data_path &lt;- file.path(here(\"data\", \"annual_dna_collection\", \"intermediate\", \"MurphyTong_Racial_Breakdown.txt\"))\n\nsummary_data &lt;- parse_dna_data(racial_data_path)\n\n# Display interactive table\ndatatable(summary_data, \n          options = list(pageLength = 10, scrollX = TRUE),\n          caption = \"Complete state-by-state breakdown of DNA collection by race\")\n\n\n\n\nTableÂ 1: Racial Breakdown of Annual DNA Collection for Each State\n\n\n\n\n\n\n\n\n\n\n\n\nDisparity Analysis\n\n\nShow disparity analysis code\n# Calculate disparity ratios\ndisparity_data &lt;- summary_data %&gt;%\n  filter(!is.na(Black_Collection_Rate) & !is.na(White_Collection_Rate)) %&gt;%\n  mutate(Black_White_Ratio = Black_Collection_Rate / White_Collection_Rate) %&gt;%\n  arrange(desc(Black_White_Ratio))\n\n# Create visualization\nggplot(disparity_data %&gt;% head(20), aes(x = reorder(State, Black_White_Ratio), y = Black_White_Ratio)) +\n  geom_bar(stat = \"identity\", fill = \"darkred\") +\n  coord_flip() +\n  labs(title = \"Top 20 States: Black-White DNA Collection Disparity Ratio\",\n       subtitle = \"Ratio of collection rates (higher = greater disparity)\",\n       x = \"State\",\n       y = \"Black/White Collection Rate Ratio\") +\n  theme_minimal() +\n  geom_hline(yintercept = 1, linetype = \"dashed\", color = \"gray50\")\n\n\n\n\n\n\n\n\nFigureÂ 1: DNA Collection Rates by Race Relative to Population Percentage"
  },
  {
    "objectID": "qmd_root/appendix_analysis.html#state-by-state-detailed-methodology",
    "href": "qmd_root/appendix_analysis.html#state-by-state-detailed-methodology",
    "title": "Annual DNA collection from Murphy & Tong Study",
    "section": "State-by-State Detailed Methodology",
    "text": "State-by-State Detailed Methodology\n\nMethodology: Parsing State Methodology Paragraphs\nThe detailed methodology for each state was extracted from a separate text file (MurphyTong_States_Paragraphs.txt) containing narrative descriptions of data collection approaches for all 50 states. This section explains how we systematically parsed this unstructured text into a structured dataset.\nProcessing Steps:\n\nState Detection: The parser identifies state entries by searching for the 50 U.S. state names as they appear in the text\nSection Extraction: For each state, the parser captures all text from the state name until the next state name appears\nComponent Parsing: Within each stateâ€™s section, the code extracts four key components:\n\nLegal Framework: The statutory basis for DNA collection in that state\nCollection Triggers: Specific offenses or events that trigger DNA collection\nData Sources: Types of data used (e.g., conviction records, prison admissions, arrest data)\nSource URLs: Web links to the original data sources\nData Limitations: Known gaps, proxies, or methodological caveats\n\nStructured Data Creation: Each data source line is parsed into a type-note pair (e.g., â€œPrison admissions: Used as proxy for conviction dataâ€)\nCategorization: The parser automatically categorizes:\n\nCollection trigger types: Comprehensive, selective, felony-only, etc.\nData limitation types: Missing conviction data, ethnicity issues, limited racial categories, etc.\nData source types: Conviction data, arrest data, prison data, sex crime data, etc.\n\n\nThis systematic extraction allows for consistent comparison across states and identification of common patterns in data collection methodologies and limitations.\n\n\nShow paragraph extraction code\n# Pre-define all 50 U.S. states\nus_states &lt;- c(\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \n               \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \n               \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \n               \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\", \n               \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \n               \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \n               \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \n               \"Wisconsin\", \"Wyoming\")\n\n# Read the text file\ndata_file &lt;- file.path(here(\"data\", \"annual_dna_collection\", \"intermediate\", \"MurphyTong_States_Paragraphs.txt\"))\n\ntext_content &lt;- readLines(data_file, warn = FALSE)\n\n# Combine all lines into a single string\nfull_text &lt;- paste(text_content, collapse = \"\\n\")\n\n# Create a pattern to match state names\nstate_pattern &lt;- paste0(\"(?m)^[[:space:]]*(\", paste(us_states, collapse = \"|\"), \")\\\\b\")\nstate_matches &lt;- str_locate_all(full_text, state_pattern)[[1]]\n\n# Extract state sections\nstate_sections &lt;- list()\nstate_names &lt;- character()\n\nif (nrow(state_matches) &gt; 0) {\n  for (i in 1:nrow(state_matches)) {\n    start_pos &lt;- state_matches[i, \"start\"]\n    if (i &lt; nrow(state_matches)) {\n      end_pos &lt;- state_matches[i + 1, \"start\"] - 1\n    } else {\n      end_pos &lt;- nchar(full_text)\n    }\n    \n    state_name &lt;- substr(full_text, start_pos, state_matches[i, \"end\"])\n    state_content &lt;- substr(full_text, start_pos, end_pos)\n    \n    state_names &lt;- c(state_names, state_name)\n    state_sections &lt;- c(state_sections, state_content)\n  }\n}\n\n# Function to extract information from each state section\nparse_state_section &lt;- function(section, state_name) {\n  if (is.na(state_name)) return(NULL)\n  \n  # Extract legal framework\n  legal_framework &lt;- str_extract(section, \"Legal Framework:[^\\n]+\") %&gt;%\n    {ifelse(is.na(.), NA, str_remove(., \"Legal Framework:\") %&gt;% str_trim())}\n  \n  # Extract collection triggers\n  collection_triggers &lt;- str_extract(section, \"Collection Triggers:[^\\n]+\") %&gt;%\n    {ifelse(is.na(.), NA, str_remove(., \"Collection Triggers:\") %&gt;% str_trim())}\n  \n  # Extract data sources - capture everything until Source URLs or Data Limitations\n  data_sources_text &lt;- str_extract(section, \"Data Sources:[\\\\s\\\\S]*?(?=Source URLs:|Data Limitations:|$)\")\n  data_source_df &lt;- tibble(data_source_type = NA_character_, data_source_note = NA_character_)\n  \n  if (!is.na(data_sources_text)) {\n    data_sources_text &lt;- str_remove(data_sources_text, \"Data Sources:\") %&gt;% str_trim()\n    # Split by newlines and clean up\n    data_source_lines &lt;- str_split(data_sources_text, \"\\\\n\")[[1]] %&gt;% \n      str_trim() %&gt;% \n      discard(~ .x == \"\" | str_detect(.x, \"^Source URLs:|^Data Limitations:\"))\n    \n    if (length(data_source_lines) &gt; 0) {\n      data_source_df &lt;- map_df(data_source_lines, function(line) {\n        if (str_detect(line, \":\")) {\n          tibble(\n            data_source_type = str_extract(line, \"^[^:]+\") %&gt;% str_trim(),\n            data_source_note = str_remove(line, \"^[^:]+:\") %&gt;% str_trim()\n          )\n        } else {\n          tibble(\n            data_source_type = line,\n            data_source_note = NA_character_\n          )\n        }\n      })\n    }\n  }\n  \n  # Extract source URLs\n  source_urls_text &lt;- str_extract(section, \"Source URLs:[\\\\s\\\\S]*?(?=Data Limitations:|$)\")\n  source_urls &lt;- character(0)\n  \n  if (!is.na(source_urls_text)) {\n    source_urls_text &lt;- str_remove(source_urls_text, \"Source URLs:\") %&gt;% str_trim()\n    source_url_lines &lt;- str_split(source_urls_text, \"\\\\n\")[[1]] %&gt;% \n      str_trim() %&gt;% \n      discard(~ .x == \"\" | str_detect(.x, \"^Data Limitations:\"))\n    \n    if (length(source_url_lines) &gt; 0) {\n      source_urls &lt;- source_url_lines\n    }\n  }\n  \n  # Extract data limitations - capture everything until next state or end\n  data_limitations_text &lt;- str_extract(section, \"Data Limitations:[\\\\s\\\\S]*?(?=\\\\b(A|Ala|Alas|Ari|Arka|Cali|Colo|Conn|Del|Flo|Geo|Haw|Ida|Ill|Ind|Iow|Kan|Ken|Lou|Mai|Mar|Mas|Mic|Min|Mis|Mon|Neb|Nev|New|Nor|Ohi|Okl|Ore|Pen|Rho|Sou|Ten|Tex|Uta|Ver|Vir|Was|Wis|Wyo)\\\\b|$)\")\n  data_limitations &lt;- NA_character_\n  \n  if (!is.na(data_limitations_text)) {\n    data_limitations_text &lt;- str_remove(data_limitations_text, \"Data Limitations:\") %&gt;% str_trim()\n    data_limitations_lines &lt;- str_split(data_limitations_text, \"\\\\n\")[[1]] %&gt;% \n      str_trim() %&gt;% \n      discard(~ .x == \"\" | str_detect(.x, \"^\\\\b(A|Ala|Alas|Ari|Arka|Cali|Colo|Conn|Del|Flo|Geo|Haw|Ida|Ill|Ind|Iow|Kan|Ken|Lou|Mai|Mar|Mas|Mic|Min|Mis|Mon|Neb|Nev|New|Nor|Ohi|Okl|Ore|Pen|Rho|Sou|Ten|Tex|Uta|Ver|Vir|Was|Wis|Wyo)\\\\b\"))\n    \n    if (length(data_limitations_lines) &gt; 0) {\n      data_limitations &lt;- paste(data_limitations_lines, collapse = \"; \")\n    }\n  }\n  \n  # If no data sources were found, create at least one row for the state\n  if (nrow(data_source_df) == 0) {\n    data_source_df &lt;- tibble(data_source_type = NA_character_, data_source_note = NA_character_)\n  }\n  \n  # Create result dataframe\n  result_df &lt;- data_source_df %&gt;%\n    mutate(\n      state = state_name,\n      legal_framework = legal_framework,\n      collection_triggers = collection_triggers,\n      source_url = ifelse(length(source_urls) &gt; 0, paste(source_urls, collapse = \"; \"), NA_character_),\n      data_limitations = data_limitations,\n      .before = everything()\n    )\n  \n  return(result_df)\n}\n\n# Parse all state sections\nstate_data_list &lt;- lapply(seq_along(state_sections), function(i) {\n  parse_state_section(state_sections[[i]], state_names[i])\n})\n\n# Combine all results into one dataframe\nstate_data &lt;- bind_rows(state_data_list)\n\n# Clean up the data - remove rows where all data columns are NA\nfinal_df &lt;- state_data %&gt;%\n  mutate(across(where(is.character), ~ ifelse(.x == \"\" | is.na(.x), NA, .x))) %&gt;%\n  filter(!(is.na(data_source_type) & is.na(source_url) & is.na(data_limitations)))\n\n# Fill in the missing legal_framework and other methodology for each state\nfinal_df_clean &lt;- final_df %&gt;%\n  group_by(state) %&gt;%\n  fill(legal_framework, collection_triggers, .direction = \"downup\") %&gt;%\n  ungroup()\n\n# Now let's fill source_url and data_limitations across all rows for each state\nfinal_df_clean &lt;- final_df_clean %&gt;%\n  group_by(state) %&gt;%\n  mutate(\n    source_url = ifelse(all(is.na(source_url)), NA, \n                       paste(na.omit(unique(source_url)), collapse = \"; \")),\n    data_limitations = ifelse(all(is.na(data_limitations)), NA,\n                             paste(na.omit(unique(data_limitations)), collapse = \"; \"))\n  ) %&gt;%\n  ungroup()\n\n# Create categorization columns for easier analysis\nfinal_df_clean &lt;- final_df_clean %&gt;%\n  mutate(\n    # Categorize collection triggers\n    collection_trigger_category = case_when(\n      str_detect(collection_triggers, \"(?i)all felony.*convictions.*arrests.*all felonies\") ~ \"Comprehensive: All felonies + broad arrests\",\n      str_detect(collection_triggers, \"(?i)all felony.*convictions.*arrests.*certain|specific\") ~ \"Selective: All felonies + specific arrests\",\n      str_detect(collection_triggers, \"(?i)all felony.*convictions.*arrests\") ~ \"Broad: All felonies + various arrests\",\n      str_detect(collection_triggers, \"(?i)all felony.*convictions\") ~ \"Felony convictions only\",\n      str_detect(collection_triggers, \"(?i)felony.*misdemeanor.*convictions\") ~ \"Mixed: Felony + misdemeanor convictions\",\n      TRUE ~ \"Other/Unspecified\"\n    ),\n    \n    # Categorize data limitations\n    data_limitation_category = case_when(\n      is.na(data_limitations) ~ \"No limitations noted\",\n      str_detect(data_limitations, \"(?i)no direct.*conviction data\") ~ \"Missing conviction data\",\n      str_detect(data_limitations, \"(?i)prison admissions.*proxy\") ~ \"Prison data as proxy\",\n      str_detect(data_limitations, \"(?i)hispanic|ethnicity\") ~ \"Ethnicity categorization issues\",\n      str_detect(data_limitations, \"(?i)racial.*limited|black.*white.*only\") ~ \"Limited racial categories\",\n      str_detect(data_limitations, \"(?i)no.*data.*available|unavailable\") ~ \"Various data unavailable\",\n      TRUE ~ \"Other limitations\"\n    ),\n    \n    # Categorize data source types\n    data_source_category = case_when(\n      str_detect(data_source_type, \"(?i)conviction\") ~ \"Conviction Data\",\n      str_detect(data_source_type, \"(?i)arrest\") ~ \"Arrest Data\",\n      str_detect(data_source_type, \"(?i)sex|sexual\") ~ \"Sex Crime Data\",\n      str_detect(data_source_type, \"(?i)prison|admission|correction\") ~ \"Prison/Incarceration Data\",\n      TRUE ~ \"Other Data Source\"\n    )\n  )\n\n# Create a summary table for quick overview - ONE ROW PER STATE\nmethodology_summary &lt;- final_df_clean %&gt;%\n  distinct(state, legal_framework, collection_triggers, collection_trigger_category, \n           data_limitations, data_limitation_category, source_url) %&gt;%\n  arrange(state) %&gt;%\n  mutate(state = str_trim(state))\n\n# Create interactive table\ndatatable_methodology &lt;- datatable(\n  methodology_summary,\n  extensions = c('Buttons', 'ColReorder', 'Scroller'),\n  options = list(\n    dom = 'Bfrtip',\n    buttons = c('copy', 'csv', 'excel', 'colvis'),\n    scrollX = TRUE,\n    scrollY = \"600px\",\n    scroller = TRUE,\n    pageLength = 10,\n    columnDefs = list(\n      list(className = 'dt-left', targets = 0:(ncol(methodology_summary)-1)),\n      list(width = '200px', targets = c(1, 2, 3))\n    ),\n    autoWidth = TRUE\n  ),\n  rownames = FALSE,\n  filter = 'top',\n  class = 'cell-border stripe hover'\n) %&gt;%\n  formatStyle(\n    columns = names(methodology_summary),\n    fontSize = '12px',\n    lineHeight = '90%'\n  )\n\n# Show the table\ndatatable_methodology"
  },
  {
    "objectID": "qmd_root/appendix_analysis.html#combining-information-into-master-dataset",
    "href": "qmd_root/appendix_analysis.html#combining-information-into-master-dataset",
    "title": "Annual DNA collection from Murphy & Tong Study",
    "section": "Combining Information into Master Dataset",
    "text": "Combining Information into Master Dataset\nThe final dataset combines quantitative DNA collection metrics with qualitative methodology to create a comprehensive one-row-per-state resource. This integration allows researchers to understand both the magnitude of DNA collection and the quality/limitations of the underlying data sources.\nIntegration Process:\n\nPrimary Dataset: The summary_data table contains all quantitative metrics:\n\nDNA collection counts and percentages by race\nState population demographics by race\nCollection rates per 100,000 population by race\n\nStudy Methodology Addition: The methodology_summary table provides contextual information:\n\nLegal framework for DNA collection\nSpecific collection triggers\nData source types and limitations\nSource URLs for verification\n\nJoining Strategy:\n\nStates are matched using full state names\nA crosswalk table converts two-letter abbreviations to full names\nLeft join ensures all states from the summary data are retained\n\nColumn Selection: The final dataset includes:\n\nState identifier: Full state name\nCollection metrics: All DNA collection counts, percentages, and rates by race\nMethodology context: Legal framework, collection triggers, data limitations\nQuality flags: Categorized data limitation types for filtering/analysis\n\nOutput Format: One row per state with 30+ columns covering demographics, collection rates, and methodology\n\nThis unified structure enables analyses that account for data quality differences across states when interpreting racial disparities in DNA collection.\n\n\nShow data combination and exportation code\n# Create state name crosswalk for joining\nstate_crosswalk &lt;- tibble(\n  State = c(\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\", \n            \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \n            \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \n            \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \n            \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\", \"DC\"),\n  state_full = c(\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \n                 \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \n                 \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \n                 \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \n                 \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \n                 \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \n                 \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \n                 \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \n                 \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \n                 \"Wisconsin\", \"Wyoming\", \"District of Columbia\")\n)\n\n# Join summary data with methodology\nannual_dna_combined &lt;- summary_data %&gt;%\n  left_join(state_crosswalk, by = \"State\") %&gt;%\n  left_join(methodology_summary, by = c(\"state_full\" = \"state\")) %&gt;%\n  select(\n    # State identifier\n    state = state_full,\n    state_abbrev = State,\n    \n    # DNA Collection metrics\n    Black_DNA_Pct, Black_DNA_N,\n    Hispanic_DNA_Pct, Hispanic_DNA_N,\n    Asian_DNA_Pct, Asian_DNA_N,\n    Native_American_DNA_Pct, Native_American_DNA_N,\n    White_DNA_Pct, White_DNA_N,\n    \n    # Population demographics\n    Black_Pop_Pct, Hispanic_Pop_Pct, Asian_Pop_Pct,\n    Native_American_Pop_Pct, White_Pop_Pct,\n    \n    # Collection rates\n    Black_Collection_Rate, Hispanic_Collection_Rate,\n    Asian_Collection_Rate, Native_American_Collection_Rate,\n    White_Collection_Rate,\n    \n    # Methodology\n    legal_framework,\n    collection_triggers,\n    collection_trigger_category,\n    data_limitations,\n    data_limitation_category,\n    source_url\n  )\n\n# Calculate total DNA profiles\nannual_dna_combined &lt;- annual_dna_combined %&gt;%\n  mutate(across(c(Black_DNA_N, Hispanic_DNA_N, Asian_DNA_N, Native_American_DNA_N, White_DNA_N), ~ replace_na(., 0)),\n  Total_DNA_Profiles = Black_DNA_N + Hispanic_DNA_N + Asian_DNA_N + \n                         Native_American_DNA_N + White_DNA_N\n  ) %&gt;%\n  relocate(Total_DNA_Profiles, .after = White_DNA_N)\n\n# Show interactive preview\ndatatable(\n  annual_dna_combined,\n  extensions = c('Buttons', 'ColReorder', 'Scroller'),\n  options = list(\n    dom = 'Bfrtip',\n    buttons = c('copy', 'csv', 'excel', 'colvis'),\n    scrollX = TRUE,\n    scrollY = \"400px\",\n    scroller = TRUE,\n    pageLength = 10,\n    columnDefs = list(\n      list(className = 'dt-left', targets = 0:(ncol(annual_dna_combined)-1))\n    ),\n    autoWidth = TRUE\n  ),\n  rownames = FALSE,\n  filter = 'top',\n  class = 'cell-border stripe hover',\n  caption = \"Complete Annual DNA Collection Dataset - One Row Per State\"\n) %&gt;%\n  formatStyle(\n    columns = names(annual_dna_combined),\n    fontSize = '11px',\n    lineHeight = '90%'\n  )"
  },
  {
    "objectID": "qmd_root/appendix_analysis.html#data-export-and-versioning",
    "href": "qmd_root/appendix_analysis.html#data-export-and-versioning",
    "title": "Annual DNA collection from Murphy & Tong Study",
    "section": "Data Export and Versioning",
    "text": "Data Export and Versioning\n\n\nShow export code\n# Create output directory structure\nintermediate_dir &lt;- here(\"data\", \"annual_dna_collection\", \"intermediate\")\ndir.create(intermediate_dir, recursive = TRUE, showWarnings = FALSE)\n\nfinal_dir &lt;- here(\"data\", \"annual_dna_collection\", \"final\")\ndir.create(final_dir, recursive = TRUE, showWarnings = FALSE)\n\nfrozen_dir &lt;- here(\"data\", \"v1.0\")\ndir.create(frozen_dir, recursive = TRUE, showWarnings = FALSE)\n\n# Export final combined dataset\noutput_path &lt;- file.path(final_dir, \"Annual_DNA_Collection.csv\")\nwrite_csv(annual_dna_combined, output_path)\ncat(paste(\"âœ“ Exported Annual DNA Collection dataset to:\", output_path, \"\\n\"))\n\n# Create frozen version (v1.0) for long-term reference\nfrozen_path &lt;- file.path(frozen_dir, \"Annual_DNA_Collection.csv\")\nwrite_csv(annual_dna_combined, frozen_path)\ncat(paste(\"âœ“ Created frozen version 1.0 at:\", frozen_path, \"\\n\\n\"))\n\n# Also export the methodology summary separately for reference\nmethodology_output_path &lt;- file.path(intermediate_dir, \"State_Methodology.csv\")\nwrite_csv(methodology_summary, methodology_output_path)\ncat(paste(\"âœ“ Exported study methodology summary to:\", methodology_output_path, \"\\n\"))\n\n\nâœ“ Exported Annual DNA Collection dataset to: C:/Users/Donadio/Documents/PODFRIDGE_Databases/data/annual_dna_collection/final/Annual_DNA_Collection.csv \nâœ“ Created frozen version 1.0 at: C:/Users/Donadio/Documents/PODFRIDGE_Databases/data/v1.0/Annual_DNA_Collection.csv \n\nâœ“ Exported study methodology summary to: C:/Users/Donadio/Documents/PODFRIDGE_Databases/data/annual_dna_collection/intermediate/State_Methodology.csv"
  },
  {
    "objectID": "qmd_root/appendix_analysis.html#summary-and-key-findings",
    "href": "qmd_root/appendix_analysis.html#summary-and-key-findings",
    "title": "Annual DNA collection from Murphy & Tong Study",
    "section": "Summary and Key Findings",
    "text": "Summary and Key Findings\n\nDataset Completeness\n\n\nShow completeness analysis\n# Analyze data completeness by category\ncompleteness_summary &lt;- annual_dna_combined %&gt;%\n  summarise(\n    States_with_Black_Data = sum(!is.na(Black_Collection_Rate)),\n    States_with_Hispanic_Data = sum(!is.na(Hispanic_Collection_Rate)),\n    States_with_Asian_Data = sum(!is.na(Asian_Collection_Rate)),\n    States_with_Native_American_Data = sum(!is.na(Native_American_Collection_Rate)),\n    States_with_White_Data = sum(!is.na(White_Collection_Rate)),\n    States_with_Legal_Framework = sum(!is.na(legal_framework))\n  )\n\nkable(t(completeness_summary), col.names = \"Count\", \n      caption = \"Data Completeness Across States\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nData Completeness Across States\n\n\n\nCount\n\n\n\n\nStates_with_Black_Data\n50\n\n\nStates_with_Hispanic_Data\n39\n\n\nStates_with_Asian_Data\n26\n\n\nStates_with_Native_American_Data\n34\n\n\nStates_with_White_Data\n50\n\n\nStates_with_Legal_Framework\n50"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "PODFRIDGE - U.S. Forensic DNA Database",
    "section": "",
    "text": "This repository contains data collection, processing, and analysis code for a comprehensive study of U.S. forensic DNA databases spanning 2001-2025. The project reconstructs the historical growth of the National DNA Index System (NDIS), compiles current state-level DNA database policies and statistics (SDIS), and standardizes demographic data from Freedom of Information Act (FOIA) requests.\n\n\nLasisi, T., Donadio, J.P., Muller, M., Wilson, J., Mooney, J., & Edge, M.D. (2025). United States forensic DNA databases: national time series (2001â€“2025) and state cross-sections.\n\n\n\n\nTina Lasisi\nJ. P. Donadio\nS. C. Muller\nJ. Wilson\nJ. Mooney\nM. D. Edge\n\nCorresponding author: tlasisi@umich.edu\nDataset DOI: [To be added]\n\n\n\nPODFRIDGE_Databases/\nâ”œâ”€â”€ _freeze/ # Quarto cache for rendered outputs\nâ”œâ”€â”€ data/ # Datasets used for NDIS analyses\nâ”‚ â”œâ”€â”€ ndis/ # NDIS related files (raw, intermediate and final)\nâ”‚ â”œâ”€â”€ sdis/ # SDIS related files (raw, intermediate and final)\nâ”‚ â”œâ”€â”€ foia/ # FOIA related files (raw, intermediate and final)\nâ”‚ â”œâ”€â”€ annual_dna_collection/ # Annual DNA Collection related files (raw, intermediate and final)\nâ”‚ â”œâ”€â”€ v1.0/ # Frozen csv files\nâ”‚ â””â”€â”€ README.md\nâ”œâ”€â”€ docs/ # Rendered website (GitHub Pages output)\nâ”œâ”€â”€ figures/ # Figures generated by analyses\nâ”œâ”€â”€ qmd_root/ # Quarto notebooks and analysis scripts\nâ”‚ â”œâ”€â”€ ndis_scraping.qmd # NDIS data scraping notebook\nâ”‚ â”œâ”€â”€ ndis_analysis.qmd # NDIS technical validation & figures\nâ”‚ â”œâ”€â”€ sdis_summary.qmd # SDIS compilation & analysis\nâ”‚ â”œâ”€â”€ foia_processing.qmd # FOIA data processing\nâ”‚ â”œâ”€â”€ foia_processing_pdf.qmd # FOIA data processing in PDF\nâ”‚ â”œâ”€â”€ appendix_analysis.qmd # Murphy & Tong appendix for Annual DNA Collection documentation\nâ”‚ â””â”€â”€ appendix_analysis.qmd # Murphy & Tong appendix for Annual DNA Collection documentation in PDF\nâ”œâ”€â”€ .gitattributes # Git file attributes\nâ”œâ”€â”€ .gitignore # Ignored files and folders\nâ”œâ”€â”€ LICENSE # Repository license\nâ”œâ”€â”€ README.md # Project overview and usage\nâ”œâ”€â”€ _quarto.yml # Quarto website configuration\nâ”œâ”€â”€ about.qmd # About page\nâ”œâ”€â”€ index.qmd # Main landing page\nâ”œâ”€â”€ requirements.txt # Python/R dependencies\nâ””â”€â”€ styles.css # Custom website styling\n\n\n\nAll final datasets are archived and publicly available on Zenodo:\nZenodo Repository: [DOI to be added upon publication]\nThe repository includes: - NDIS_time_series.csv - Monthly NDIS statistics (2001-2025) - SDIS_cross_section.csv - State-level profiles and policies (2025) - FOIA_Demographics.csv - Demographic composition from FOIA responses - Annual_DNA_Collection.csv - Annual collection rates (Murphy & Tong 2020) - Raw HTML files, intermediate processing outputs, and complete documentation\nFor detailed data dictionaries and usage notes, see data/README.md.\n\n\n\nCode: MIT License Data: CC BY 4.0 (pending Zenodo publication; FOIA-derived data subject to original authorsâ€™ permissions)"
  },
  {
    "objectID": "about.html#project-overview",
    "href": "about.html#project-overview",
    "title": "About PODFRIDGE-Databases",
    "section": "",
    "text": "PODFRIDGE-Databases is an open-source research initiative focused on forensic DNA database systems in the United States. Our mission is to increase transparency and accessibility of DNA database information through systematic data collection, analysis, and public documentation."
  },
  {
    "objectID": "about.html#research-objectives",
    "href": "about.html#research-objectives",
    "title": "About PODFRIDGE-Databases",
    "section": "Research Objectives",
    "text": "Research Objectives\n\nDocument Historical Growth: Track the expansion of the National DNA Index System (NDIS) from 2001-2025 using archived data from the FBI website\nAnalyze State Variations: Examine differences in state DNA database policies, practices, and legal frameworks\nInvestigate Demographic Patterns: Process and standardize demographic data from FOIA responses to study racial disparities\nEnsure Reproducibility: Create transparent, documented pipelines for forensic data research"
  },
  {
    "objectID": "about.html#data-sources",
    "href": "about.html#data-sources",
    "title": "About PODFRIDGE-Databases",
    "section": "Data Sources",
    "text": "Data Sources\n\nPrimary Data Collections\n\nWayback Machine Archives: Historical snapshots of FBI NDIS statistics (2001-2025)\nFOIA Responses: Demographic data from 7 states (CA, FL, IN, ME, NV, SD, TX)\nState Statutes: Legal frameworks governing DNA collection and database management\nGovernment Reports: Official publications and statistical reports\n\n\n\nMethodology\nOur approach emphasizes:\n\nTransparency: All data processing steps are documented and reproducible\nQuality Control: Multiple validation checks for data integrity\nStandardization: Consistent formatting across heterogeneous data sources\nEthical Considerations: Responsible handling of sensitive criminal justice data"
  },
  {
    "objectID": "about.html#project-structure",
    "href": "about.html#project-structure",
    "title": "About PODFRIDGE-Databases",
    "section": "Project Structure",
    "text": "Project Structure\nPODFRIDGE-Databases/\nâ”œâ”€â”€ data/ # Frozen datasets\nâ”œâ”€â”€ raw/ # Raw data files\nâ”œâ”€â”€ output/ # Analysis outputs and visualizations\nâ”œâ”€â”€ analysis/ # Processing pipelines and analysis code\nâ”œâ”€â”€ archive/ # Documentation and research papers\nâ””â”€â”€ _site/ # Quarto website files"
  },
  {
    "objectID": "about.html#team",
    "href": "about.html#team",
    "title": "About PODFRIDGE-Databases",
    "section": "Team",
    "text": "Team\nTina Lasisi - Principal Investigator\nAssistant Professor at the University of Michigan Anthropology\nJoÃ£o P. Donadio - Editor & Technical Advisor\nData Scientist & Research Collaborator"
  },
  {
    "objectID": "about.html#acknowledgments",
    "href": "about.html#acknowledgments",
    "title": "PODFRIDGE - U.S. Forensic DNA Database",
    "section": "",
    "text": "We thank Erin Murphy for providing access to state-level demographic disclosures and FOIA response materials. We acknowledge the Internet Archiveâ€™s Wayback Machine for preserving historical web content that made this reconstruction possible."
  },
  {
    "objectID": "about.html#citation",
    "href": "about.html#citation",
    "title": "PODFRIDGE - U.S. Forensic DNA Database",
    "section": "",
    "text": "If you use these data or code, please cite both the dataset and the associated paper:\n@article{lasisi2025dna,\n  title={United States forensic DNA databases: national time series (2001â€“2025) and state cross-sections},\n  author={Lasisi, Temi and Donadio, J. P. and Muller, M. and Wilson, J. and Mooney, J. and Edge, Michael D.},\n  journal={-},\n  year={2025},\n  note={In press}\n}\n\n@dataset{lasisi2025dna_data,\n  author={Lasisi, Temi and Donadio, J. P. and Muller, M. and Wilson, J. and Mooney, J. and Edge, Michael D.},\n  title={United States forensic DNA databases: NDIS, SDIS, and FOIA datasets},\n  year={2025},\n  publisher={Zenodo},\n  doi={[to be added]}\n}"
  },
  {
    "objectID": "about.html#license",
    "href": "about.html#license",
    "title": "PODFRIDGE - U.S. Forensic DNA Database",
    "section": "",
    "text": "Code: MIT License Data: CC BY 4.0 (pending Zenodo publication; FOIA-derived data subject to original authorsâ€™ permissions)"
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About PODFRIDGE-Databases",
    "section": "Contact",
    "text": "Contact\nFor questions about this research or to collaborate:\n\nEmail:\nGitHub: https://github.com/tinalasisi/PODFRIDGE-Databases"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PODFRIDGE-Databases",
    "section": "",
    "text": "PODFRIDGE-Databases is a comprehensive research platform analyzing forensic DNA database systems in the United States. This project combines multiple data sources to examine the growth, composition, and demographic patterns within national and state DNA index systems."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "PODFRIDGE-Databases",
    "section": "",
    "text": "PODFRIDGE-Databases is a comprehensive research platform analyzing forensic DNA database systems in the United States. This project combines multiple data sources to examine the growth, composition, and demographic patterns within national and state DNA index systems."
  },
  {
    "objectID": "index.html#research-projects",
    "href": "index.html#research-projects",
    "title": "PODFRIDGE-Databases",
    "section": "Research Projects",
    "text": "Research Projects\n\n1. NDIS Database Analysis\nTracking FBIâ€™s National DNA Index System (2001-2025)\n\nObjective: Reconstruct NDIS growth through Wayback Machine archives\nKey Findings: Exponential profile growth, jurisdictional participation patterns\nMethods: Web scraping, data validation, time-series analysis\n\nView Scraping Methodology â†’\nView Full Analysis â†’\n\n\n2. SDIS Summary Analysis\nState DNA Database Policies and Practices\n\nObjective: Examine variation in state DNA database policies\nKey Findings: Arrestee collection policies, familial search allowances\nMethods: Legal statute analysis, policy documentation\n\nView Full Analysis â†’\n\n\n3. FOIA Document Processing\nRacial Composition of State DNA Databases\n\nObjective: Process FOIA responses on demographic disparities\nKey Findings: Standardized data from 7 states, transparency framework\nMethods: OCR processing, data standardization, quality control\n\nView Full Analysis â†’\n\n\n4. Annual DNA Collection\nMurphy & Tong Study Appendix\n\nObjective: Document data sources and methodology for racial disparity research\nKey Findings: Consistent disparities, data limitations, methodological challenges\nMethods: Data provenance tracking, methodology documentation\n\nView Full Analysis â†’"
  },
  {
    "objectID": "qmd_root/ndis_scraping.html",
    "href": "qmd_root/ndis_scraping.html",
    "title": "NDIS Database",
    "section": "",
    "text": "The National DNA Index System (NDIS) is the central database that allows accredited forensic laboratories across the United States to electronically exchange and compare DNA profiles. Maintained by the FBI as part of CODIS (Combined DNA Index System), NDIS tracks the accumulation of DNA records contributed by federal, state, and local laboratories.\nThis project focuses on systematically compiling the growth and evolution of NDIS by parsing historical statistics published on the FBIâ€™s website and preserved in the Internet Archiveâ€™s Wayback Machine. These snapshots contain tables reporting the number of DNA profiles stored in NDIS (offender, arrestee, forensic), as well as information on laboratory participation across jurisdictions.\n\n\n\nDevelop a reproducible pipeline to extract NDIS statistics from archived FBI webpages in the Wayback Machine.\nIdentify and correct inconsistencies and data quality issues across historical snapshots.\nDocument the expansion of DNA profiles (offender, arrestee, forensic) over time."
  },
  {
    "objectID": "qmd_root/ndis_scraping.html#introduction",
    "href": "qmd_root/ndis_scraping.html#introduction",
    "title": "NDIS Database",
    "section": "",
    "text": "The National DNA Index System (NDIS) is the central database that allows accredited forensic laboratories across the United States to electronically exchange and compare DNA profiles. Maintained by the FBI as part of CODIS (Combined DNA Index System), NDIS tracks the accumulation of DNA records contributed by federal, state, and local laboratories.\nThis project focuses on systematically compiling the growth and evolution of NDIS by parsing historical statistics published on the FBIâ€™s website and preserved in the Internet Archiveâ€™s Wayback Machine. These snapshots contain tables reporting the number of DNA profiles stored in NDIS (offender, arrestee, forensic), as well as information on laboratory participation across jurisdictions.\n\n\n\nDevelop a reproducible pipeline to extract NDIS statistics from archived FBI webpages in the Wayback Machine.\nIdentify and correct inconsistencies and data quality issues across historical snapshots.\nDocument the expansion of DNA profiles (offender, arrestee, forensic) over time."
  },
  {
    "objectID": "qmd_root/ndis_scraping.html#setup-configuration",
    "href": "qmd_root/ndis_scraping.html#setup-configuration",
    "title": "NDIS Database",
    "section": "Setup and Configuration",
    "text": "Setup and Configuration\n\nSystem Requirements\nRequired Packages:\n\nCore: requests, beautifulsoup4, and lxml (scraping/parsing).\nData/Visualization: pandas and tqdm (progress tracking).\n\n\n\nShow Configuration code\nimport sys\nimport subprocess\nimport importlib\n\nrequired_packages = [\n    'requests',         # API/HTTP\n    'beautifulsoup4',   # HTML parsing\n    'lxml',             # Faster parsing\n    'pandas',           # Data handling\n    'tqdm',              # Progress bars\n    'hashlib',\n    'collections',\n    'pathlib',\n    'datetime',\n    'os'\n]\n\nfor package in required_packages:\n    try:\n        importlib.import_module(package)\n        print(f\"âœ“ {package} already installed\")\n    except ImportError:\n        print(f\"Installing {package}...\")\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n\nprint(f\"ðŸ“š All required packages are installed.\")\n\n\n\n\nProject Structure\nMain Configurations:\n\nDirectory paths for raw HTML (data_snapshots), metadata (data_metadata), and outputs (data_outputs).\nStandardization mappings for jurisdiction names and known data typos.\n\n\n\nShow Configuration code\nfrom pathlib import Path\nimport re, json, requests, time, hashlib\nfrom datetime import datetime\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nfrom tqdm.auto import tqdm\nfrom datetime import datetime\nfrom collections import defaultdict\nimport os\n\n# Configuration\nBASE_DIR = Path(\"..\")  # Project root directory\nHTML_DIR = BASE_DIR / \"data\" / \"ndis\" / \"raw\" / \"ndis_snapshots\"    # Storage for downloaded HTML\nMETA_DIR = BASE_DIR / \"data\" / \"ndis\" / \"raw\" / \"ndis_metadata\"    # Metadata storage\nOUTPUT_DIR = BASE_DIR / \"data\" / \"ndis\" / \"raw\" / \"ndis_outputs\"       # Processed data output\n\nNDIS_SNAPSHOTS_DIR = HTML_DIR\nNDIS_SNAPSHOTS_DIR.mkdir(parents=True, exist_ok=True)\n\n# Create directory structure\nfor directory in [HTML_DIR, META_DIR, OUTPUT_DIR]:\n    directory.mkdir(parents=True, exist_ok=True)\n\n\n\n\nProject directories initialized:\n  - Working directory: C:\\Users\\Donadio\\Documents\\PODFRIDGE_Databases\n  - HTML storage: ..\\data\\ndis\\raw\\ndis_snapshots\n  - Metadata directory: ..\\data\\ndis\\raw\\ndis_metadata\n  - Output directory: ..\\data\\ndis\\raw\\ndis_outputs"
  },
  {
    "objectID": "qmd_root/ndis_scraping.html#WMSS",
    "href": "qmd_root/ndis_scraping.html#WMSS",
    "title": "NDIS Database",
    "section": "Wayback Machine Snapshot Search",
    "text": "Wayback Machine Snapshot Search\nA function was developed to systematically search the Internet Archiveâ€™s Wayback Machine for all preserved snapshots of FBI NDIS statistics pages using a comprehensive multi-phase approach.\n\nScraping Method\n\nMulti-Phase Search Strategy:\n\n\nFirst searches for snapshots from the pre-2007 era using state-specific URLs\nThen targets consolidated pages from post-2007 periods\nHandles both HTTP and HTTPS protocol variants\n\n\nRobust Error Handling:\n\n\nAutomatic retries with exponential backoff (1s â†’ 2s â†’ 4s delays)\nDeduplicates results by timestamp\nFailed requests are tracked and retried with increased retry attempts\nPreserves complete error context for troubleshooting\n\n\n\nTechnical Implementation\n\nAPI Request\nConverts JSON responses to clean DataFrame\nSorts chronologically (oldest â†’ newest)\n\n\n\nCore Search Implementation\n\nmake_request_with_retry(): Implements exponential backoff (1s â†’ 2s â†’ 4s delays) for fault-tolerant API requests with configurable retry attempts.\n\n\n\nShow search helpers function code\ndef make_request_with_retry(params, max_retries=3, initial_delay=1):\n    API_URL = \"https://web.archive.org/cdx/search/cdx\"\n    delay = initial_delay\n    for attempt in range(max_retries):\n        try:\n            resp = requests.get(API_URL, params=params, timeout=30)\n            resp.raise_for_status()\n            print(f\"âœ“ Successful request for {params['url']} (offset: {params.get('offset', 0)})\")\n            return resp\n        except requests.exceptions.RequestException as e:\n            if attempt == max_retries - 1:\n                print(f\"âœ— Final attempt failed for {params['url']} (offset: {params.get('offset', 0)}): {str(e)}\")\n                return None\n            print(f\"! Attempt {attempt+1} failed for {params['url']}, retrying in {delay} seconds...\")\n            time.sleep(delay)\n            delay *= 2\n\n\n\nEvolution of the NDIS Webpage (2001â€“2025)\nOver the years, the NDIS statistics webpage has evolved considerably in both structure and design. Early versions (2001â€“2007) displayed state-level statistics through individual HTML pages (e.g., ne.htm for Nebraska), accessible via a clickable U.S. map interface. From 2008 onward, the FBI consolidated all states and agencies into unified pages, simplifying data retrieval and standardizing presentation formats. This evolution reflects ongoing modernization of the FBIâ€™s online data systems, transitioning from fragmented, state-specific sources to more cohesive and accessible datasets.\n\n\n\nWebsite Formats\n\n\n\n\nPre-2007 (Clickmap Era)\n\nEach state/agency has its own page (e.g., ne.htm for Nebraska, dc.htm for DC/FBI Lab).\nThe search iterates through the known two-letter codes and queries Wayback for each URL individually.\nsearch_pre2007_snapshots(): Searches individual state-specific pages using 50+ state codes (al.htm, ak.htm, etc.).\n\n\n\nShow pre-2007 search function code\ndef search_pre2007_snapshots():\n    state_codes = [\"al\", \"ak\", \"az\", \"ar\", \"ca\", \"co\", \"ct\", \"de\", \"dc\",\n                   \"fl\", \"ga\", \"hi\", \"id\", \"il\", \"in\", \"ia\", \"ks\", \"ky\",\n                   \"la\", \"me\", \"md\", \"ma\", \"mi\", \"mn\", \"ms\", \"mo\", \"mt\",\n                   \"ne\", \"nv\", \"nh\", \"nj\", \"nm\", \"ny\", \"nc\", \"nd\", \"oh\",\n                   \"ok\", \"or\", \"pa\", \"pr\", \"ri\", \"sc\", \"sd\", \"tn\", \"tx\",\n                   \"army\", \"ut\", \"vt\", \"va\", \"wa\", \"wv\", \"wi\", \"wy\"]\n    all_rows = []\n    seen_timestamps = set()\n    total_saved = 0\n    \n    print(f\"Starting pre-2007 snapshot search for {len(state_codes)} state codes at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    \n    for i, code in enumerate(state_codes, 1):\n        url = f\"http://www.fbi.gov/hq/lab/codis/{code}.htm\"\n        offset = 0\n        state_snapshots = 0\n        has_more_results = True\n        \n        print(f\"\\n[{i}/{len(state_codes)}] Searching for {code.upper()} snapshots...\")\n        \n        while has_more_results:\n            params = {\n                \"url\": url,\n                \"matchType\": \"exact\",\n                \"output\": \"json\",\n                \"fl\": \"timestamp,original,mimetype,statuscode\",\n                \"filter\": [\"statuscode:200\", \"mimetype:text/html\"],\n                \"limit\": \"5000\",\n                \"offset\": str(offset)\n            }\n            resp = make_request_with_retry(params, max_retries=5, initial_delay=2)\n            if not resp: \n                print(f\"âœ— Failed to retrieve {code.upper()}\")\n                break\n                \n            data = resp.json()\n            \n            if len(data) &lt;= 1:\n                print(f\"  No more results for {code.upper()} at offset {offset}\")\n                has_more_results = False\n                break\n                \n            new_snapshots = 0\n            for row in data[1:]:\n                timestamp = row[0]\n                if timestamp not in seen_timestamps:\n                    all_rows.append(row)\n                    seen_timestamps.add(timestamp)\n                    new_snapshots += 1\n                    state_snapshots += 1\n                    total_saved += 1\n            \n            if new_snapshots &gt; 0:\n                print(f\"  â†’ Saved {new_snapshots} new snapshots for {code.upper()} (offset: {offset})\")\n            \n            # Check if we've reached the end of results\n            if len(data) &lt; 5001:\n                print(f\"  Reached end of results for {code.upper()} at offset {offset}\")\n                has_more_results = False\n            else:\n                offset += 5000\n                time.sleep(1)  # Brief pause between requests\n        \n        if state_snapshots &gt; 0:\n            print(f\"âœ“ Found {state_snapshots} total snapshots for {code.upper()}\")\n        else:\n            print(f\"âœ— No snapshots found for {code.upper()}\")\n    \n    print(f\"\\nPre-2007 search completed. Total snapshots saved: {total_saved}\")\n    return pd.DataFrame(all_rows, columns=[\"timestamp\",\"original\",\"mimetype\",\"status\"]), 0\n\n\n\n\nPost-2007 (Consolidated Pages)\n\nAll state/agency records are on a single page per snapshot.\nSearches use the known consolidated URL patterns per era.\nsearch_post2007_snapshots(): Searches consolidated pages across 5 historical URL patterns with both HTTP and HTTPS variants.\n\n\n\nShow search function code\ndef search_post2007_snapshots():\n    urls = [\n        \"https://www.fbi.gov/hq/lab/codis/stats.htm\",\n        \"https://www.fbi.gov/about-us/lab/codis/ndis-statistics\",\n        \"https://www.fbi.gov/about-us/lab/biometric-analysis/codis/ndis-statistics\",\n        \"https://www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics\",\n        \"https://le.fbi.gov/science-and-lab/biometrics-and-fingerprints/codis/codis-ndis-statistics\"\n    ]\n    \n    all_rows = []\n    seen_timestamps = set()\n    protocols = [\"http://\", \"https://\"]\n    total_saved = 0\n    \n    print(f\"Starting post-2007 snapshot search for {len(urls)} URLs at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    \n    for i, base_url in enumerate(urls, 1):\n        url_snapshots = 0\n        \n        for protocol in protocols:\n            current_url = base_url.replace(\"https://\",\"\").replace(\"http://\",\"\")\n            full_url = f\"{protocol}{current_url}\"\n            offset = 0\n            has_more_results = True\n            \n            print(f\"\\n[{i}/{len(urls)}] Searching for {full_url} snapshots...\")\n            \n            while has_more_results:\n                params = {\n                    \"url\": full_url,\n                    \"matchType\": \"exact\",\n                    \"output\": \"json\",\n                    \"fl\": \"timestamp,original,mimetype,statuscode\",\n                    \"filter\": [\"statuscode:200\", \"mimetype:text/html\"],\n                    \"limit\": \"5000\",\n                    \"offset\": str(offset)\n                }\n                resp = make_request_with_retry(params, max_retries=5, initial_delay=2)\n                if not resp: \n                    print(f\"âœ— Failed to retrieve {full_url}\")\n                    break\n                    \n                data = resp.json()\n                \n                if len(data) &lt;= 1:\n                    print(f\"  No more results for {full_url} at offset {offset}\")\n                    has_more_results = False\n                    break\n                    \n                new_snapshots = 0\n                for row in data[1:]:\n                    timestamp = row[0]\n                    if timestamp not in seen_timestamps:\n                        all_rows.append(row)\n                        seen_timestamps.add(timestamp)\n                        new_snapshots += 1\n                        url_snapshots += 1\n                        total_saved += 1\n                \n                if new_snapshots &gt; 0:\n                    print(f\"  â†’ Saved {new_snapshots} new snapshots for {full_url} (offset: {offset})\")\n                \n                if len(data) &lt; 5001:\n                    print(f\"  Reached end of results for {full_url} at offset {offset}\")\n                    has_more_results = False\n                else:\n                    offset += 5000\n                    time.sleep(1)\n        \n        if url_snapshots &gt; 0:\n            print(f\"âœ“ Found {url_snapshots} total snapshots for {base_url}\")\n        else:\n            print(f\"âœ— No snapshots found for {base_url}\")\n    \n    print(f\"\\nPost-2007 search completed. Total snapshots saved: {total_saved}\")\n    return pd.DataFrame(all_rows, columns=[\"timestamp\",\"original\",\"mimetype\",\"status\"]), 0\n\n\n\n\n\nSearch Execution\n\nCalls both pre-2007 and post-2007 search functions to retrieve comprehensive NDIS records.\nCombines results and removes duplicates based on timestamps.\nStores technical details (timestamps, URL variants, duplicate counts) in structured JSON metadata.\nProvides detailed logging and progress tracking throughout the search process.\n\n\n\nShow search code\npre2007_df, _ = search_pre2007_snapshots()\npost2007_df, _ = search_post2007_snapshots()\n\n# Combine all snapshots\nsnap_df = pd.concat([pre2007_df, post2007_df]).drop_duplicates(\"timestamp\").sort_values(\"timestamp\").reset_index(drop=True)\n\n# Save search metadata\nsearch_meta = {\n    \"search_performed\": datetime.now().isoformat(),\n    \"total_snapshots\": len(snap_df),\n    \"time_span\": {\n        \"first\": snap_df[\"timestamp\"].min(),\n        \"last\": snap_df[\"timestamp\"].max()\n    },\n    \"url_variants\": snap_df[\"original\"].nunique()\n}\n\nwith open(META_DIR / \"search_metadata.json\", \"w\") as f:\n    json.dump(search_meta, f, indent=2)\n\n\n\n\n\nSearch Results Summary\n=====================\nLoaded 10310 unique snapshots\nTime coverage:\n20010715040342\nto\n20250815002050\nUnique URL patterns found: 255\nOutput saved to: C:\\Users\\Donadio\\Documents\\PODFRIDGE_Databases\\data\\ndis\\raw\\ndis_metadata/search_metadata.json"
  },
  {
    "objectID": "qmd_root/ndis_scraping.html#downloaderSnap",
    "href": "qmd_root/ndis_scraping.html#downloaderSnap",
    "title": "NDIS Database",
    "section": "Snapshot Downloader",
    "text": "Snapshot Downloader\nThis system provides a robust method for downloading historical webpage snapshots from the Internet Archiveâ€™s Wayback Machine, specifically designed for the FBI NDIS statistics pages.\n\nDownload Methods\nThe download system implements a sequential approach optimized for reliability and respectful API usage:\n\nResilient Downloading: Automatic retries with exponential backoff (2s â†’ 4s â†’ 8s â†’ 16s â†’ 32s delays) and extended 60-second timeouts for reliable network handling\nSmart File Management: Context-aware naming scheme using timestamp + state/scope identifier (e.g., 20040312_ne.html for pre-2007, 20150621_ndis.html for post-2007)\nDuplicate Prevention: Automatically skips already downloaded files to prevent redundant operations\nProgress Tracking: Real-time download status with completion counters and detailed success/failure reporting\nRate Limiting: 1.5-second delays between requests to avoid overloading the Wayback Machine servers\n\n\n\nShow downloader helpers code\ndef download_with_retry(url, save_path, max_retries=4, initial_delay=2):\n    \"\"\"\n    Download an archived snapshot with retries and exponential backoff.\n    \"\"\"\n    delay = initial_delay\n    for attempt in range(max_retries):\n        try:\n            resp = requests.get(url, timeout=120)\n            resp.raise_for_status()\n            \n            # Validate content is reasonable\n            if len(resp.content) &lt; 500:\n                if b\"error\" in resp.content.lower() or b\"not found\" in resp.content.lower():\n                    raise requests.exceptions.RequestException(f\"Possible error page: {len(resp.content)} bytes\")\n            \n            # Save to disk\n            with open(save_path, \"wb\") as f:\n                f.write(resp.content)\n            print(f\"âœ“ Downloaded: {save_path}\")\n            return True\n        except requests.exceptions.RequestException as e:\n            if attempt == max_retries - 1:\n                print(f\"âœ— Final attempt failed for {url}: {str(e)}\")\n                return False\n            print(f\"! Attempt {attempt+1} failed, retrying in {delay}s...\")\n            time.sleep(delay)\n            delay *= 2\n    return False\n\n\ndef snapshot_to_filepath(row):\n    \"\"\"\n    Map a snapshot record to a local filename.\n    Format: {timestamp}_{state_or_scope}.html\n    \"\"\"\n    ts = row[\"timestamp\"]\n    original = row[\"original\"]\n    \n    # derive name from FBI URL pattern\n    if \"/codis/\" in original and original.endswith(\".htm\"):\n        # Pre-2007: use last part (state code or army/dc)\n        suffix = Path(original).stem\n    else:\n        # Post-2007: consolidated page, use 'ndis'\n        suffix = \"ndis\"\n    \n    save_dir = HTML_DIR\n    save_dir.mkdir(parents=True, exist_ok=True)  # Ensure directory exists\n    \n    return save_dir / f\"{ts}_{suffix}.html\"\n\n\n\n\nDownload Execution\nThe download execution phase performs bulk retrieval of historical NDIS snapshots with comprehensive error handling:\n\nSequential Processing: Iterates through snapshot DataFrame chronologically, processing each file individually for maximum reliability\nURL Construction: Uses identity flag (id_) in archive URLs to retrieve unmodified original content: https://web.archive.org/web/{timestamp}id_/{original}\nBinary Preservation: Saves files as binary content to maintain original encoding and prevent character corruption\nComprehensive Logging: Provides real-time progress updates with attempt counters and final success/failure statistics\nFlexible Limiting: Optional download limits for testing or partial processing\n\n\n\nShow download execution code\ndef download_snapshots(snap_df, limit=None):\n    \"\"\"\n    Iterate over snapshot DataFrame and download archived HTML pages.\n    \"\"\"\n    total = len(snap_df) if limit is None else min(limit, len(snap_df))\n    print(f\"Starting download of {total} snapshots at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    \n    successes, failures = 0, 0\n    failed_downloads = []\n    \n    for i, row in enumerate(snap_df.head(total).itertuples(index=False), 1):\n        ts, original, _, _ = row\n        save_path = snapshot_to_filepath(row._asdict())\n        \n        if save_path.exists() and save_path.stat().st_size &gt; 1000:\n            print(f\"- [{i}/{total}] Already exists: {save_path}\")\n            successes += 1\n            continue\n        \n        archive_url = f\"https://web.archive.org/web/{ts}id_/{original}\"\n        print(f\"- [{i}/{total}] Downloading {archive_url}\")\n        \n        if download_with_retry(archive_url, save_path):\n            successes += 1\n        else:\n            failures += 1\n            failed_downloads.append({\n                'timestamp': ts,\n                'url': original,\n                'archive_url': archive_url\n            })\n        \n        if i % 50 == 0:\n            print(f\"Progress: {i}/{total} ({i/total*100:.1f}%) - {successes} success, {failures} failures\")\n        \n        time.sleep(1.5)\n    \n    # Save failure report\n    if failed_downloads:\n        failure_report = {\n            'download_completed': datetime.now().isoformat(),\n            'total_attempted': total,\n            'successful': successes,\n            'failed': failures,\n            'failed_downloads': failed_downloads\n        }\n        with open(META_DIR / \"download_failures.json\", 'w') as f:\n            json.dump(failure_report, f, indent=2)\n    \n    print(f\"\\nDownload completed. Success: {successes}, Failures: {failures}\")\n    return successes, failures\n\n\nsuccesses, failures = download_snapshots(snap_df, limit=None)\n\n\n\n\nDownload Validation\nPost-download validation ensures data integrity and identifies potential issues:\n\nFile Existence Verification: Checks that all expected files were successfully downloaded to the target directory\nContent Quality Assessment: Validates HTML content by examining file headers for proper HTML tags\nError Categorization: Separates missing files from corrupted/non-HTML files for targeted remediation\nMetadata Generation: Creates JSON validation reports with detailed statistics and file counts\nActionable Reporting: Provides clear feedback on download success rates and files requiring attention\n\n\n\nShow download validation code\ndef validate_downloads(snap_df):\n    \"\"\"\n    Validate downloaded files exist and are HTML-like.\n    \"\"\"\n    missing, bad_html = [], []\n    \n    for row in snap_df.itertuples(index=False):\n        save_path = snapshot_to_filepath(row._asdict())\n        if not save_path.exists():\n            missing.append(save_path)\n            continue\n        try:\n            file_size = save_path.stat().st_size\n            if file_size &lt; 1000:\n                bad_html.append(save_path)\n                continue\n                \n            with open(save_path, \"rb\") as f:\n                start = f.read(500).lower()\n            if b\"&lt;html\" not in start and b\"&lt;!doctype\" not in start:\n                bad_html.append(save_path)\n        except Exception:\n            bad_html.append(save_path)\n    \n    print(f\"Validation results â†’ Missing: {len(missing)}, Bad HTML: {len(bad_html)}\")\n    return missing, bad_html\n\n# Run validation\nmissing, bad_html = validate_downloads(snap_df)\n\n# Save validation summary\nvalidation_meta = {\n    \"validation_performed\": datetime.now().isoformat(),\n    \"missing_files\": len(missing),\n    \"bad_html_files\": len(bad_html),\n    \"total_snapshots\": len(snap_df),\n    \"successful_downloads\": successes,\n    \"success_rate\": f\"{(successes/len(snap_df))*100:.1f}%\"\n}\nwith open(META_DIR / \"validation_metadata.json\", \"w\") as f:\n    json.dump(validation_meta, f, indent=2)\n\n\n\n\nShow validation report code\n# Save and print report\nreport_path = META_DIR / f\"validation_metadata.json\"\nwith open(report_path, 'w') as f:\n    json.dump(validation_meta, f, indent=2)\n\nprint(f\"\\n{'='*60}\")\nprint(\"DOWNLOAD VALIDATION REPORT\")\nprint(f\"{'='*60}\")\nprint(f\"  Missing files: {validation_meta['missing_files']}/{validation_meta['total_snapshots']}\")\nprint(f\"  Bad HTML files: {validation_meta['bad_html_files']}/{validation_meta['total_snapshots']}\")\nprint(f\"  Successful downloads: {validation_meta['successful_downloads']}/{validation_meta['total_snapshots']} ({validation_meta['success_rate']})\")\n\nprint(f\"\\nFull report: {report_path}\")\n\n\n\n============================================================\nDOWNLOAD VALIDATION REPORT\n============================================================\n  Missing files: 47/10310\n  Bad HTML files: 4/10310\n  Successful downloads: 10263/10310 (99.5%)\n\nFull report: ..\\data\\ndis\\raw\\ndis_metadata\\validation_metadata.json"
  },
  {
    "objectID": "qmd_root/ndis_scraping.html#extraction-pipe",
    "href": "qmd_root/ndis_scraping.html#extraction-pipe",
    "title": "NDIS Database",
    "section": "Data Extraction",
    "text": "Data Extraction\nThe data extraction pipeline converts downloaded HTML snapshots into structured tabular data, handling the evolution of FBI NDIS reporting formats across different time periods.\n\nExtraction Overview\nThe extraction system processes three distinct eras of NDIS reporting:\n\nPre-2007 Era: Basic statistics without date metadata or arrestee data\n2007-2011 Era: Includes â€œas ofâ€ dates but no arrestee profiles\nPost-2012 Era: Complete format with all profile types and consistent dating\n\nKey Features:\n\nEra-Aware Processing: Automatically routes files to appropriate parsers based on timestamp\nMetadata Recovery: Extracts report dates from â€œas ofâ€ statements when available\nComplete Traceability: Links each record to its source HTML file and original URL\nRobust Error Handling: Processes files individually to prevent single failures from stopping the entire batch\n\n\n\nCore Parser Functions\nEssential text processing utilities for NDIS data extraction:\n\nHTML Cleaning: Removes navigation, scripts, and styling elements to focus on data content\nDate Extraction: Identifies and parses â€œas ofâ€ dates using multiple pattern variations\nText Normalization: Standardizes whitespace and jurisdiction name formatting\nEncoding Handling: Manages various character encodings found in historical snapshots\n\n\n\nShow setup and normalization functions code\ndef extract_ndis_metadata(html_content):\n    \"\"\"\n    Extract key metadata from NDIS HTML content including report dates\n    \n    Returns:\n    --------\n    dict:\n        - report_month: Month from \"as of\" statement (None if not found)\n        - report_year: Year from \"as of\" statement (None if not found)  \n        - clean_text: Normalized text content\n    \"\"\"\n    \n    # Multiple patterns to catch different \"as of\" formats\n    date_patterns = [\n        r'[Aa]s of ([A-Za-z]+) (\\d{4})',           # \"as of November 2008\"\n        r'[Aa]s of ([A-Za-z]+) (\\d{1,2}), (\\d{4})', # \"as of November 15, 2008\"\n        r'Statistics as of ([A-Za-z]+) (\\d{4})',    # \"Statistics as of November 2008\"\n        r'Statistics as of ([A-Za-z]+) (\\d{1,2}), (\\d{4})' # \"Statistics as of November 15, 2008\"\n    ]\n    \n    report_month = None\n    report_year = None\n    \n    # Find first occurrence of any date pattern\n    for pattern in date_patterns:\n        date_match = re.search(pattern, html_content)\n        if date_match:\n            month_str = date_match.group(1)\n            if len(date_match.groups()) == 2:  # Month + Year only\n                year_str = date_match.group(2)\n            else:  # Month + Day + Year\n                year_str = date_match.group(3)\n            \n            # Convert month name to number\n            try:\n                month_num = pd.to_datetime(f\"{month_str} 1, 2000\").month\n                report_month = month_num\n                report_year = int(year_str)\n                break\n            except:\n                continue\n    \n    # Clean HTML and normalize text\n    soup = BeautifulSoup(html_content, 'lxml')\n    \n    # Remove scripts, styles, and navigation elements\n    for element in soup(['script', 'style', 'nav', 'header', 'footer']):\n        element.decompose()\n    \n    # Get clean text with normalized whitespace\n    clean_text = re.sub(r'\\s+', ' ', soup.get_text(' ', strip=True))\n    \n    return {\n        'report_month': report_month,\n        'report_year': report_year, \n        'clean_text': clean_text\n    }\n\ndef standardize_jurisdiction_name(name):\n    \"\"\"\n    Clean and standardize jurisdiction names for consistency\n    \"\"\"\n    if not name:\n        return name\n        \n    # Remove common prefixes and suffixes\n    name = re.sub(r'^.*?(Back to top|Tables by NDIS Participant|ation\\.)\\s*', \n                 '', name, flags=re.I).strip()\n    \n    # Standardize known variants\n    replacements = {\n        'D.C./FBI Lab': 'DC/FBI Lab',\n        'D.C./Metro PD': 'DC/Metro PD', \n        'US Army': 'U.S. Army',\n        'D.C.': 'DC'\n    }\n    \n    for old, new in replacements.items():\n        name = name.replace(old, new)\n    \n    return name.strip()\n\ndef extract_original_url_from_filename(html_file):\n    \"\"\"\n    Reconstruct original URL from filename and timestamp\n    \"\"\"\n    filename = html_file.name\n    timestamp = filename.split('_')[0]  # Get timestamp part\n    \n    # Determine URL pattern based on filename suffix\n    if filename.endswith('_ndis.html'):\n        # Post-2007 consolidated format - use most common URL pattern\n        return \"https://www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics\"\n    else:\n        # Pre-2007 state-specific format\n        state_code = filename.split('_')[1].replace('.html', '')\n        return f\"http://www.fbi.gov/hq/lab/codis/{state_code}.htm\"\n\n\n\n\nEra-Specific Parsers\nTime-period-adapted parsing logic that accounts for format evolution:\nPre-2007 Parser:\n\nExtracts basic statistics from state-specific pages\nUses timestamp-derived year (no report dates available)\nSets arrestee counts to 0 (not reported in this era)\nHandles missing NDIS labs and investigations data\n\n2008-2011 Parser:\n\nProcesses consolidated pages with â€œBack to topâ€ section dividers\nExtracts month and year from report dates\nHandles missing arrestee data (sets to 0)\nMultiple pattern matching for jurisdiction identification\n\n2012-2016 Parser:\n\nFirst era with arrestee data extraction\nProcesses consolidated pages with â€œBack to topâ€ section dividers\nMultiple pattern matching for jurisdiction identification\nComplete jurisdiction coverage with standardized names\n\nPost-2017 Parser:\n\nModern format with consistent structure and all fields\nRobust regex pattern for reliable extraction\nFull feature extraction including arrestee profiles\nComplete jurisdiction coverage with standardized names\n\n\n\nShow parser functions code\ndef parse_pre2007_ndis(text, timestamp, html_file, report_month=None, report_year=None):\n    \"\"\"\n    Parse NDIS snapshots from 2001-2007 era (state-specific pages)\n    HTML has table structure with state name followed by data rows\n    \"\"\"\n    records = []\n    \n    # Clean up the text for better matching\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Try multiple patterns to extract state name\n    jurisdiction = None\n    \n    # Pattern 1: State name in large font (from graphic alt text or heading)\n    # Looking for patterns like \"Graphic of Pennsylvania Pennsylvania\" or just the state name\n    state_pattern1 = r'(?:Graphic of|alt=\")([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*?)(?:\"|&gt;|\\s+Offender)'\n    state_match = re.search(state_pattern1, text, re.IGNORECASE)\n    \n    if state_match:\n        jurisdiction = state_match.group(1).strip()\n    \n    # Pattern 2: Try to extract from filename as fallback\n    if not jurisdiction:\n        # Extract state code from filename (e.g., \"20010715040342_pa.html\")\n        filename = html_file.name\n        state_code_match = re.search(r'_([a-z]{2,5})\\.html$', filename)\n        if state_code_match:\n            state_code = state_code_match.group(1)\n            # Map common state codes to names\n            state_map = {\n                'pa': 'Pennsylvania', 'nc': 'North Carolina', 'ct': 'Connecticut',\n                'wv': 'West Virginia', 'ks': 'Kansas', 'nd': 'North Dakota',\n                'wy': 'Wyoming', 'ky': 'Kentucky', 'la': 'Louisiana',\n                'dc': 'DC/FBI Lab', 'de': 'Delaware', 'ne': 'Nebraska',\n                'sc': 'South Carolina', 'tn': 'Tennessee', 'ma': 'Massachusetts',\n                'fl': 'Florida', 'nh': 'New Hampshire', 'sd': 'South Dakota',\n                'me': 'Maine', 'hi': 'Hawaii', 'nm': 'New Mexico',\n                'al': 'Alabama', 'tx': 'Texas', 'mi': 'Michigan',\n                'ut': 'Utah', 'ar': 'Arkansas', 'az': 'Arizona',\n                'mo': 'Missouri', 'ny': 'New York', 'mn': 'Minnesota',\n                'vt': 'Vermont', 'id': 'Idaho', 'oh': 'Ohio',\n                'ok': 'Oklahoma', 'or': 'Oregon', 'ca': 'California',\n                'il': 'Illinois', 'wi': 'Wisconsin', 'ms': 'Mississippi',\n                'wa': 'Washington', 'mt': 'Montana', 'in': 'Indiana',\n                'co': 'Colorado', 'va': 'Virginia', 'ga': 'Georgia',\n                'ak': 'Alaska', 'md': 'Maryland', 'nj': 'New Jersey',\n                'nv': 'Nevada', 'ri': 'Rhode Island', 'ia': 'Iowa',\n                'army': 'U.S. Army'\n            }\n            jurisdiction = state_map.get(state_code, state_code.upper())\n    \n    if jurisdiction:\n        jurisdiction = standardize_jurisdiction_name(jurisdiction)\n        \n        # Extract individual values with more flexible patterns\n        # These patterns work with the table structure in your example\n        offender_match = re.search(r'Offender\\s+Profiles?\\s+([\\d,]+)', text, re.IGNORECASE)\n        forensic_match = re.search(r'Forensic\\s+(?:Samples?|Profiles?)\\s+([\\d,]+)', text, re.IGNORECASE)\n        \n        # NDIS labs can appear as \"NDIS Participating Labs\" or just \"Number of CODIS Labs\"\n        ndis_labs_match = re.search(r'(?:NDIS\\s+Participating\\s+Labs?|Number\\s+of\\s+CODIS\\s+Labs?)\\s+(\\d+)', text, re.IGNORECASE)\n        \n        investigations_match = re.search(r'Investigations?\\s+Aided\\s+([\\d,]+)', text, re.IGNORECASE)\n        \n        if offender_match and forensic_match:\n            records.append({\n                'timestamp': timestamp,\n                'report_month': None,  # Not available pre-2007\n                'report_year': None,   # Not available pre-2007\n                'jurisdiction': jurisdiction,\n                'offender_profiles': int(offender_match.group(1).replace(',', '')),\n                'arrestee': 0,  # Not reported pre-2007\n                'forensic_profiles': int(forensic_match.group(1).replace(',', '')),\n                'ndis_labs': int(ndis_labs_match.group(1)) if ndis_labs_match else 0,\n                'investigations_aided': int(investigations_match.group(1).replace(',', '')) if investigations_match else 0\n            })\n    \n    return records\n\ndef parse_2008_2011_ndis(text, timestamp, html_file, report_month=None, report_year=None):\n    \"\"\"\n    Parse NDIS snapshots from 2008-2011 era\n    Consolidated page with state anchors and \"Back to top\" links\n    No arrestee data in this period\n    \"\"\"\n    records = []\n    \n    # Clean up the text\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Split by \"Back to top\" to isolate each state section\n    sections = re.split(r'Back\\s+to\\s+top', text, flags=re.IGNORECASE)\n    \n    for section in sections:\n        # Look for state name pattern (appears as anchor or bold text)\n        # Try multiple patterns to catch different HTML formats\n        jurisdiction = None\n        \n        # Pattern 1: &lt;a name=\"State\"&gt;&lt;/a&gt;&lt;strong&gt;State&lt;/strong&gt;\n        state_match = re.search(r'&lt;a\\s+name=\"([^\"]+)\"[^&gt;]*&gt;.*?(?:&lt;strong&gt;|&lt;b&gt;)\\s*([A-Z][^&lt;]+?)(?:&lt;/strong&gt;|&lt;/b&gt;)', section, re.IGNORECASE)\n        if state_match:\n            jurisdiction = state_match.group(2).strip()\n        \n        # Pattern 2: Just the state name in bold/strong tags before \"Statistical Information\"\n        if not jurisdiction:\n            state_match = re.search(r'(?:&lt;strong&gt;|&lt;b&gt;)\\s*([A-Z][^&lt;]+?)(?:&lt;/strong&gt;|&lt;/b&gt;).*?Statistical\\s+Information', section, re.IGNORECASE)\n            if state_match:\n                jurisdiction = state_match.group(1).strip()\n        \n        # Pattern 3: State name without tags before \"Statistical Information\"\n        if not jurisdiction:\n            state_match = re.search(r'([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\s+Statistical\\s+Information', section)\n            if state_match:\n                jurisdiction = state_match.group(1).strip()\n        \n        if jurisdiction:\n            jurisdiction = standardize_jurisdiction_name(jurisdiction)\n            \n            # Extract values\n            offender_match = re.search(r'Offender\\s+Profiles?\\s+([\\d,]+)', section, re.IGNORECASE)\n            forensic_match = re.search(r'Forensic\\s+(?:Samples?|Profiles?)\\s+([\\d,]+)', section, re.IGNORECASE)\n            ndis_labs_match = re.search(r'NDIS\\s+Participating\\s+Labs?\\s+(\\d+)', section, re.IGNORECASE)\n            investigations_match = re.search(r'Investigations?\\s+Aided\\s+([\\d,]+)', section, re.IGNORECASE)\n            \n            if offender_match and forensic_match:\n                records.append({\n                    'timestamp': timestamp,\n                    'report_month': report_month,\n                    'report_year': report_year,\n                    'jurisdiction': jurisdiction,\n                    'offender_profiles': int(offender_match.group(1).replace(',', '')),\n                    'arrestee': 0,  # Not reported 2008-2011\n                    'forensic_profiles': int(forensic_match.group(1).replace(',', '')),\n                    'ndis_labs': int(ndis_labs_match.group(1)) if ndis_labs_match else 0,\n                    'investigations_aided': int(investigations_match.group(1).replace(',', '')) if investigations_match else 0\n                })\n    \n    return records\n\n\ndef parse_2012_2016_ndis(text, timestamp, html_file, report_month=None, report_year=None):\n    \"\"\"\n    Parse NDIS snapshots from 2012-2016 era\n    Includes arrestee data for the first time\n    \"\"\"\n    records = []\n    \n    # Clean up the text\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Split by \"Back to top\" to isolate each state section\n    sections = re.split(r'Back\\s+to\\s+top', text, flags=re.IGNORECASE)\n    \n    for section in sections:\n        jurisdiction = None\n        \n        # Look for state name patterns\n        # Pattern 1: &lt;a name=\"State\"&gt;&lt;/a&gt;&lt;b&gt;State&lt;/b&gt;\n        state_match = re.search(r'&lt;a\\s+name=\"([^\"]+)\"[^&gt;]*&gt;.*?&lt;b&gt;([^&lt;]+?)&lt;/b&gt;', section, re.IGNORECASE)\n        if state_match:\n            jurisdiction = state_match.group(2).strip()\n        \n        # Pattern 2: Just bold state name\n        if not jurisdiction:\n            state_match = re.search(r'&lt;b&gt;([A-Z][^&lt;]+?)&lt;/b&gt;.*?Statistical\\s+Information', section, re.IGNORECASE)\n            if state_match:\n                jurisdiction = state_match.group(1).strip()\n        \n        # Pattern 3: State name without tags\n        if not jurisdiction:\n            state_match = re.search(r'([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\s+Statistical\\s+Information', section)\n            if state_match:\n                jurisdiction = state_match.group(1).strip()\n        \n        if jurisdiction:\n            jurisdiction = standardize_jurisdiction_name(jurisdiction)\n            \n            # Extract values INCLUDING arrestee which appears starting 2012\n            offender_match = re.search(r'Offender\\s+Profiles?\\s+([\\d,]+)', section, re.IGNORECASE)\n            arrestee_match = re.search(r'Arrestee\\s+([\\d,]+)', section, re.IGNORECASE)\n            forensic_match = re.search(r'Forensic\\s+Profiles?\\s+([\\d,]+)', section, re.IGNORECASE)\n            ndis_labs_match = re.search(r'NDIS\\s+Participating\\s+Labs?\\s+(\\d+)', section, re.IGNORECASE)\n            investigations_match = re.search(r'Investigations?\\s+Aided\\s+([\\d,]+)', section, re.IGNORECASE)\n            \n            if offender_match and forensic_match:\n                records.append({\n                    'timestamp': timestamp,\n                    'report_month': report_month,\n                    'report_year': report_year,\n                    'jurisdiction': jurisdiction,\n                    'offender_profiles': int(offender_match.group(1).replace(',', '')),\n                    'arrestee': int(arrestee_match.group(1).replace(',', '')) if arrestee_match else 0,\n                    'forensic_profiles': int(forensic_match.group(1).replace(',', '')),\n                    'ndis_labs': int(ndis_labs_match.group(1)) if ndis_labs_match else 0,\n                    'investigations_aided': int(investigations_match.group(1).replace(',', '')) if investigations_match else 0\n                })\n    \n    return records\n\n\ndef parse_post2017_ndis(text, timestamp, html_file, report_month=None, report_year=None):\n    \"\"\"\n    Parse NDIS snapshots from 2017+ era\n    Modern format with consistent structure and all fields\n    Keep using your existing working pattern for this era\n    \"\"\"\n    records = []\n    \n    # This is your existing working pattern - don't change it\n    pattern = re.compile(\n        r'([A-Z][\\w\\s\\.\\-\\'\\/&\\(\\)]+?)Statistical Information'\n        r'.*?Offender Profiles\\s+([\\d,]+)'\n        r'.*?Arrestee\\s+([\\d,]+)'\n        r'.*?Forensic Profiles\\s+([\\d,]+)'\n        r'.*?NDIS Participating Labs\\s+(\\d+)'\n        r'.*?Investigations Aided\\s+([\\d,]+)',\n        re.IGNORECASE | re.DOTALL\n    )\n    \n    for match in pattern.finditer(text):\n        records.append({\n            'timestamp': timestamp,\n            'report_month': report_month,\n            'report_year': report_year,\n            'jurisdiction': standardize_jurisdiction_name(match.group(1)),\n            'offender_profiles': int(match.group(2).replace(',', '')),\n            'arrestee': int(match.group(3).replace(',', '')),\n            'forensic_profiles': int(match.group(4).replace(',', '')),\n            'ndis_labs': int(match.group(5)),\n            'investigations_aided': int(match.group(6).replace(',', ''))\n        })\n    \n    return records\n\n\n\n\nOutput Schema\nEach extracted record contains the following standardized fields:\n\n\n\n\n\n\n\n\nField\nDescription\nAvailability\n\n\n\n\ntimestamp\nWayback capture timestamp (YYYYMMDDHHMMSS)\nAll eras\n\n\nreport_month\nMonth from â€œas ofâ€ statement\n2007+ only\n\n\nreport_year\nYear from â€œas ofâ€ statement\n2007+ only\n\n\njurisdiction\nStandardized state/agency name\nAll eras\n\n\noffender_profiles\nDNA profiles from convicted offenders\nAll eras\n\n\narrestee\nDNA profiles from arrestees\n2012+ only\n\n\nforensic_profiles\nCrime scene DNA profiles\nAll eras\n\n\nndis_labs\nNumber of participating laboratories\nAll eras\n\n\ninvestigations_aided\nCases assisted by DNA matches\nAll eras\n\n\n\n\n\nBatch Processing\nThe complete extraction workflow:\nFile Discovery\n\nScans download directory for HTML files\nSorts chronologically for consistent processing\nTracks progress with detailed logging\n\nIndividual File Processing\n\nReads HTML content with encoding fallback\nExtracts metadata and cleans content\nRoutes to era-appropriate parser based on timestamp\nCaptures source file information for traceability\n\nData Consolidation\n\nCombines all records into single DataFrame\nAdds derived timestamp columns (capture_date, year)\nValidates data integrity and completeness\nSorts by capture date and jurisdiction for consistency\n\n\n\nShow batch processing function code\ndef process_ndis_snapshot(html_file):\n    \"\"\"\n    Convert single NDIS HTML file to structured data\n    Routes to appropriate parser based on timestamp year\n    \"\"\"\n    try:\n        # Read HTML content with multiple encoding attempts\n        content = None\n        for encoding in ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']:\n            try:\n                content = html_file.read_text(encoding=encoding)\n                break\n            except UnicodeDecodeError:\n                continue\n        \n        if content is None:\n            # Final fallback with error ignoring\n            content = html_file.read_text(encoding='latin-1', errors='ignore')\n        \n        # Check if file has reasonable content\n        if len(content.strip()) &lt; 100:\n            print(f\"âš   Small file detected: {html_file.name} ({len(content)} chars)\")\n            return []\n        \n        # Extract metadata\n        metadata = extract_ndis_metadata(content)\n        timestamp = html_file.stem.split('_')[0]\n        \n        try:\n            year = int(timestamp[:4])\n        except ValueError:\n            print(f\"âš   Invalid timestamp in filename: {html_file.name}\")\n            return []\n        \n        # Route to appropriate parser with fallback\n        records = []\n        parser_used = None\n        \n        if year &lt;= 2007:\n            records = parse_pre2007_ndis(metadata['clean_text'], timestamp, html_file,\n                                       metadata['report_month'], metadata['report_year'])\n            parser_used = \"pre2007\"\n        elif year &lt;= 2011:\n            records = parse_2008_2011_ndis(metadata['clean_text'], timestamp, html_file,\n                                         metadata['report_month'], metadata['report_year'])\n            parser_used = \"2008-2011\"\n        elif year &lt;= 2016:\n            records = parse_2012_2016_ndis(metadata['clean_text'], timestamp, html_file,\n                                         metadata['report_month'], metadata['report_year'])\n            parser_used = \"2012-2016\"\n        else:\n            records = parse_post2017_ndis(metadata['clean_text'], timestamp, html_file,\n                                        metadata['report_month'], metadata['report_year'])\n            parser_used = \"post2017\"\n        \n        if records:\n            # Add parser info for debugging\n            for record in records:\n                record['parser_used'] = parser_used\n                record['source_file'] = html_file.name\n            return records\n        else:\n            print(f\"âš   No records extracted from {html_file.name} (year: {year}, parser: {parser_used})\")\n            return []\n            \n    except Exception as e:\n        print(f\"âŒ Error processing {html_file.name}: {str(e)}\")\n        return []\n\n\ndef process_all_snapshots():\n    \"\"\"\n    Process all downloaded snapshots into a single DataFrame\n    \n    Returns:\n    --------\n    pd.DataFrame\n        Combined dataset with all snapshots\n    \"\"\"\n    all_records = []\n    html_files = sorted(NDIS_SNAPSHOTS_DIR.glob(\"*.html\"))\n    \n    print(f\"Processing {len(html_files)} HTML snapshots...\")\n    \n    successful_files = 0\n    failed_files = 0\n    failed_list = []\n    \n    for html_file in tqdm(html_files, desc=\"Extracting NDIS data\"):\n        records = process_ndis_snapshot(html_file)\n        if records:\n            all_records.extend(records)\n            successful_files += 1\n        else:\n            failed_files += 1\n            failed_list.append(html_file.name)\n    \n    print(f\"Processing complete: {successful_files} successful, {failed_files} failed\")\n    \n    # Save failure report\n    if failed_list:\n        failure_report_path = OUTPUT_DIR / \"processing_failures.json\"\n        with open(failure_report_path, 'w') as f:\n            json.dump({\n                'timestamp': datetime.now().isoformat(),\n                'total_files': len(html_files),\n                'successful': successful_files,\n                'failed': failed_files,\n                'failed_files': failed_list[:100]  # Limit to first 100\n            }, f, indent=2)\n        print(f\"Failure report saved: {failure_report_path}\")\n    \n    if not all_records:\n        print(\"Warning: No records extracted!\")\n        return pd.DataFrame()\n    \n    df = pd.DataFrame(all_records)\n    \n    # Add derived date columns\n    df['capture_date'] = pd.to_datetime(df['timestamp'], format='%Y%m%d%H%M%S', errors='coerce')\n    df['capture_year'] = df['capture_date'].dt.year\n    \n    # Remove any records with invalid dates\n    invalid_dates = df['capture_date'].isna().sum()\n    if invalid_dates &gt; 0:\n        print(f\"âš   Removed {invalid_dates} records with invalid dates\")\n        df = df[df['capture_date'].notna()]\n    \n    return df.sort_values(['capture_date', 'jurisdiction']).reset_index(drop=True)\n\n\n\n\nExport & Validation\nStructured output generation with comprehensive quality control:\nValidation Checks:\n\nVerifies all required columns are present\nChecks for null values in critical fields\nValidates numeric ranges (non-negative counts)\nConfirms timestamp format consistency\nEnsures jurisdiction names contain valid characters\n\nExport Features\n\nSaves as UTF-8 encoded CSV for maximum compatibility\nGenerates timestamped filenames for version control\nCreates metadata summary with file statistics\nPerforms round-trip validation to confirm data integrity\n\nQuality Metrics\n\nRecords total file count and processing success rate\nTracks temporal coverage (earliest to latest snapshots)\nDocuments jurisdiction coverage across time periods\nReports data completeness by era and field\n\n\n\nShow execution function code\ndef export_ndis_data(df, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Export processed NDIS data with comprehensive metadata\n    \"\"\"\n    if df.empty:\n        print(\"Warning: DataFrame is empty, skipping export\")\n        return None\n    \n    # Generate output filename\n    export_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    csv_path = output_dir / f\"ndis_data_raw.csv\"\n    \n    # Export main dataset\n    df.to_csv(csv_path, index=False, encoding='utf-8')\n    \n    # Calculate export metadata\n    file_size_mb = csv_path.stat().st_size / (1024 * 1024)\n    \n    export_metadata = {\n        'export_timestamp': export_timestamp,\n        'export_path': str(csv_path.resolve()),\n        'record_count': len(df),\n        'file_size_mb': round(file_size_mb, 2),\n        'unique_snapshots': df['timestamp'].nunique(),\n        'unique_jurisdictions': df['jurisdiction'].nunique(),\n        'date_coverage': {\n            'earliest_capture': df['capture_date'].min().isoformat(),\n            'latest_capture': df['capture_date'].max().isoformat(),\n            'span_years': df['capture_year'].max() - df['capture_year'].min() + 1\n        },\n        'data_completeness': {\n            'with_report_dates': len(df[df['report_year'].notna()]),\n            'with_arrestee_data': len(df[df['arrestee'] &gt; 0]),\n            'total_investigations_aided': int(df['investigations_aided'].sum())\n        }\n    }\n    \n    # Save metadata\n    metadata_path = output_dir / f\"ndis_export_metadata_{export_timestamp}.json\"\n    import json\n    with open(metadata_path, 'w') as f:\n        json.dump(export_metadata, f, indent=2, default=str)\n    \n    print(f\"âœ“ Data exported: {csv_path}\")\n    print(f\"âœ“ Metadata saved: {metadata_path}\")\n    print(f\"âœ“ Export summary: {len(df):,} records, {file_size_mb:.1f} MB\")\n    \n    return export_metadata\n\n\nData integrity validation\n\nSchema Checking: Ensures proper field types and formats.\nNull Validation: Confirms mandatory fields are populated.\nValue Sanity Checks: Verifies non-negative numbers.\n\n\n\nShow validation function code\ndef validate_extracted_data(df):\n    \"\"\"\n    Comprehensive validation of extracted NDIS data\n    \"\"\"\n    print(\"Validating extracted data...\")\n    \n    # Required columns check\n    required_cols = [\n        'timestamp', 'jurisdiction',\n        'offender_profiles', 'arrestee', 'forensic_profiles', \n        'ndis_labs', 'investigations_aided'\n    ]\n    \n    validation_results = {}\n    \n    # Check column presence\n    missing_cols = set(required_cols) - set(df.columns)\n    validation_results['missing_columns'] = list(missing_cols)\n    \n    if missing_cols:\n        print(f\"âœ— Missing required columns: {missing_cols}\")\n        return validation_results\n    \n    # Data quality checks\n    validation_results['total_records'] = len(df)\n    validation_results['unique_timestamps'] = df['timestamp'].nunique()\n    validation_results['unique_jurisdictions'] = df['jurisdiction'].nunique()\n    validation_results['date_range'] = {\n        'earliest': df['capture_date'].min().strftime('%Y-%m-%d'),\n        'latest': df['capture_date'].max().strftime('%Y-%m-%d')\n    }\n       \n    # Value range checks\n    numeric_cols = ['offender_profiles', 'arrestee', 'forensic_profiles', 'ndis_labs', 'investigations_aided']\n    negative_values = {}\n    for col in numeric_cols:\n        negative_count = (df[col] &lt; 0).sum()\n        if negative_count &gt; 0:\n            negative_values[col] = negative_count\n    validation_results['negative_values'] = negative_values\n    \n    # Era-specific validation\n    pre2007_count = len(df[df['capture_year'] &lt; 2007])\n    arrestee_pre2012 = len(df[(df['capture_year'] &lt; 2012) & (df['arrestee'] &gt; 0)])\n    \n    validation_results['era_checks'] = {\n        'pre2007_records': pre2007_count,\n        'arrestee_before_2012': arrestee_pre2012  # Should be 0\n    }\n    \n    # Print summary\n    print(f\"âœ“ Total records: {validation_results['total_records']:,}\")\n    print(f\"âœ“ Unique snapshots: {validation_results['unique_timestamps']}\")\n    print(f\"âœ“ Unique jurisdictions: {validation_results['unique_jurisdictions']}\")\n    print(f\"âœ“ Date range: {validation_results['date_range']['earliest']} to {validation_results['date_range']['latest']}\")\n    \n    if validation_results['negative_values']:\n        print(f\"! Negative values found: {validation_results['negative_values']}\")\n    \n    if validation_results['era_checks']['arrestee_before_2012'] &gt; 0:\n        print(f\"! Data integrity issue: {validation_results['era_checks']['arrestee_before_2012']} arrestee records found before 2012\")\n    \n    return validation_results\n\n\n\nExtraction Execution\n\n\nShow main execution code\nif __name__ == \"__main__\":\n    # Process all snapshots\n    ndis_data = process_all_snapshots()\n    \n    if not ndis_data.empty:\n        # Validate data quality\n        validation_results = validate_extracted_data(ndis_data)\n        \n        # Export if validation passes\n        export_metadata = export_ndis_data(ndis_data)\n        print(\"\\n\" + \"=\"*50)\n        print(\"EXTRACTION COMPLETE\")\n        print(\"=\"*50)\n        print(f\"Records extracted: {len(ndis_data):,}\")\n        print(f\"Time span: {ndis_data['capture_year'].min()}-{ndis_data['capture_year'].max()}\")\n        print(f\"Jurisdictions: {ndis_data['jurisdiction'].nunique()}\")\n    else:\n        print(\"No data extracted - check HTML files and parsing logic\")\n\n\nProcessing 11359 HTML snapshots...\n\n\n\n\n\nâš   No records extracted from 20120101203804_ndis.html (year: 2012, parser: 2012-2016)\nâš   No records extracted from 20120111115907_ndis.html (year: 2012, parser: 2012-2016)\nâš   No records extracted from 20120508173804_ndis.html (year: 2012, parser: 2012-2016)\nâš   No records extracted from 20161228141646_nj.html (year: 2016, parser: 2012-2016)\nâš   No records extracted from 20161228141656_stats.html (year: 2016, parser: 2012-2016)\nâš   No records extracted from 20161228141708_ca.html (year: 2016, parser: 2012-2016)\nâš   No records extracted from 20161228143828_or.html (year: 2016, parser: 2012-2016)\nâš   No records extracted from 20161228143832_mt.html (year: 2016, parser: 2012-2016)\nâš   No records extracted from 20161228143841_wy.html (year: 2016, parser: 2012-2016)\nâš   No records extracted from 20161228143848_co.html (year: 2016, parser: 2012-2016)\nâš   No records extracted from 20161228143853_nm.html (year: 2016, parser: 2012-2016)\nâš   No records extracted from 20161228144009_nd.html (year: 2016, parser: 2012-2016)\nâš   No records extracted from 20161228144424_sd.html (year: 2016, parser: 2012-2016)\nâš   No records extracted from 20161228144434_ne.html (year: 2016, parser: 2012-2016)\nâš   No records extracted from 20161228144438_ks.html (year: 2016, parser: 2012-2016)\nâš   No records extracted from 20161228144451_ok.html (year: 2016, parser: 2012-2016)\nâš   No records extracted from 20210811085903_nj.html (year: 2021, parser: post2017)\nâš   No records extracted from 20210811085906_stats.html (year: 2021, parser: post2017)\nâš   No records extracted from 20210811085910_ca.html (year: 2021, parser: post2017)\nâš   No records extracted from 20210811085918_or.html (year: 2021, parser: post2017)\nâš   No records extracted from 20210811085922_mt.html (year: 2021, parser: post2017)\nâš   No records extracted from 20210811085926_wy.html (year: 2021, parser: post2017)\nâš   No records extracted from 20210811085929_co.html (year: 2021, parser: post2017)\nâš   No records extracted from 20210811085932_nm.html (year: 2021, parser: post2017)\nâš   No records extracted from 20210811085935_nd.html (year: 2021, parser: post2017)\nâš   No records extracted from 20210811085939_sd.html (year: 2021, parser: post2017)\nâš   No records extracted from 20210811085943_ne.html (year: 2021, parser: post2017)\nâš   No records extracted from 20210811085947_ks.html (year: 2021, parser: post2017)\nâš   No records extracted from 20210811085950_ok.html (year: 2021, parser: post2017)\nâš   No records extracted from 20220209033036_ndis.html (year: 2022, parser: post2017)\nâš   No records extracted from 20220210054625_ndis.html (year: 2022, parser: post2017)\nâš   No records extracted from 20231113033840_ndis.html (year: 2023, parser: post2017)\nâš   No records extracted from 20240415150311_ndis.html (year: 2024, parser: post2017)\nProcessing complete: 11326 successful, 33 failed\nFailure report saved: ..\\data\\ndis\\raw\\ndis_outputs\\processing_failures.json\nValidating extracted data...\nâœ“ Total records: 32,009\nâœ“ Unique snapshots: 11280\nâœ“ Unique jurisdictions: 169\nâœ“ Date range: 2001-07-15 to 2025-08-15\nâœ“ Data exported: ..\\data\\ndis\\raw\\ndis_outputs\\ndis_data_raw.csv\nâœ“ Metadata saved: ..\\data\\ndis\\raw\\ndis_outputs\\ndis_export_metadata_20251009_143739.json\nâœ“ Export summary: 32,009 records, 3.5 MB\n\n==================================================\nEXTRACTION COMPLETE\n==================================================\nRecords extracted: 32,009\nTime span: 2001-2025\nJurisdictions: 169\n\n\nView Analyses â†’"
  },
  {
    "objectID": "about.html#overview",
    "href": "about.html#overview",
    "title": "PODFRIDGE - U.S. Forensic DNA Database",
    "section": "",
    "text": "This repository contains data collection, processing, and analysis code for a comprehensive study of U.S. forensic DNA databases spanning 2001-2025. The project reconstructs the historical growth of the National DNA Index System (NDIS), compiles current state-level DNA database policies and statistics (SDIS), and standardizes demographic data from Freedom of Information Act (FOIA) requests."
  },
  {
    "objectID": "about.html#associated-publication",
    "href": "about.html#associated-publication",
    "title": "PODFRIDGE - U.S. Forensic DNA Database",
    "section": "",
    "text": "Lasisi, T., Donadio, J.P., Muller, M., Wilson, J., Mooney, J., & Edge, M.D. (2025). United States forensic DNA databases: national time series (2001â€“2025) and state cross-sections."
  },
  {
    "objectID": "about.html#project-components",
    "href": "about.html#project-components",
    "title": "PODFRIDGE - U.S. Forensic DNA Database",
    "section": "",
    "text": "Reconstructs the growth of the FBIâ€™s National DNA Index System using archived snapshots from the Internet Archiveâ€™s Wayback Machine.\n\nData Source: FBI CODIS-NDIS Statistics pages\nCoverage: Monthly snapshots from 2001-2025\nMetrics: Offender profiles, arrestee profiles, forensic profiles, participating laboratories, investigations aided\nMethods: Web scraping, HTML parsing, temporal validation, outlier detection\n\nView NDIS Scraping Methodology â†’\nView NDIS Analysis â†’\n\n\n\nCompiles current state-level DNA database statistics and policy information across all 50 states and Washington D.C.\n\nData Source: State government websites, legislative databases\nCoverage: Current snapshot (August 2025)\nContent: Profile counts by type (where available), arrestee collection policies, familial search authorization, statutory citations\nMethods: Systematic web searches, policy documentation, legal statute review\n\nView SDIS Analysis â†’\n\n\n\nStandardizes demographic composition data from state DNA databases obtained through public records requests documented in Murphy & Tong (2020).\n\nData Source: FOIA responses from 7 states (Murphy & Tong, 2020, Appendix A)\nCoverage: 2012-2018 (varies by state)\nContent: Racial and gender composition by profile type (offender/arrestee/forensic)\nMethods: OCR processing, data standardization, quality validation\n\nView FOIA Analysis â†’\n\n\n\nDocuments the methodology and data sources used in Murphy & Tong (2020) for calculating annual DNA collection rates by race.\n\nData Source: Murphy & Tong (2020, Appendix B)\nCoverage: All 50 states\nContent: Annual collection estimates, Census demographics, calculated collection rates by race\nMethods: Data provenance tracking, methodology documentation\n\nView Methodology â†’"
  },
  {
    "objectID": "about.html#repository-structure",
    "href": "about.html#repository-structure",
    "title": "PODFRIDGE - U.S. Forensic DNA Database",
    "section": "",
    "text": "PODFRIDGE_Databases/\nâ”œâ”€â”€ _freeze/ # Quarto cache for rendered outputs\nâ”œâ”€â”€ data/ # Datasets used for NDIS analyses\nâ”‚ â”œâ”€â”€ ndis/ # NDIS related files (raw, intermediate and final)\nâ”‚ â”œâ”€â”€ sdis/ # SDIS related files (raw, intermediate and final)\nâ”‚ â”œâ”€â”€ foia/ # FOIA related files (raw, intermediate and final)\nâ”‚ â”œâ”€â”€ annual_dna_collection/ # Annual DNA Collection related files (raw, intermediate and final)\nâ”‚ â”œâ”€â”€ v1.0/ # Frozen csv files\nâ”‚ â””â”€â”€ README.md\nâ”œâ”€â”€ docs/ # Rendered website (GitHub Pages output)\nâ”œâ”€â”€ figures/ # Figures generated by analyses\nâ”œâ”€â”€ qmd_root/ # Quarto notebooks and analysis scripts\nâ”‚ â”œâ”€â”€ ndis_scraping.qmd # NDIS data scraping notebook\nâ”‚ â”œâ”€â”€ ndis_analysis.qmd # NDIS technical validation & figures\nâ”‚ â”œâ”€â”€ sdis_summary.qmd # SDIS compilation & analysis\nâ”‚ â”œâ”€â”€ foia_processing.qmd # FOIA data processing\nâ”‚ â”œâ”€â”€ foia_processing_pdf.qmd # FOIA data processing in PDF\nâ”‚ â”œâ”€â”€ appendix_analysis.qmd # Murphy & Tong appendix for Annual DNA Collection documentation\nâ”‚ â””â”€â”€ appendix_analysis.qmd # Murphy & Tong appendix for Annual DNA Collection documentation in PDF\nâ”œâ”€â”€ .gitattributes # Git file attributes\nâ”œâ”€â”€ .gitignore # Ignored files and folders\nâ”œâ”€â”€ LICENSE # Repository license\nâ”œâ”€â”€ README.md # Project overview and usage\nâ”œâ”€â”€ _quarto.yml # Quarto website configuration\nâ”œâ”€â”€ about.qmd # About page\nâ”œâ”€â”€ index.qmd # Main landing page\nâ”œâ”€â”€ requirements.txt # Python/R dependencies\nâ””â”€â”€ styles.css # Custom website styling"
  },
  {
    "objectID": "about.html#authors",
    "href": "about.html#authors",
    "title": "PODFRIDGE - U.S. Forensic DNA Database",
    "section": "",
    "text": "Tina Lasisi\nJ. P. Donadio\nS. C. Muller\nJ. Wilson\nJ. Mooney\nM. D. Edge\n\nCorresponding author: tlasisi@umich.edu\nDataset DOI: [To be added]"
  },
  {
    "objectID": "about.html#technical-details",
    "href": "about.html#technical-details",
    "title": "PODFRIDGE - U.S. Forensic DNA Database",
    "section": "",
    "text": "Python (â‰¥ 3.13)\nR (â‰¥ 4.0)\nQuarto (â‰¥ 1.3)\nPython packages: requests, beautifulsoup4, lxml, pandas, tqdm, hashlib, collections, pathlib, datetime, os\nR packages: tidyverse, rvest, httr, lubridate, jsonlite, knitr, plotly\n\n\n\n\n\nWeb Scraping: Internet Archive Wayback Machine API\nData Validation: Monotonicity testing, median absolute deviation (MAD) outlier detection\nExternal Validation: Comparison with peer-reviewed publications and FBI press releases\nReproducibility: All processing code available; versioned datasets archived on Zenodo"
  },
  {
    "objectID": "about.html#data-access",
    "href": "about.html#data-access",
    "title": "PODFRIDGE - U.S. Forensic DNA Database",
    "section": "",
    "text": "All final datasets are archived and publicly available on Zenodo:\nZenodo Repository: [DOI to be added upon publication]\nThe repository includes: - NDIS_time_series.csv - Monthly NDIS statistics (2001-2025) - SDIS_cross_section.csv - State-level profiles and policies (2025) - FOIA_Demographics.csv - Demographic composition from FOIA responses - Annual_DNA_Collection.csv - Annual collection rates (Murphy & Tong 2020) - Raw HTML files, intermediate processing outputs, and complete documentation\nFor detailed data dictionaries and usage notes, see data/README.md."
  },
  {
    "objectID": "index.html#project-sections",
    "href": "index.html#project-sections",
    "title": "PODFRIDGE-Databases",
    "section": "Project Sections",
    "text": "Project Sections\n\n1. NDIS Database Analysis\nTracking FBIâ€™s National DNA Index System (2001-2025)\n\nObjective: Reconstruct NDIS growth through Wayback Machine archives\nKey Findings: Exponential profile growth, jurisdictional participation patterns\nMethods: Web scraping, data validation, time-series analysis\n\nView Scraping Methodology â†’\nView Full Analysis â†’\n\n\n2. SDIS Summary Analysis\nState DNA Database Policies and Practices\n\nObjective: Examine variation in state DNA database policies\nKey Findings: Arrestee collection policies, familial search allowances\nMethods: Legal statute analysis, policy documentation\n\nView Full Analysis â†’\n\n\n3. FOIA Document Processing\nRacial Composition of State DNA Databases\n\nObjective: Process FOIA responses on demographic disparities\nKey Findings: Standardized data from 7 states, transparency framework\nMethods: OCR processing, data standardization, quality control\n\nView Full Analysis â†’\n\n\n4. Annual DNA Collection\nMurphy & Tong Study Appendix\n\nObjective: Document data sources and methodology for racial disparity research\nKey Findings: Consistent disparities, data limitations, methodological challenges\nMethods: Data provenance tracking, methodology documentation\n\nView Full Analysis â†’"
  },
  {
    "objectID": "qmd_root/ndis_analysis.html#detection-rules",
    "href": "qmd_root/ndis_analysis.html#detection-rules",
    "title": "NDIS Database",
    "section": "Detection Rules",
    "text": "Detection Rules\n\n1. Spike-Dip Detection\nA point is flagged as spike_dip if it deviates significantly from adjacent observations:\n\\[N_{j,t}^{(x)} \\text{ is flagged if any of the following holds:}\\]\n\n\\(N_{j,t}^{(x)} &gt; 2 \\times N_{j,t-1}^{(x)}\\) (more than double the previous value)\n\\(N_{j,t}^{(x)} &lt; 0.5 \\times N_{j,t-1}^{(x)}\\) (less than half the previous value)\n\\(N_{j,t}^{(x)} &gt; 2 \\times N_{j,t+1}^{(x)}\\) (more than double the next value)\n\\(N_{j,t}^{(x)} &lt; 0.5 \\times N_{j,t+1}^{(x)}\\) (less than half the next value)\n\nA continuation of spike-dip is flagged as cont_spike_dip when the previous point was flagged as spike_dip AND the current point shows recovery:\n\\[\\text{If } N_{j,t-1}^{(x)} \\text{ flag} = \\text{spike\\_dip AND} \\left[ N_{j,t}^{(x)} &gt; 0.5 \\times N_{j,t-1}^{(x)} \\text{ OR } N_{j,t}^{(x)} &lt; 2 \\times N_{j,t-1}^{(x)} \\right]\\]\nDescription: These flags capture temporary data surges or unexplained dips. A spike followed by recovery to near-normal levels, or isolated low values surrounded by higher values, indicate reporting anomalies rather than real changes in profiles.\n\n\n2. Zero Error Detection\nA point is flagged as zero_error if a zero appears after positive values:\n\\[\\text{If } N_{j,t}^{(x)} = 0 \\text{ AND } N_{j,t-1}^{(x)} &gt; 0\\]\nConsecutive zeros after an initial error are flagged as cont_zero_error:\n\\[\\text{If } N_{j,t}^{(x)} = 0 \\text{ AND } N_{j,t-1}^{(x)} \\text{ flag} = \\text{zero\\_error}\\]\nDescription: Legitimate DNA profile data cannot drop from positive to zero. When this occurs, it represents a reporting system error. All subsequent zeros until the data recovers to positive values are propagations of the same error and should be marked together.\n\n\n3. Update Lag Detection\nA point is flagged as osc_lag when values oscillate between similar levels in a compressed timeframe:\n\\[\\text{If } \\left[ \\left( N_{j,t}^{(x)} &lt; N_{j,t-1}^{(x)} \\text{ AND } N_{j,t}^{(x)} &lt; N_{j,t+1}^{(x)} \\right) \\text{ OR } \\left( N_{j,t}^{(x)} = N_{j,t-1}^{(x)} \\text{ AND } N_{j,t}^{(x)} &lt; N_{j,t+1}^{(x)} \\right) \\right.\\]\n\\[\\left. \\text{OR } \\left( N_{j,t}^{(x)} &lt; N_{j,t-1}^{(x)} \\text{ AND } N_{j,t}^{(x)} = N_{j,t+1}^{(x)} \\right) \\right]\\]\n\\[\\text{AND } [t - (t-1) \\leq 2 \\text{ days}] \\text{ AND } [(t+1) - t \\leq 2 \\text{ days}]\\]\nDescription: When sequential reports within a 48-hour window show values that decrease or remain flat relative to neighbors, this indicates system synchronization delays where data is updating across multiple databases at different times. The same profile count is being reported inconsistently during the synchronization process."
  },
  {
    "objectID": "qmd_root/ndis_analysis.html#correction-rules",
    "href": "qmd_root/ndis_analysis.html#correction-rules",
    "title": "NDIS Database",
    "section": "Correction Rules",
    "text": "Correction Rules\n\nFor spike_dip and cont_spike_dip Flags\nAction: Remove the flagged point from the dataset.\nReason: Temporary data surges or isolated dips do not represent actual growth in profiles. Removing these points preserves the genuine underlying trend while eliminating reporting artifacts.\n\n\nFor zero_error and cont_zero_error Flags\nAction: Remove all consecutive zero values starting from the first zero that follows a positive value, continuing until the data recovers to positive numbers.\nReason: Zeros appearing after positive counts are reporting failures, not real data. Removing the entire sequence of consecutive zeros eliminates the error cascade while preserving the valid trajectory before and after the error window.\n\n\nFor osc_lag Flags\nAction: Within each oscillation cluster, retain only the highest value and remove all other points in the sequence.\nReason: The highest value represents the true data point; lower values in the cluster are transient states during system synchronization. Keeping the maximum preserves the actual profile count while removing the synchronization noise.\n\n\nFor Legitimate Decreases\nAction: Preserve all decreases that do not match the patterns above.\nReason: Not all decreases are errors. Some reflect genuine profile removals due to expungements, legal stays, or case dismissals. Decreases outside the detection rules represent real changes in the database and should be retained."
  },
  {
    "objectID": "qmd_root/ndis_analysis.html#offender-profiles-correction",
    "href": "qmd_root/ndis_analysis.html#offender-profiles-correction",
    "title": "NDIS Database",
    "section": "Offender Profiles Correction",
    "text": "Offender Profiles Correction\n\n\nShow Offender profiles visualization and correction code\n# Filtering for ndis_labs &gt; 0 and deduplication for same jurisdiction in the same capture_datetime\nndis_intermediate &lt;- ndis_intermediate %&gt;%\n  mutate(capture_datetime = lubridate::round_date(capture_datetime, \"second\")) %&gt;%\n  group_by(jurisdiction, capture_datetime) %&gt;%\n  slice(1) %&gt;%\n  ungroup()\n\n#### Raw Offender profiles plot\n\n# Flag anomalies for offender profiles using formal detection rules\noffender_validation &lt;- ndis_intermediate %&gt;%\n  arrange(jurisdiction, capture_datetime) %&gt;%\n  group_by(jurisdiction) %&gt;%\n  mutate(\n    prev_value = lag(offender_profiles),\n    next_value = lead(offender_profiles),\n    \n    # Time between observations (in days)\n    days_prev = as.numeric(difftime(capture_datetime, lag(capture_datetime), units = \"days\")),\n    days_next = as.numeric(difftime(lead(capture_datetime), capture_datetime, units = \"days\")),\n    \n    # Rule 1: Spike-Dip Detection\n    # Flag if: (N_t &gt; 2*N_{t-1}) OR (N_t &lt; 0.5*N_{t-1}) OR (N_t &gt; 2*N_{t+1}) OR (N_t &lt; 0.5*N_{t+1})\n    flag_spike_dip = (\n      (!is.na(prev_value) & offender_profiles &lt; 0.5 * prev_value) |\n      (!is.na(next_value) & offender_profiles &lt; 0.5 * next_value)\n    ),\n    \n    # Continuation of spike-dip: previous was flagged AND current shows recovery\n    prev_was_spike_dip = lag(flag_spike_dip),\n    flag_cont_spike_dip = (\n      !is.na(prev_was_spike_dip) & prev_was_spike_dip &\n      ((!is.na(prev_value) & offender_profiles &gt; 0.5 * prev_value & offender_profiles &lt; 2 * prev_value) |\n      (!is.na(prev_value) & offender_profiles == prev_value))\n    ),\n    \n    # Rule 2: Zero Error Detection\n    # Flag if: (N_t == 0 AND N_{t-1} &gt; 0)\n    flag_zero_error = (\n      offender_profiles == 0 & !is.na(prev_value) & prev_value &gt; 0\n    ),\n    \n    # Continuation of zero error: previous was flagged zero error AND current is zero\n    prev_was_zero_error = lag(flag_zero_error),\n    flag_cont_zero_error = (\n      offender_profiles == 0 & !is.na(prev_was_zero_error) & prev_was_zero_error\n    ),\n    \n    # Rule 3: Update Lag Detection (oscillation)\n    # Flag if: [(N_t &lt; N_{t-1} AND N_t &lt; N_{t+1}) OR (N_t == N_{t-1} AND N_t &lt; N_{t+1}) OR (N_t &lt; N_{t-1} AND N_t == N_{t+1})]\n    # AND [days_prev &lt;= 2 AND days_next &lt;= 2]\n    flag_osc_lag = (\n      !is.na(prev_value) & !is.na(next_value) &\n      (\n        (offender_profiles &lt; prev_value & offender_profiles &lt; next_value) |\n        (offender_profiles &lt; prev_value & offender_profiles == next_value)\n      ) &\n      (!is.na(days_next) & days_next &lt;= 5)\n    ),\n\n    prev_was_osc_lag = lag(flag_osc_lag),\n\n    flag_cont_osc_lag = (\n      !is.na(prev_was_osc_lag) & prev_was_osc_lag &\n      !is.na(prev_value) & offender_profiles == prev_value\n    ),\n\n    # Combine all anomaly flags\n    flag_any = flag_spike_dip | flag_cont_spike_dip | flag_zero_error | flag_cont_zero_error | flag_osc_lag,\n    \n    # Replace NA with FALSE\n    across(starts_with(\"flag_\"), ~ifelse(is.na(.), FALSE, .)),\n\n    # --- New rule: propagate by metric value within jurisdiction ---\n    # TRUE if this offender_profiles value appears among the flagged values in this jurisdiction\n    flag_same_value_propagate = ifelse(\n      is.na(offender_profiles), \n      FALSE,\n      offender_profiles %in% offender_profiles[flag_any]\n    ),\n\n    # Update final flag_any to include this propagated-by-value flag\n    flag_any = flag_any | flag_same_value_propagate\n  ) %&gt;%\n  ungroup()\n\n# Create initial interactive plot for offender profiles with flagged points by type\np_offender_raw &lt;- offender_validation %&gt;%\n  plot_ly(x = ~capture_datetime, y = ~offender_profiles, color = ~jurisdiction, \n          type = 'scatter', mode = 'lines+markers', alpha = 0.7,\n          name = ~jurisdiction) %&gt;%\n  add_markers(data = offender_validation %&gt;% filter(flag_any),\n              x = ~capture_datetime, y = ~offender_profiles,\n              marker = list(size = 12, symbol = 'x', color = \"red\",\n                           line = list(width = 3, color = 'red'))) %&gt;%\n  layout(title = \"Convicted Offender Profiles - Raw Data (Flagged Points Marked)\",\n         xaxis = list(title = \"Date and Time\",\n         tickformat = \"%Y-%m-%d %H:%M\"),\n         yaxis = list(title = \"Offender Profiles\"))\n\np_offender_raw\n\n\n\n\n\n\nShow Offender profiles visualization and correction code\n#### Offender Profiles Correction #####\n\n# Correction for spike_dip and cont_spike_dip: Remove flagged points\noffender_clean &lt;- offender_validation %&gt;%\n  filter(!(flag_any)) %&gt;%\n  select(-starts_with(\"flag_\"), -starts_with(\"prev_\"), -starts_with(\"days_\"), -next_value)\n\n# Plot cleaned data\np_offender_clean &lt;- offender_clean %&gt;%\n  plot_ly(x = ~capture_datetime, y = ~offender_profiles, color = ~jurisdiction, \n          type = 'scatter', mode = 'lines+markers', alpha = 0.7) %&gt;%\n  layout(title = \"Convicted Offender Profiles - Cleaned Data\",\n         xaxis = list(title = \"Date\"),\n         yaxis = list(title = \"Offender Profiles\"))\n\np_offender_clean\n\n\n\n\n\n\nShow Offender profiles visualization and correction code\n# Summarise highest offender profile per jurisdiction per year\noffender_yearly &lt;- offender_clean %&gt;%\n  mutate(year = year(capture_datetime)) %&gt;%\n  group_by(jurisdiction, year) %&gt;%\n  summarise(max_offender = max(offender_profiles, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  group_by(year) %&gt;%\n  summarise(total_max_offender = sum(max_offender, na.rm = TRUE), .groups = \"drop\")\n\n# Plot yearly sums\np_offender_yearly &lt;- offender_yearly %&gt;%\n  plot_ly(x = ~year, y = ~total_max_offender,\n          type = 'scatter', mode = 'lines+markers',\n          line = list(color = \"steelblue\", width = 3),\n          marker = list(size = 8, color = \"darkred\")) %&gt;%\n  layout(title = \"Yearly Sum of Max Offender Profiles per Jurisdiction\",\n         xaxis = list(title = \"Year\"),\n         yaxis = list(title = \"Total Max Offender Profiles\"))\n\np_offender_yearly"
  },
  {
    "objectID": "qmd_root/ndis_analysis.html#forensic-profiles-correction",
    "href": "qmd_root/ndis_analysis.html#forensic-profiles-correction",
    "title": "NDIS Database",
    "section": "Forensic Profiles Correction",
    "text": "Forensic Profiles Correction\n\n\nShow Forensic profiles visualization and correction code\n#### Raw Forensic profiles plot\n\n# Flag anomalies for forensic profiles using formal detection rules\nforensic_validation &lt;- ndis_intermediate %&gt;%\n  arrange(jurisdiction, capture_datetime) %&gt;%\n  group_by(jurisdiction) %&gt;%\n  mutate(\n    prev_value = lag(forensic_profiles),\n    next_value = lead(forensic_profiles),\n    \n    # Time between observations (in days)\n    days_prev = as.numeric(difftime(capture_datetime, lag(capture_datetime), units = \"days\")),\n    days_next = as.numeric(difftime(lead(capture_datetime), capture_datetime, units = \"days\")),\n    \n    # Rule 1: Spike-Dip Detection\n    # Flag if: (N_t &gt; 2*N_{t-1}) OR (N_t &lt; 0.5*N_{t-1}) OR (N_t &gt; 2*N_{t+1}) OR (N_t &lt; 0.5*N_{t+1})\n    flag_spike_dip = (\n      (!is.na(prev_value) & forensic_profiles &lt; 0.5 * prev_value) |\n      (!is.na(next_value) & forensic_profiles &lt; 0.5 * next_value) |\n      (!is.na(next_value) & forensic_profiles &gt; 2.5 * next_value)\n    ),\n    \n    # Continuation of spike-dip: previous was flagged AND current shows recovery\n    prev_was_spike_dip = lag(flag_spike_dip),\n    flag_cont_spike_dip = (\n      !is.na(prev_was_spike_dip) & prev_was_spike_dip &\n      ((!is.na(prev_value) & forensic_profiles &gt; 0.5 * prev_value & forensic_profiles &lt; 2 * prev_value) |\n      (!is.na(prev_value) & forensic_profiles == prev_value))\n    ),\n    \n    # Rule 2: Zero Error Detection\n    # Flag if: (N_t == 0 AND N_{t-1} &gt; 0)\n    flag_zero_error = (\n      forensic_profiles == 0 & !is.na(prev_value) & prev_value &gt; 0\n    ),\n    \n    # Continuation of zero error: previous was flagged zero error AND current is zero\n    prev_was_zero_error = lag(flag_zero_error),\n    flag_cont_zero_error = (\n      forensic_profiles == 0 & !is.na(prev_was_zero_error) & prev_was_zero_error\n    ),\n    \n    # Rule 3: Update Lag Detection (oscillation)\n    # Flag if: [(N_t &lt; N_{t-1} AND N_t &lt; N_{t+1}) OR (N_t == N_{t-1} AND N_t &lt; N_{t+1}) OR (N_t &lt; N_{t-1} AND N_t == N_{t+1})]\n    # AND [days_prev &lt;= 2 AND days_next &lt;= 2]\n    flag_osc_lag = (\n      !is.na(prev_value) & !is.na(next_value) &\n      (\n        (forensic_profiles &lt; prev_value & forensic_profiles &lt; next_value) |\n        (forensic_profiles &lt; prev_value & forensic_profiles == next_value)\n      ) &\n      (!is.na(days_next) & days_next &lt;= 2)\n    ),\n\n    prev_was_osc_lag = lag(flag_osc_lag),\n\n    flag_cont_osc_lag = (\n      !is.na(prev_was_osc_lag) & prev_was_osc_lag &\n      !is.na(prev_value) & forensic_profiles == prev_value\n    ),\n\n    # Combine all anomaly flags\n    flag_any = flag_spike_dip | flag_cont_spike_dip | flag_zero_error | flag_cont_zero_error | flag_osc_lag,\n    \n    # Replace NA with FALSE\n    across(starts_with(\"flag_\"), ~ifelse(is.na(.), FALSE, .)),\n\n    # --- New rule: propagate by metric value within jurisdiction ---\n    # TRUE if this forensic_profiles value appears among the flagged values in this jurisdiction\n    flag_same_value_propagate = ifelse(\n      is.na(forensic_profiles), \n      FALSE,\n      forensic_profiles %in% forensic_profiles[flag_any]\n    ),\n\n    # Update final flag_any to include this propagated-by-value flag\n    flag_any = flag_any | flag_same_value_propagate\n  ) %&gt;%\n  ungroup()\n\n# Create initial interactive plot for forensic profiles with flagged points by type\np_forensic_raw &lt;- forensic_validation %&gt;%\n  plot_ly(x = ~capture_datetime, y = ~forensic_profiles, color = ~jurisdiction, \n          type = 'scatter', mode = 'lines+markers', alpha = 0.7,\n          name = ~jurisdiction) %&gt;%\n  add_markers(data = forensic_validation %&gt;% filter(flag_any),\n              x = ~capture_datetime, y = ~forensic_profiles,\n              marker = list(size = 12, symbol = 'x', color = \"red\",\n                           line = list(width = 3, color = 'red'))) %&gt;%\n  layout(title = \"Forensic Profiles - Raw Data (Flagged Points Marked)\",\n         xaxis = list(title = \"Date and Time\",\n         tickformat = \"%Y-%m-%d %H:%M\"),\n         yaxis = list(title = \"Forensic Profiles\"))\n\np_forensic_raw\n\n\n\n\n\n\nShow Forensic profiles visualization and correction code\n#### Forensic Profiles Correction #####\n\n# Correction for spike_dip and cont_spike_dip: Remove flagged points\nforensic_clean &lt;- forensic_validation %&gt;%\n  filter(!(flag_any)) %&gt;%\n  select(-starts_with(\"flag_\"), -starts_with(\"prev_\"), -starts_with(\"days_\"), -next_value)\n\n# Plot cleaned data\np_forensic_clean &lt;- forensic_clean %&gt;%\n  plot_ly(x = ~capture_datetime, y = ~forensic_profiles, color = ~jurisdiction, \n          type = 'scatter', mode = 'lines+markers', alpha = 0.7) %&gt;%\n  layout(title = \"Forensic Profiles - Cleaned Data\",\n         xaxis = list(title = \"Date\"),\n         yaxis = list(title = \"Forensic Profiles\"))\n\np_forensic_clean\n\n\n\n\n\n\nShow Forensic profiles visualization and correction code\n# Summarise highest forensic profile per jurisdiction per year\nforensic_yearly &lt;- forensic_clean %&gt;%\n  mutate(year = year(capture_datetime)) %&gt;%\n  group_by(jurisdiction, year) %&gt;%\n  summarise(max_forensic = max(forensic_profiles, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  group_by(year) %&gt;%\n  summarise(total_max_forensic = sum(max_forensic, na.rm = TRUE), .groups = \"drop\")\n\n# Plot yearly sums\np_forensic_yearly &lt;- forensic_yearly %&gt;%\n  plot_ly(x = ~year, y = ~total_max_forensic,\n          type = 'scatter', mode = 'lines+markers',\n          line = list(color = \"steelblue\", width = 3),\n          marker = list(size = 8, color = \"darkred\")) %&gt;%\n  layout(title = \"Yearly Sum of Max Forensic Profiles per Jurisdiction\",\n         xaxis = list(title = \"Year\"),\n         yaxis = list(title = \"Total Max Forensic Profiles\"))\n\np_forensic_yearly"
  },
  {
    "objectID": "qmd_root/ndis_analysis.html#arrestee-profiles-correction",
    "href": "qmd_root/ndis_analysis.html#arrestee-profiles-correction",
    "title": "NDIS Database",
    "section": "Arrestee Profiles Correction",
    "text": "Arrestee Profiles Correction\n\n\nShow Arrestee profiles visualization and correction code\n#### Raw Arrestee profiles plot\n\n# Flag anomalies for arrestee profiles using formal detection rules\narrestee_validation &lt;- ndis_intermediate %&gt;%\n  filter(capture_datetime &gt;= as.Date(\"2012-01-01\")) %&gt;%\n  arrange(jurisdiction, capture_datetime) %&gt;%\n  group_by(jurisdiction) %&gt;%\n  mutate(\n    prev_value = lag(arrestee),\n    next_value = lead(arrestee),\n    \n    # Time between observations (in days)\n    days_prev = as.numeric(difftime(capture_datetime, lag(capture_datetime), units = \"days\")),\n    days_next = as.numeric(difftime(lead(capture_datetime), capture_datetime, units = \"days\")),\n    \n    # Rule 2: Zero Error Detection\n    # Flag if: (N_t == 0 AND N_{t-1} &gt; 0)\n    flag_zero_error = (\n      arrestee == 0 & !is.na(prev_value) & jurisdiction == \"California\"\n    ),\n\n    # Combine all anomaly flags\n    flag_any = flag_zero_error,\n    \n    # Replace NA with FALSE\n    across(starts_with(\"flag_\"), ~ifelse(is.na(.), FALSE, .))\n  ) %&gt;%\n  ungroup()\n\n# Create initial interactive plot for arrestee profiles with flagged points\np_arrestee_raw &lt;- arrestee_validation %&gt;%\n  plot_ly(x = ~capture_datetime, y = ~arrestee, color = ~jurisdiction, \n          type = 'scatter', mode = 'lines+markers', alpha = 0.7,\n          name = ~jurisdiction) %&gt;%\n  add_markers(data = arrestee_validation %&gt;% filter(flag_any),\n              x = ~capture_datetime, y = ~arrestee, \n              color = ~jurisdiction,\n              marker = list(size = 12, symbol = 'x', \n                           line = list(width = 3, color = 'red')),\n              name = ~paste0(jurisdiction, \" - Flagged\"),\n              showlegend = FALSE) %&gt;%\n  layout(title = \"Arrestee Profiles - Raw Data (Flagged Points Marked)\",\n         xaxis = list(title = \"Date\"),\n         yaxis = list(title = \"Arrestee Profiles\"))\n\np_arrestee_raw\n\n\n\n\n\n\nShow Arrestee profiles visualization and correction code\n#### Arrestee Profiles Correction #####\n\narrestee_clean &lt;- arrestee_validation %&gt;%\n  filter(!(flag_any)) %&gt;%\n  select(-starts_with(\"flag_\"), -starts_with(\"prev_\"), -starts_with(\"days_\"), -next_value)\n\n# Plot cleaned data\np_arrestee_clean &lt;- arrestee_clean %&gt;%\n  plot_ly(x = ~capture_datetime, y = ~arrestee, color = ~jurisdiction, \n          type = 'scatter', mode = 'lines+markers', alpha = 0.7) %&gt;%\n  layout(title = \"Arrestee Profiles - Cleaned Data\",\n         xaxis = list(title = \"Date\"),\n         yaxis = list(title = \"Arrestee Profiles\"))\n\np_arrestee_clean\n\n\n\n\n\n\nShow Arrestee profiles visualization and correction code\n# Summarise highest arrestee profile per jurisdiction per year\narrestee_yearly &lt;- arrestee_clean %&gt;%\n  mutate(year = year(capture_datetime)) %&gt;%\n  group_by(jurisdiction, year) %&gt;%\n  summarise(max_arrestee = max(arrestee, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  group_by(year) %&gt;%\n  summarise(total_max_arrestee = sum(max_arrestee, na.rm = TRUE), .groups = \"drop\")\n\n# Plot yearly sums\np_arrestee_yearly &lt;- arrestee_yearly %&gt;%\n  plot_ly(x = ~year, y = ~total_max_arrestee,\n          type = 'scatter', mode = 'lines+markers',\n          line = list(color = \"purple\", width = 3),\n          marker = list(size = 8, color = \"magenta\")) %&gt;%\n  layout(title = \"Yearly Sum of Max Arrestee Profiles per Jurisdiction\",\n         xaxis = list(title = \"Year\"),\n         yaxis = list(title = \"Total Max Arrestee Profiles\"))\n\np_arrestee_yearly"
  },
  {
    "objectID": "qmd_root/ndis_analysis.html#investigations-aided-correction",
    "href": "qmd_root/ndis_analysis.html#investigations-aided-correction",
    "title": "NDIS Database",
    "section": "Investigations Aided Correction",
    "text": "Investigations Aided Correction\n\n\nShow Investigations Aided visualization and correction code\n#### Raw Investigations Aided plot\n\n# Flag anomalies for investigations aided using formal detection rules\ninvestigations_validation &lt;- ndis_intermediate %&gt;%\n  arrange(jurisdiction, capture_datetime) %&gt;%\n  group_by(jurisdiction) %&gt;%\n  mutate(\n    prev_value = lag(investigations_aided),\n    next_value = lead(investigations_aided),\n    \n    # Time between observations (in days)\n    days_prev = as.numeric(difftime(capture_datetime, lag(capture_datetime), units = \"days\")),\n    days_next = as.numeric(difftime(lead(capture_datetime), capture_datetime, units = \"days\")),\n    \n    # Rule 1: Spike-Dip Detection\n    # Flag if: (N_t &gt; 2*N_{t-1}) OR (N_t &lt; 0.5*N_{t-1}) OR (N_t &gt; 2*N_{t+1}) OR (N_t &lt; 0.5*N_{t+1})\n    flag_spike_dip = (\n      (!is.na(prev_value) & investigations_aided &gt; 10 * prev_value)\n    ),\n    \n    # Continuation of spike-dip: previous was flagged AND current shows recovery\n    prev_was_spike_dip = lag(flag_spike_dip),\n    flag_cont_spike_dip = (\n      !is.na(prev_was_spike_dip) & prev_was_spike_dip &\n      ((!is.na(prev_value) & investigations_aided &gt; 0.5 * prev_value & investigations_aided &lt; 2 * prev_value) |\n      (!is.na(prev_value) & investigations_aided == prev_value))\n    ),\n    \n    # Rule 2: Zero Error Detection\n    # Flag if: (N_t == 0 AND N_{t-1} &gt; 0)\n    flag_zero_error = (\n      investigations_aided == 0 & !is.na(prev_value) & prev_value &gt; 0\n    ),\n    \n    # Continuation of zero error: previous was flagged zero error AND current is zero\n    prev_was_zero_error = lag(flag_zero_error),\n    flag_cont_zero_error = (\n      investigations_aided == 0 & !is.na(prev_was_zero_error) & prev_was_zero_error\n    ),\n\n    # Combine all anomaly flags\n    flag_any = flag_spike_dip | flag_cont_spike_dip | flag_zero_error | flag_cont_zero_error,\n    \n    # Replace NA with FALSE\n    across(starts_with(\"flag_\"), ~ifelse(is.na(.), FALSE, .)),\n\n    # --- New rule: propagate by metric value within jurisdiction ---\n    # TRUE if this investigations_aided value appears among the flagged values in this jurisdiction\n    flag_same_value_propagate = ifelse(\n      is.na(investigations_aided), \n      FALSE,\n      investigations_aided %in% investigations_aided[flag_any]\n    ),\n\n    # Update final flag_any to include this propagated-by-value flag\n    flag_any = flag_any | flag_same_value_propagate\n  ) %&gt;%\n  ungroup()\n\n# Create initial interactive plot for investigations aided with flagged points\np_investigations_raw &lt;- investigations_validation %&gt;%\n  plot_ly(x = ~capture_datetime, y = ~investigations_aided, color = ~jurisdiction, \n          type = 'scatter', mode = 'lines+markers', alpha = 0.7,\n          name = ~jurisdiction) %&gt;%\n  add_markers(data = investigations_validation %&gt;% filter(flag_any),\n              x = ~capture_datetime, y = ~investigations_aided, \n              color = ~jurisdiction,\n              marker = list(size = 12, symbol = 'x', \n                           line = list(width = 3, color = 'red')),\n              name = ~paste0(jurisdiction, \" - Flagged\"),\n              showlegend = FALSE) %&gt;%\n  layout(title = \"Investigations Aided - Raw Data (Flagged Points Marked)\",\n         xaxis = list(title = \"Date\"),\n         yaxis = list(title = \"Investigations Aided\"))\n\np_investigations_raw\n\n\n\n\n\n\nShow Investigations Aided visualization and correction code\n#### Investigations Aided Correction #####\n\n# Remove spike_dip and cont_spike_dip\ninvestigations_clean &lt;- investigations_validation %&gt;%\n  filter(!(flag_any)) %&gt;%\n  select(-starts_with(\"flag_\"), -starts_with(\"prev_\"), -starts_with(\"days_\"), -next_value)\n\n# Plot cleaned data\np_investigations_clean &lt;- investigations_clean %&gt;%\n  plot_ly(x = ~capture_datetime, y = ~investigations_aided, color = ~jurisdiction, \n          type = 'scatter', mode = 'lines+markers', alpha = 0.7) %&gt;%\n  layout(title = \"Investigations Aided - Cleaned Data\",\n         xaxis = list(title = \"Date\"),\n         yaxis = list(title = \"Investigations Aided\"))\n\np_investigations_clean\n\n\n\n\n\n\nShow Investigations Aided visualization and correction code\n# Summarise highest investigations_aided per jurisdiction per year\ninvestigations_yearly &lt;- investigations_clean %&gt;%\n  mutate(year = year(capture_datetime)) %&gt;%\n  group_by(jurisdiction, year) %&gt;%\n  summarise(max_investigations = max(investigations_aided, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  group_by(year) %&gt;%\n  summarise(total_max_investigations = sum(max_investigations, na.rm = TRUE), .groups = \"drop\")\n\n# Plot yearly sums\np_investigations_yearly &lt;- investigations_yearly %&gt;%\n  plot_ly(x = ~year, y = ~total_max_investigations,\n          type = 'scatter', mode = 'lines+markers',\n          line = list(color = \"darkblue\", width = 3),\n          marker = list(size = 8, color = \"red\")) %&gt;%\n  layout(title = \"Yearly Sum of Max Investigations Aided per Jurisdiction\",\n         xaxis = list(title = \"Year\"),\n         yaxis = list(title = \"Total Max Investigations Aided\"))\n\np_investigations_yearly"
  },
  {
    "objectID": "qmd_root/ndis_analysis.html#participating-laboratories-correction",
    "href": "qmd_root/ndis_analysis.html#participating-laboratories-correction",
    "title": "NDIS Database",
    "section": "Participating Laboratories Correction",
    "text": "Participating Laboratories Correction\n\n\nShow Labs visualization and correction code\n#### Raw NDIS Labs plot\n\n# Flag anomalies for ndis_labs using formal detection rules\nlabs_validation &lt;- ndis_intermediate %&gt;%\n  arrange(jurisdiction, capture_datetime) %&gt;%\n  group_by(jurisdiction) %&gt;%\n  mutate(\n    prev_value = lag(ndis_labs),\n    next_value = lead(ndis_labs),\n    \n    # Time between observations (in days)\n    days_prev = as.numeric(difftime(capture_datetime, lag(capture_datetime), units = \"days\")),\n    days_next = as.numeric(difftime(lead(capture_datetime), capture_datetime, units = \"days\")),\n    \n    # Rule 1: Spike-Dip Detection\n    # Flag if: (N_t &gt; 2*N_{t-1}) OR (N_t &lt; 0.5*N_{t-1}) OR (N_t &gt; 2*N_{t+1}) OR (N_t &lt; 0.5*N_{t+1})\n    flag_spike_dip = (\n      (!is.na(prev_value) & ndis_labs &gt; 3 * prev_value & jurisdiction == \"Oklahoma\") |\n      (!is.na(prev_value) & ndis_labs &lt;= 0.25 * prev_value & jurisdiction == \"Michigan\")\n    ),\n    \n    # Continuation of spike-dip: previous was flagged AND current shows recovery\n    prev_was_spike_dip = lag(flag_spike_dip),\n    flag_cont_spike_dip = (\n      !is.na(prev_was_spike_dip) & prev_was_spike_dip &\n      ((!is.na(prev_value) & ndis_labs &gt; 0.5 * prev_value & ndis_labs &lt; 2 * prev_value) |\n      (!is.na(prev_value) & ndis_labs == prev_value))\n    ),\n\n    # Combine all anomaly flags\n    flag_any = flag_spike_dip | flag_cont_spike_dip,\n    \n    # Replace NA with FALSE\n    across(starts_with(\"flag_\"), ~ifelse(is.na(.), FALSE, .))\n  ) %&gt;%\n  ungroup()\n\n\n# Create initial interactive plot for ndis_labs with flagged points\np_labs_raw &lt;- labs_validation %&gt;%\n  plot_ly(x = ~capture_datetime, y = ~ndis_labs, color = ~jurisdiction, \n          type = 'scatter', mode = 'lines+markers', alpha = 0.7,\n          name = ~jurisdiction) %&gt;%\n  add_markers(data = labs_validation %&gt;% filter(flag_any),\n              x = ~capture_datetime, y = ~ndis_labs, \n              color = ~jurisdiction,\n              marker = list(size = 12, symbol = 'x', \n                           line = list(width = 3, color = 'red')),\n              name = ~paste0(jurisdiction, \" - Flagged\"),\n              showlegend = FALSE) %&gt;%\n  layout(title = \"NDIS Labs - Raw Data (Flagged Points Marked)\",\n         xaxis = list(title = \"Date\"),\n         yaxis = list(title = \"NDIS Labs\"))\n\np_labs_raw\n\n\n\n\n\n\nShow Labs visualization and correction code\n#### NDIS Labs Correction #####\n\n# Remove spike_dip and cont_spike_dip\nlabs_clean &lt;- labs_validation %&gt;%\n  filter(!(flag_any)) %&gt;%\n  select(-starts_with(\"flag_\"), -starts_with(\"prev_\"), -starts_with(\"days_\"), -next_value)\n\n# Plot cleaned data\np_labs_clean &lt;- labs_clean %&gt;%\n  plot_ly(x = ~capture_datetime, y = ~ndis_labs, color = ~jurisdiction, \n          type = 'scatter', mode = 'lines+markers', alpha = 0.7) %&gt;%\n  layout(title = \"NDIS Labs - Cleaned Data\",\n         xaxis = list(title = \"Date\"),\n         yaxis = list(title = \"NDIS Labs\"))\n\np_labs_clean"
  },
  {
    "objectID": "qmd_root/ndis_analysis.html#total-profiles-aggregation-and-correction",
    "href": "qmd_root/ndis_analysis.html#total-profiles-aggregation-and-correction",
    "title": "NDIS Database",
    "section": "Total Profiles Aggregation and Correction",
    "text": "Total Profiles Aggregation and Correction\nAnalysis Approach:\nThe total profiles metric aggregates all DNA profile types (Offender + Arrestee + Forensic) to provide a comprehensive view of the NDIS database size.\nThe analysis tracks cumulative growth per jurisdiction, shows individual jurisdiction contributions, and reveals relative database sizes across jurisdictions.\n\n\nShow Total Profiles correction code\n# Combine cleaned datasets - preserve all variables\nndis_joined &lt;- forensic_clean %&gt;%\n  select(jurisdiction, capture_datetime, asof_date, forensic_profiles) %&gt;%\n  distinct(jurisdiction, capture_datetime, .keep_all = TRUE) %&gt;%\n  full_join(\n    arrestee_clean %&gt;%\n      select(jurisdiction, capture_datetime, asof_date, arrestee) %&gt;%\n      distinct(jurisdiction, capture_datetime, .keep_all = TRUE),\n    by = c(\"jurisdiction\", \"capture_datetime\", \"asof_date\")\n  ) %&gt;%\n  mutate(arrestee = ifelse(is.na(arrestee), 0, arrestee)) %&gt;%\n  arrange(jurisdiction, capture_datetime)\n\n# Join investigations_aided\nndis_joined &lt;- ndis_joined %&gt;%\n  inner_join(\n    investigations_clean %&gt;%\n      select(jurisdiction, capture_datetime, asof_date, investigations_aided) %&gt;%\n      distinct(jurisdiction, capture_datetime, .keep_all = TRUE),\n    by = c(\"jurisdiction\", \"capture_datetime\", \"asof_date\")\n  ) %&gt;%\n  distinct(jurisdiction, capture_datetime, .keep_all = TRUE)\n\n# Join offender_profiles\nndis_joined &lt;- ndis_joined %&gt;%\n  inner_join(\n    offender_clean %&gt;%\n      select(jurisdiction, capture_datetime, asof_date, offender_profiles) %&gt;%\n      distinct(jurisdiction, capture_datetime, .keep_all = TRUE),\n    by = c(\"jurisdiction\", \"capture_datetime\", \"asof_date\")\n  ) %&gt;%\n  distinct(jurisdiction, capture_datetime, .keep_all = TRUE)\n\n# Join ndis_labs\nndis_joined &lt;- ndis_joined %&gt;%\n  inner_join(\n    labs_clean %&gt;%\n      select(jurisdiction, capture_datetime, asof_date, ndis_labs) %&gt;%\n      distinct(jurisdiction, capture_datetime, .keep_all = TRUE),\n    by = c(\"jurisdiction\", \"capture_datetime\", \"asof_date\")\n  ) %&gt;%\n  distinct(jurisdiction, capture_datetime, .keep_all = TRUE) %&gt;%\n  arrange(jurisdiction, capture_datetime)\n\n# Create final clean dataset with all variables\nndis_clean &lt;- ndis_joined %&gt;%\n  mutate(\n    year = year(capture_datetime),\n    offender_profiles = ifelse(is.na(offender_profiles), 0, offender_profiles),\n    arrestee = ifelse(is.na(arrestee), 0, arrestee),\n    forensic_profiles = ifelse(is.na(forensic_profiles), 0, forensic_profiles),\n    investigations_aided = ifelse(is.na(investigations_aided), 0, investigations_aided),\n    ndis_labs = ifelse(is.na(ndis_labs), 0, ndis_labs),\n    total_profiles = offender_profiles + arrestee + forensic_profiles\n  ) %&gt;%\n  filter(!(jurisdiction == \"California\" & arrestee == 0 & year &gt; 2013)) %&gt;%\n  arrange(jurisdiction, capture_datetime)\n\n# Plot cleaned total\np_total_cleaned &lt;- ndis_clean %&gt;%\n  plot_ly(x = ~capture_datetime, y = ~total_profiles, color = ~jurisdiction, \n          type = 'scatter', mode = 'lines+markers', alpha = 0.7, connectgaps = TRUE) %&gt;%\n  layout(title = \"Total Profiles - Cleaned Data\",\n         xaxis = list(title = \"Date\"),\n         yaxis = list(title = \"Total Profiles\"))\n\np_total_cleaned\n\n\n\n\n\n\nShow Total Profiles correction code\n# Get most recent data for stacked bar plot\nmost_recent_date &lt;- max(ndis_clean$capture_datetime, na.rm = TRUE)\n\nprofiles_latest &lt;- ndis_clean %&gt;%\n  filter(capture_datetime == most_recent_date) %&gt;%\n  select(jurisdiction, offender_profiles, arrestee, forensic_profiles) %&gt;%\n  pivot_longer(\n    cols = c(offender_profiles, arrestee, forensic_profiles),\n    names_to = \"profile_type\",\n    values_to = \"count\"\n  ) %&gt;%\n  mutate(\n    profile_type = case_when(\n      profile_type == \"offender_profiles\" ~ \"Offender Profiles\",\n      profile_type == \"arrestee\" ~ \"Arrestee Profiles\",\n      profile_type == \"forensic_profiles\" ~ \"Forensic Profiles\"\n    )\n  )\n\np_profiles_stacked &lt;- ggplot(profiles_latest, \n                             aes(x = jurisdiction, y = count, fill = profile_type)) +\n  geom_bar(stat = \"identity\", color = \"black\", linewidth = 0.3) +\n  scale_fill_manual(\n    name = \"Profile Type\",\n    values = c(\n      \"Offender Profiles\" = \"#1f4e79\",\n      \"Arrestee Profiles\" = \"#2e75b6\", \n      \"Forensic Profiles\" = \"#5b9bd5\"\n    )\n  ) +\n  scale_y_continuous(\n    labels = function(x) {\n      ifelse(x &gt;= 1e6, paste0(x/1e6, \"M\"), \n             ifelse(x &gt;= 1e3, paste0(x/1e3, \"K\"), x))\n    },\n    limits = c(0, NA),\n    expand = c(0, 0)\n  ) +\n  theme_minimal(base_size = 11) +\n  theme(\n    panel.grid = element_blank(),\n    axis.line = element_line(color = \"black\", linewidth = 0.4),\n    axis.ticks = element_line(color = \"black\", linewidth = 0.4),\n    axis.text = element_text(color = \"black\", size = 12),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    axis.title.x = element_text(size = 20, face = \"bold\", margin = margin(t = 15)),\n    axis.title.y = element_text(size = 20, face = \"bold\", margin = margin(t = 15)),\n    axis.title = element_text(size = 18, face = \"bold\"),\n    legend.title = element_text(size = 20, face = \"bold\"),\n    legend.text = element_text(size = 15),\n    legend.key.size = unit(0.5, \"cm\"),\n    legend.key.width = unit(0.4, \"cm\"),\n    legend.key.height = unit(0.4, \"cm\"),\n    aspect.ratio = 0.65\n  ) +\n  labs(\n    x = \"Jurisdiction\",\n    y = \"Number of DNA Profiles\",\n    title = \" \"\n  )\n\np_profiles_stacked"
  },
  {
    "objectID": "qmd_root/ndis_analysis.html#anomaly-detection-and-metadata-logging",
    "href": "qmd_root/ndis_analysis.html#anomaly-detection-and-metadata-logging",
    "title": "NDIS Database",
    "section": "Anomaly Detection and Metadata Logging",
    "text": "Anomaly Detection and Metadata Logging\nThis section systematically documents all data anomalies detected during the validation process.\nEach flagged observation is recorded with comprehensive metadata including anomaly type, jurisdiction, timestamp, and contextual values.\nThe log serves as both an audit trail for data quality decisions and a source for transparency reporting.\nKey outputs include:\n\nDetailed anomaly records for technical review\nSummary statistics for quality assessment\nVisualization of anomaly distribution patterns\nFormatted summaries for reporting\n\n\n\nShow anomaly detection and logging code\n### Anomaly Detection and Metadata Logging\n\n# Helper function to ensure all flag columns exist and are properly set to FALSE\nensure_flag_columns &lt;- function(df) {\n  flag_cols &lt;- c(\"flag_spike_dip\", \"flag_cont_spike_dip\", \"flag_zero_error\", \n                 \"flag_cont_zero_error\", \"flag_osc_lag\")\n  \n  for (col in flag_cols) {\n    if (!col %in% names(df)) {\n      df[[col]] &lt;- FALSE\n    } else {\n      df[[col]] &lt;- coalesce(df[[col]], FALSE)\n    }\n  }\n  return(df)\n}\n\n# Function to create standardized anomaly records with formal detection rule names\ncreate_anomaly_record &lt;- function(metric_name, jurisdiction, date, value, \n                                 flag_spike_dip = FALSE, flag_cont_spike_dip = FALSE,\n                                 flag_zero_error = FALSE, flag_cont_zero_error = FALSE,\n                                 flag_osc_lag = FALSE, prev_value = NA, next_value = NA) {\n  \n  # Determine primary anomaly type based on flags\n  anomaly_type &lt;- case_when(\n    flag_spike_dip ~ \"spike_dip\",\n    flag_cont_spike_dip ~ \"cont_spike_dip\",\n    flag_zero_error ~ \"zero_error\",\n    flag_cont_zero_error ~ \"cont_zero_error\",\n    flag_osc_lag ~ \"osc_lag\",\n    TRUE ~ \"other\"\n  )\n  \n  data.frame(\n    metric = metric_name,\n    jurisdiction = jurisdiction,\n    date = date,\n    value = value,\n    previous_value = prev_value,\n    next_value = next_value,\n    flag_spike_dip = flag_spike_dip,\n    flag_cont_spike_dip = flag_cont_spike_dip,\n    flag_zero_error = flag_zero_error,\n    flag_cont_zero_error = flag_cont_zero_error,\n    flag_osc_lag = flag_osc_lag,\n    anomaly_type = anomaly_type,\n    stringsAsFactors = FALSE\n  )\n}\n\n# Initialize empty anomaly log\nanomaly_log &lt;- data.frame()\n\n### Offender Profiles Anomalies\noffender_anomalies &lt;- offender_validation %&gt;%\n  filter(flag_any) %&gt;%\n  select(jurisdiction, capture_datetime, offender_profiles, prev_value, next_value,\n         any_of(c(\"flag_spike_dip\", \"flag_cont_spike_dip\", \"flag_zero_error\", \n                  \"flag_cont_zero_error\", \"flag_osc_lag\"))) %&gt;%\n  ensure_flag_columns()\n\nif(nrow(offender_anomalies) &gt; 0) {\n  for(i in 1:nrow(offender_anomalies)) {\n    row &lt;- offender_anomalies[i, ]\n    anomaly_record &lt;- create_anomaly_record(\n      metric_name = \"Offender Profiles\",\n      jurisdiction = row$jurisdiction,\n      date = row$capture_datetime,\n      value = row$offender_profiles,\n      flag_spike_dip = row$flag_spike_dip,\n      flag_cont_spike_dip = row$flag_cont_spike_dip,\n      flag_zero_error = row$flag_zero_error,\n      flag_cont_zero_error = row$flag_cont_zero_error,\n      flag_osc_lag = row$flag_osc_lag,\n      prev_value = row$prev_value,\n      next_value = row$next_value\n    )\n    anomaly_log &lt;- bind_rows(anomaly_log, anomaly_record)\n  }\n}\n\n### Forensic Profiles Anomalies\nforensic_anomalies &lt;- forensic_validation %&gt;%\n  filter(flag_any) %&gt;%\n  select(jurisdiction, capture_datetime, forensic_profiles, prev_value, next_value,\n         any_of(c(\"flag_spike_dip\", \"flag_cont_spike_dip\", \"flag_zero_error\", \n                  \"flag_cont_zero_error\", \"flag_osc_lag\"))) %&gt;%\n  ensure_flag_columns()\n\nif(nrow(forensic_anomalies) &gt; 0) {\n  for(i in 1:nrow(forensic_anomalies)) {\n    row &lt;- forensic_anomalies[i, ]\n    anomaly_record &lt;- create_anomaly_record(\n      metric_name = \"Forensic Profiles\",\n      jurisdiction = row$jurisdiction,\n      date = row$capture_datetime,\n      value = row$forensic_profiles,\n      flag_spike_dip = row$flag_spike_dip,\n      flag_cont_spike_dip = row$flag_cont_spike_dip,\n      flag_zero_error = row$flag_zero_error,\n      flag_cont_zero_error = row$flag_cont_zero_error,\n      flag_osc_lag = row$flag_osc_lag,\n      prev_value = row$prev_value,\n      next_value = row$next_value\n    )\n    anomaly_log &lt;- bind_rows(anomaly_log, anomaly_record)\n  }\n}\n\n### Arrestee Profiles Anomalies\narrestee_anomalies &lt;- arrestee_validation %&gt;%\n  filter(flag_any) %&gt;%\n  select(jurisdiction, capture_datetime, arrestee, prev_value, next_value,\n         any_of(c(\"flag_spike_dip\", \"flag_cont_spike_dip\", \"flag_zero_error\", \n                  \"flag_cont_zero_error\", \"flag_osc_lag\"))) %&gt;%\n  ensure_flag_columns()\n\nif(nrow(arrestee_anomalies) &gt; 0) {\n  for(i in 1:nrow(arrestee_anomalies)) {\n    row &lt;- arrestee_anomalies[i, ]\n    anomaly_record &lt;- create_anomaly_record(\n      metric_name = \"Arrestee Profiles\",\n      jurisdiction = row$jurisdiction,\n      date = row$capture_datetime,\n      value = row$arrestee,\n      flag_spike_dip = row$flag_spike_dip,\n      flag_cont_spike_dip = row$flag_cont_spike_dip,\n      flag_zero_error = row$flag_zero_error,\n      flag_cont_zero_error = row$flag_cont_zero_error,\n      flag_osc_lag = row$flag_osc_lag,\n      prev_value = row$prev_value,\n      next_value = row$next_value\n    )\n    anomaly_log &lt;- bind_rows(anomaly_log, anomaly_record)\n  }\n}\n\n### Investigations Aided Anomalies\ninvestigations_anomalies &lt;- investigations_validation %&gt;%\n  filter(flag_any) %&gt;%\n  select(jurisdiction, capture_datetime, investigations_aided, prev_value, next_value,\n         any_of(c(\"flag_spike_dip\", \"flag_cont_spike_dip\", \"flag_zero_error\", \n                  \"flag_cont_zero_error\", \"flag_osc_lag\"))) %&gt;%\n  ensure_flag_columns()\n\nif(nrow(investigations_anomalies) &gt; 0) {\n  for(i in 1:nrow(investigations_anomalies)) {\n    row &lt;- investigations_anomalies[i, ]\n    anomaly_record &lt;- create_anomaly_record(\n      metric_name = \"Investigations Aided\",\n      jurisdiction = row$jurisdiction,\n      date = row$capture_datetime,\n      value = row$investigations_aided,\n      flag_spike_dip = row$flag_spike_dip,\n      flag_cont_spike_dip = row$flag_cont_spike_dip,\n      flag_zero_error = row$flag_zero_error,\n      flag_cont_zero_error = row$flag_cont_zero_error,\n      flag_osc_lag = row$flag_osc_lag,\n      prev_value = row$prev_value,\n      next_value = row$next_value\n    )\n    anomaly_log &lt;- bind_rows(anomaly_log, anomaly_record)\n  }\n}\n\n### NDIS Labs Anomalies\nlabs_anomalies &lt;- labs_validation %&gt;%\n  filter(flag_any) %&gt;%\n  select(jurisdiction, capture_datetime, ndis_labs, prev_value, next_value,\n         any_of(c(\"flag_spike_dip\", \"flag_cont_spike_dip\", \"flag_zero_error\", \n                  \"flag_cont_zero_error\", \"flag_osc_lag\"))) %&gt;%\n  ensure_flag_columns()\n\nif(nrow(labs_anomalies) &gt; 0) {\n  for(i in 1:nrow(labs_anomalies)) {\n    row &lt;- labs_anomalies[i, ]\n    anomaly_record &lt;- create_anomaly_record(\n      metric_name = \"NDIS Labs\",\n      jurisdiction = row$jurisdiction,\n      date = row$capture_datetime,\n      value = row$ndis_labs,\n      flag_spike_dip = row$flag_spike_dip,\n      flag_cont_spike_dip = row$flag_cont_spike_dip,\n      flag_zero_error = row$flag_zero_error,\n      flag_cont_zero_error = row$flag_cont_zero_error,\n      flag_osc_lag = row$flag_osc_lag,\n      prev_value = row$prev_value,\n      next_value = row$next_value\n    )\n    anomaly_log &lt;- bind_rows(anomaly_log, anomaly_record)\n  }\n}\n\n### Set metric and anomaly type ordering for consistent display\nmetric_order &lt;- c(\"Offender Profiles\", \"Forensic Profiles\", \"Arrestee Profiles\", \n                  \"Investigations Aided\", \"NDIS Labs\")\nanomaly_type_order &lt;- c(\"spike_dip\", \"cont_spike_dip\", \"zero_error\", \"cont_zero_error\", \"osc_lag\")\n\n# Convert to factors with desired order\nanomaly_log &lt;- anomaly_log %&gt;%\n  mutate(\n    metric = factor(metric, levels = metric_order),\n    anomaly_type = factor(anomaly_type, levels = anomaly_type_order)\n  )\n\n### Create Summary Tables for Reporting\n\n# Overview of detected anomalies by metric\nanomaly_overview &lt;- anomaly_log %&gt;%\n  group_by(metric) %&gt;%\n  summarise(\n    total_anomalies = n(),\n    affected_jurisdictions = n_distinct(jurisdiction),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(metric)\n\nprint(knitr::kable(anomaly_overview, format = \"simple\", \n                   caption = \"Overview of Detected Anomalies by Metric\"))\n\n\n\n\nTable: Overview of Detected Anomalies by Metric\n\nmetric                  total_anomalies   affected_jurisdictions\n---------------------  ----------------  -----------------------\nOffender Profiles                  1290                       52\nForensic Profiles                  1011                       54\nArrestee Profiles                     6                        1\nInvestigations Aided               1002                       25\nNDIS Labs                            10                        2\n\n\nShow anomaly detection and logging code\n# Detailed summary by metric, jurisdiction, and anomaly type\nanomaly_detailed &lt;- anomaly_log %&gt;%\n  group_by(metric, jurisdiction, anomaly_type) %&gt;%\n  summarise(\n    count = n(),\n    earliest_date = min(date),\n    latest_date = max(date),\n    avg_value = mean(value, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(metric, jurisdiction, anomaly_type)\n\nprint(knitr::kable(anomaly_detailed, format = \"simple\",\n                   caption = \"Detailed Anomaly Summary by Metric, Jurisdiction, and Type\"))\n\n\n\n\nTable: Detailed Anomaly Summary by Metric, Jurisdiction, and Type\n\nmetric                 jurisdiction     anomaly_type       count  earliest_date         latest_date               avg_value\n---------------------  ---------------  ----------------  ------  --------------------  --------------------  -------------\nOffender Profiles      Alabama          spike_dip              4  2002-10-03 07:38:04   2015-01-07 16:29:32    1.187225e+04\nOffender Profiles      Alabama          cont_spike_dip         1  2014-12-26 09:29:17   2014-12-26 09:29:17    2.219200e+04\nOffender Profiles      Alabama          osc_lag                1  2008-09-16 15:14:50   2008-09-16 15:14:50    1.571870e+05\nOffender Profiles      Alabama          NA                    16  2001-07-15 04:15:59   2009-08-25 08:29:04    3.929675e+04\nOffender Profiles      Alaska           osc_lag                1  2008-09-16 15:09:02   2008-09-16 15:09:02    1.095400e+04\nOffender Profiles      Alaska           NA                     6  2008-05-24 12:00:21   2009-08-25 08:30:46    1.095400e+04\nOffender Profiles      Arizona          osc_lag                3  2008-09-16 15:04:19   2008-11-08 14:56:53    1.256920e+05\nOffender Profiles      Arizona          NA                     3  2008-07-09 09:37:20   2009-08-25 08:25:42    1.256920e+05\nOffender Profiles      Arkansas         osc_lag                1  2008-09-16 15:08:24   2008-09-16 15:08:24    8.791300e+04\nOffender Profiles      Arkansas         NA                     2  2008-07-09 09:33:18   2009-08-25 08:03:42    8.791300e+04\nOffender Profiles      California       spike_dip              2  2002-02-07 23:56:00   2002-10-03 07:29:55    2.882400e+04\nOffender Profiles      California       osc_lag                4  2008-09-15 03:04:04   2009-05-12 12:40:30    9.965390e+05\nOffender Profiles      California       NA                    25  2001-07-15 04:23:36   2010-04-09 22:37:24    6.052421e+05\nOffender Profiles      Colorado         osc_lag                3  2008-09-16 15:04:55   2009-05-12 12:40:53    8.436600e+04\nOffender Profiles      Colorado         NA                    13  2008-07-09 09:34:09   2010-04-09 22:40:08    8.436600e+04\nOffender Profiles      Connecticut      osc_lag                3  2008-09-16 15:06:28   2021-01-24 09:07:28    6.763433e+04\nOffender Profiles      Connecticut      NA                    10  2008-07-09 09:32:17   2021-02-14 00:46:27    9.670640e+04\nOffender Profiles      Delaware         spike_dip              1  2002-06-06 18:27:45   2002-06-06 18:27:45    0.000000e+00\nOffender Profiles      Delaware         osc_lag                1  2021-01-24 09:07:28   2021-01-24 09:07:28    1.970300e+04\nOffender Profiles      Delaware         NA                    20  2001-07-15 04:09:05   2021-02-14 00:46:27    6.896050e+03\nOffender Profiles      Florida          osc_lag                1  2008-09-16 15:21:04   2008-09-16 15:21:04    4.715620e+05\nOffender Profiles      Florida          NA                     4  2008-07-09 09:38:11   2009-08-25 08:21:28    4.715620e+05\nOffender Profiles      Hawaii           spike_dip              1  2004-08-05 06:45:30   2004-08-05 06:45:30    0.000000e+00\nOffender Profiles      Hawaii           osc_lag                1  2008-09-16 15:14:13   2008-09-16 15:14:13    9.247000e+03\nOffender Profiles      Hawaii           NA                    32  2001-07-15 04:14:32   2009-08-25 08:10:39    5.779375e+02\nOffender Profiles      Idaho            spike_dip              2  2001-12-29 00:12:07   2002-08-27 02:57:39    2.030000e+02\nOffender Profiles      Idaho            osc_lag                1  2015-04-30 12:57:08   2015-04-30 12:57:08    3.020400e+04\nOffender Profiles      Idaho            NA                    11  2001-07-15 04:19:55   2015-06-05 17:24:00    5.491636e+03\nOffender Profiles      Illinois         osc_lag                1  2008-09-16 15:08:48   2008-09-16 15:08:48    2.989750e+05\nOffender Profiles      Illinois         NA                     4  2008-07-09 09:37:29   2009-08-25 08:12:58    2.989750e+05\nOffender Profiles      Indiana          osc_lag                3  2005-03-02 11:44:05   2012-10-21 06:13:38    1.084710e+05\nOffender Profiles      Indiana          NA                    16  2005-03-06 12:50:22   2012-11-14 07:24:58    1.384814e+05\nOffender Profiles      Iowa             spike_dip              1  2002-06-06 18:33:59   2002-06-06 18:33:59    0.000000e+00\nOffender Profiles      Iowa             osc_lag                1  2008-09-16 15:19:51   2008-09-16 15:19:51    4.551900e+04\nOffender Profiles      Iowa             NA                     6  2001-09-13 04:17:46   2009-08-25 08:17:59    1.517300e+04\nOffender Profiles      Kansas           osc_lag                6  2005-03-02 11:46:08   2018-04-25 12:54:59    4.843417e+04\nOffender Profiles      Kansas           NA                    30  2005-03-06 12:51:20   2018-05-17 02:41:14    3.575687e+04\nOffender Profiles      Kentucky         osc_lag                1  2008-09-16 15:08:05   2008-09-16 15:08:05    1.314200e+04\nOffender Profiles      Kentucky         NA                     2  2008-07-09 09:33:43   2009-08-25 07:59:34    1.314200e+04\nOffender Profiles      Louisiana        spike_dip              3  2002-10-17 18:05:12   2003-12-23 15:11:16    2.071000e+03\nOffender Profiles      Louisiana        osc_lag                1  2006-10-04 09:04:03   2006-10-04 09:04:03    4.559100e+04\nOffender Profiles      Louisiana        NA                    15  2001-07-15 04:08:16   2006-11-01 10:14:46    1.836980e+04\nOffender Profiles      Maine            osc_lag                1  2008-09-16 15:18:03   2008-09-16 15:18:03    8.853000e+03\nOffender Profiles      Maine            NA                     3  2008-06-11 15:59:05   2009-08-25 08:00:50    8.853000e+03\nOffender Profiles      Maryland         spike_dip              1  2001-11-05 20:27:23   2001-11-05 20:27:23    2.466000e+03\nOffender Profiles      Maryland         osc_lag                3  2004-10-27 02:52:19   2008-09-16 15:15:40    3.522500e+04\nOffender Profiles      Maryland         NA                    29  2004-10-28 11:35:44   2009-08-25 07:52:59    2.432641e+04\nOffender Profiles      Massachusetts    spike_dip              6  2001-11-09 20:39:14   2007-05-09 05:37:43    1.714667e+03\nOffender Profiles      Massachusetts    cont_spike_dip         1  2004-09-29 16:25:10   2004-09-29 16:25:10    0.000000e+00\nOffender Profiles      Massachusetts    osc_lag                4  2001-11-05 20:25:21   2009-01-14 20:15:24    3.267475e+04\nOffender Profiles      Massachusetts    NA                    51  2001-07-15 04:11:54   2009-08-25 08:28:33    4.101412e+03\nOffender Profiles      Michigan         spike_dip              9  2001-07-15 04:16:43   2008-05-23 18:17:46    1.959622e+04\nOffender Profiles      Michigan         osc_lag                2  2008-09-16 15:05:50   2024-07-13 14:39:31    3.166135e+05\nOffender Profiles      Michigan         NA                     8  2008-07-09 09:34:50   2024-11-28 02:53:27    3.533067e+05\nOffender Profiles      Minnesota        osc_lag                5  2008-09-16 15:04:36   2021-05-07 09:09:54    1.304742e+05\nOffender Profiles      Minnesota        NA                    18  2008-07-09 09:37:58   2021-05-27 10:08:01    1.506762e+05\nOffender Profiles      Mississippi      spike_dip              2  2004-08-05 06:44:45   2008-04-09 08:53:33    3.231500e+03\nOffender Profiles      Mississippi      osc_lag                1  2008-09-16 15:19:34   2008-09-16 15:19:34    1.992700e+04\nOffender Profiles      Mississippi      NA                    32  2001-07-15 04:24:22   2009-08-25 08:17:29    2.255281e+03\nOffender Profiles      Missouri         osc_lag                1  2008-09-16 15:21:16   2008-09-16 15:21:16    1.625010e+05\nOffender Profiles      Missouri         NA                     2  2008-07-09 09:38:04   2009-08-25 08:07:44    1.625010e+05\nOffender Profiles      Montana          spike_dip              4  2002-02-08 02:44:21   2010-04-09 22:38:43    4.443000e+03\nOffender Profiles      Montana          osc_lag                4  2008-09-16 15:15:06   2019-05-02 13:50:53    1.421725e+04\nOffender Profiles      Montana          NA                    23  2001-07-15 04:24:31   2019-05-11 00:03:25    9.375130e+03\nOffender Profiles      Nebraska         spike_dip              3  2011-04-10 20:28:36   2017-05-16 14:40:07    2.674367e+04\nOffender Profiles      Nebraska         cont_spike_dip         1  2017-05-16 20:14:25   2017-05-16 20:14:25    3.762800e+04\nOffender Profiles      Nebraska         osc_lag                2  2009-01-14 20:19:16   2009-05-12 12:39:49    4.019000e+03\nOffender Profiles      Nebraska         NA                    19  2008-07-09 09:35:43   2017-05-20 05:02:54    7.556789e+03\nOffender Profiles      Nevada           osc_lag                2  2008-09-16 15:06:46   2021-01-24 09:07:28    6.589250e+04\nOffender Profiles      Nevada           NA                     9  2008-07-09 09:35:04   2021-02-14 00:46:27    8.471778e+04\nOffender Profiles      New Hampshire    spike_dip              2  2003-12-23 15:10:10   2004-12-29 03:27:43    2.050000e+01\nOffender Profiles      New Hampshire    osc_lag                1  2008-09-16 15:07:27   2008-09-16 15:07:27    1.803000e+03\nOffender Profiles      New Hampshire    NA                    49  2001-07-15 04:12:24   2009-08-25 08:11:06    1.045510e+02\nOffender Profiles      New Jersey       osc_lag                7  2008-10-12 11:31:42   2010-07-12 18:11:34    1.673140e+05\nOffender Profiles      New Jersey       NA                    15  2008-07-09 09:34:57   2010-08-19 16:15:27    1.673140e+05\nOffender Profiles      New Mexico       spike_dip              1  2001-09-13 19:14:45   2001-09-13 19:14:45    2.899000e+03\nOffender Profiles      New Mexico       osc_lag                3  2008-09-16 15:03:11   2009-05-12 12:39:43    4.632400e+04\nOffender Profiles      New Mexico       NA                    17  2001-07-15 04:14:54   2010-04-09 22:40:25    3.610635e+04\nOffender Profiles      New York         osc_lag                1  2008-09-16 15:13:11   2008-09-16 15:13:11    2.657890e+05\nOffender Profiles      New York         NA                     3  2008-07-09 09:34:31   2010-01-18 20:33:35    2.657890e+05\nOffender Profiles      North Carolina   spike_dip              2  2017-04-09 20:26:27   2017-05-13 04:05:12    1.213300e+04\nOffender Profiles      North Carolina   cont_spike_dip         1  2017-05-05 05:42:22   2017-05-05 05:42:22    1.213300e+04\nOffender Profiles      North Carolina   osc_lag                1  2008-09-16 15:14:33   2008-09-16 15:14:33    1.410710e+05\nOffender Profiles      North Carolina   NA                     2  2008-07-09 09:32:04   2009-08-25 08:14:39    1.410710e+05\nOffender Profiles      North Dakota     spike_dip              4  2012-09-21 12:24:19   2017-05-16 14:40:07    1.079200e+04\nOffender Profiles      North Dakota     cont_spike_dip         2  2012-10-22 16:23:30   2017-05-16 20:14:25    1.092100e+04\nOffender Profiles      North Dakota     osc_lag                3  2008-09-16 15:12:02   2009-05-12 12:39:32    4.553000e+03\nOffender Profiles      North Dakota     NA                    30  2008-07-09 09:32:24   2017-05-20 05:02:54    7.548467e+03\nOffender Profiles      Ohio             osc_lag                2  2008-09-16 15:07:13   2021-09-24 18:42:45    3.743470e+05\nOffender Profiles      Ohio             NA                    11  2008-07-09 09:32:44   2021-11-14 09:28:57    4.414827e+05\nOffender Profiles      Oklahoma         spike_dip              6  2002-08-27 03:04:00   2008-09-20 14:18:46    4.497250e+04\nOffender Profiles      Oklahoma         cont_spike_dip         1  2008-10-12 11:50:00   2008-10-12 11:50:00    7.132500e+04\nOffender Profiles      Oklahoma         osc_lag                2  2009-01-14 20:17:02   2009-05-12 12:41:21    5.396700e+04\nOffender Profiles      Oklahoma         NA                    20  2001-07-15 04:21:52   2010-04-09 22:42:35    4.134315e+04\nOffender Profiles      Oregon           osc_lag                4  2004-08-11 20:52:55   2009-05-12 12:40:17    7.766600e+04\nOffender Profiles      Oregon           NA                    26  2004-08-12 02:11:07   2010-04-09 22:38:24    6.859400e+04\nOffender Profiles      Pennsylvania     osc_lag                1  2008-09-16 15:12:20   2008-09-16 15:12:20    1.787080e+05\nOffender Profiles      Pennsylvania     NA                     2  2008-07-09 09:35:23   2009-08-25 07:57:42    1.787080e+05\nOffender Profiles      Rhode Island     spike_dip              2  2003-12-23 15:09:03   2009-08-25 07:54:13    1.109500e+03\nOffender Profiles      Rhode Island     osc_lag                1  2008-09-16 15:16:45   2008-09-16 15:16:45    2.219000e+03\nOffender Profiles      Rhode Island     NA                    13  2001-09-13 00:18:10   2008-07-09 09:36:54    1.706923e+02\nOffender Profiles      South Carolina   spike_dip              1  2002-08-27 02:42:17   2002-08-27 02:42:17    1.658000e+03\nOffender Profiles      South Carolina   osc_lag                3  2004-09-19 07:01:32   2015-04-30 12:57:08    1.100120e+05\nOffender Profiles      South Carolina   NA                    12  2001-07-15 04:10:37   2015-05-04 10:01:16    4.617083e+04\nOffender Profiles      South Dakota     spike_dip              1  2002-10-17 18:37:05   2002-10-17 18:37:05    0.000000e+00\nOffender Profiles      South Dakota     osc_lag                3  2008-09-16 15:19:06   2009-05-12 12:40:58    1.554900e+04\nOffender Profiles      South Dakota     NA                    20  2001-07-15 04:13:01   2010-04-09 22:41:08    1.010685e+04\nOffender Profiles      Tennessee        spike_dip              1  2002-02-08 03:56:33   2002-02-08 03:56:33    0.000000e+00\nOffender Profiles      Tennessee        osc_lag                7  2008-09-16 15:13:59   2019-10-26 04:32:49    1.710951e+05\nOffender Profiles      Tennessee        NA                    29  2001-07-15 04:11:23   2019-11-24 09:52:29    1.584466e+05\nOffender Profiles      Texas            osc_lag                1  2008-09-16 15:06:04   2008-09-16 15:06:04    3.493860e+05\nOffender Profiles      Texas            NA                     3  2008-07-09 09:33:12   2009-08-25 08:19:46    3.493860e+05\nOffender Profiles      Utah             osc_lag                2  2008-09-16 15:12:34   2014-02-09 00:23:04    5.668850e+04\nOffender Profiles      Utah             NA                     4  2008-06-11 15:58:09   2014-02-09 23:32:30    4.115775e+04\nOffender Profiles      Vermont          spike_dip              1  2002-02-08 04:05:43   2002-02-08 04:05:43    2.070000e+02\nOffender Profiles      Vermont          osc_lag                7  2008-09-16 15:18:25   2021-09-24 18:42:45    1.794729e+04\nOffender Profiles      Vermont          NA                    36  2001-08-22 05:47:08   2021-11-14 09:28:57    1.410894e+04\nOffender Profiles      Virginia         osc_lag                1  2008-09-16 15:05:19   2008-09-16 15:05:19    2.727530e+05\nOffender Profiles      Virginia         NA                     2  2008-07-09 09:36:40   2009-08-25 08:09:15    2.727530e+05\nOffender Profiles      Washington       osc_lag                1  2008-09-16 15:18:43   2008-09-16 15:18:43    1.190070e+05\nOffender Profiles      Washington       NA                     4  2008-07-09 09:36:25   2009-08-25 08:24:54    1.190070e+05\nOffender Profiles      West Virginia    spike_dip             10  2001-09-14 21:17:13   2008-04-09 09:00:28    4.317000e+02\nOffender Profiles      West Virginia    cont_spike_dip         1  2005-02-03 08:04:30   2005-02-03 08:04:30    1.630000e+02\nOffender Profiles      West Virginia    NA                   118  2001-07-15 04:05:59   2007-02-14 08:03:09    2.293729e+02\nOffender Profiles      Wisconsin        osc_lag                1  2008-09-16 15:13:41   2008-09-16 15:13:41    1.104870e+05\nOffender Profiles      Wisconsin        NA                     2  2008-07-09 09:35:30   2009-08-25 07:50:46    1.104870e+05\nOffender Profiles      Wyoming          spike_dip              8  2004-10-27 02:53:45   2010-04-09 22:39:20    3.914750e+03\nOffender Profiles      Wyoming          cont_spike_dip         2  2004-10-28 11:44:56   2007-06-13 05:59:56    4.380000e+02\nOffender Profiles      Wyoming          osc_lag                4  2004-08-11 20:54:11   2015-01-20 15:58:13    9.540750e+03\nOffender Profiles      Wyoming          NA                   185  2001-07-15 04:06:47   2015-02-14 03:59:09    2.893459e+03\nOffender Profiles      Puerto Rico      spike_dip              1  2010-09-02 17:21:59   2010-09-02 17:21:59    0.000000e+00\nOffender Profiles      Puerto Rico      NA                     7  2010-01-05 05:42:22   2010-08-19 01:20:22    0.000000e+00\nOffender Profiles      DC/FBI Lab       spike_dip              4  2002-08-27 02:41:23   2025-02-01 15:23:05    4.706662e+05\nOffender Profiles      DC/FBI Lab       cont_spike_dip         1  2025-01-16 20:53:11   2025-01-16 20:53:11    9.408700e+05\nOffender Profiles      DC/FBI Lab       osc_lag                1  2008-09-16 15:17:24   2008-09-16 15:17:24    6.125000e+04\nOffender Profiles      DC/FBI Lab       NA                    49  2001-07-15 04:08:23   2025-01-17 08:45:27    2.194880e+04\nOffender Profiles      U.S. Army        spike_dip              4  2001-12-29 01:16:26   2007-12-12 09:05:10    1.428750e+03\nOffender Profiles      U.S. Army        NA                     9  2001-08-22 11:58:53   2001-11-05 16:10:38    0.000000e+00\nForensic Profiles      Alabama          spike_dip              5  2002-10-03 07:38:04   2005-01-23 03:43:06    7.540000e+01\nForensic Profiles      Alabama          osc_lag                1  2008-09-16 15:14:50   2008-09-16 15:14:50    3.564000e+03\nForensic Profiles      Alabama          NA                    66  2001-07-15 04:15:59   2009-08-25 08:29:04    3.408788e+02\nForensic Profiles      Alaska           spike_dip              1  2003-02-02 14:56:19   2003-02-02 14:56:19    7.500000e+01\nForensic Profiles      Alaska           osc_lag                1  2009-03-11 03:19:28   2009-03-11 03:19:28    7.040000e+02\nForensic Profiles      Alaska           NA                     3  2009-01-14 20:15:24   2009-03-11 16:25:59    7.040000e+02\nForensic Profiles      Arizona          osc_lag                1  2008-09-16 15:04:19   2008-09-16 15:04:19    6.783000e+03\nForensic Profiles      Arizona          NA                     5  2008-07-09 09:37:20   2009-08-25 08:25:42    6.783000e+03\nForensic Profiles      Arkansas         osc_lag                1  2008-09-16 15:08:24   2008-09-16 15:08:24    2.019000e+03\nForensic Profiles      Arkansas         NA                     2  2008-07-09 09:33:18   2009-08-25 08:03:42    2.019000e+03\nForensic Profiles      California       spike_dip              1  2003-02-02 15:06:53   2003-02-02 15:06:53    1.029000e+03\nForensic Profiles      California       osc_lag                3  2008-09-15 03:04:04   2009-05-12 12:40:30    1.637300e+04\nForensic Profiles      California       NA                    16  2008-07-09 09:34:44   2010-04-09 22:37:24    1.637300e+04\nForensic Profiles      Colorado         osc_lag                3  2008-09-16 15:04:55   2014-02-09 00:23:04    5.236667e+03\nForensic Profiles      Colorado         NA                    15  2008-07-09 09:34:09   2014-02-09 23:32:30    3.743333e+03\nForensic Profiles      Connecticut      osc_lag                1  2008-09-16 15:06:28   2008-09-16 15:06:28    2.097000e+03\nForensic Profiles      Connecticut      NA                     2  2008-07-09 09:32:17   2009-08-25 08:11:56    2.097000e+03\nForensic Profiles      Delaware         spike_dip              4  2002-06-06 18:27:45   2006-08-03 02:40:43    4.867500e+02\nForensic Profiles      Delaware         cont_spike_dip         1  2006-08-09 15:31:06   2006-08-09 15:31:06    1.700000e+02\nForensic Profiles      Delaware         osc_lag                2  2008-09-16 15:15:22   2009-03-11 03:19:28    2.600000e+02\nForensic Profiles      Delaware         NA                    31  2001-07-15 04:09:05   2009-08-25 08:15:26    2.521935e+02\nForensic Profiles      Florida          osc_lag                1  2008-09-16 15:21:04   2008-09-16 15:21:04    1.871500e+04\nForensic Profiles      Florida          NA                     4  2008-07-09 09:38:11   2009-08-25 08:21:28    1.871500e+04\nForensic Profiles      Georgia          osc_lag                1  2008-09-16 15:17:05   2008-09-16 15:17:05    6.418000e+03\nForensic Profiles      Georgia          NA                     2  2008-07-09 09:37:07   2009-08-25 08:06:05    6.418000e+03\nForensic Profiles      Hawaii           spike_dip              1  2003-09-19 04:30:50   2003-09-19 04:30:50    0.000000e+00\nForensic Profiles      Hawaii           osc_lag                1  2008-09-16 15:14:13   2008-09-16 15:14:13    1.340000e+02\nForensic Profiles      Hawaii           NA                    19  2001-07-15 04:14:32   2009-08-25 08:10:39    1.410526e+01\nForensic Profiles      Idaho            spike_dip              1  2001-12-29 00:12:07   2001-12-29 00:12:07    0.000000e+00\nForensic Profiles      Idaho            osc_lag                1  2008-09-16 15:13:22   2008-09-16 15:13:22    1.860000e+02\nForensic Profiles      Idaho            NA                    12  2001-07-15 04:19:55   2009-08-25 07:53:31    4.650000e+01\nForensic Profiles      Illinois         osc_lag                1  2008-09-16 15:08:48   2008-09-16 15:08:48    1.539700e+04\nForensic Profiles      Illinois         NA                     4  2008-07-09 09:37:29   2009-08-25 08:12:58    1.539700e+04\nForensic Profiles      Indiana          spike_dip              5  2001-07-15 04:25:09   2007-06-08 23:40:56    3.276600e+03\nForensic Profiles      Indiana          cont_spike_dip         1  2007-06-13 05:58:14   2007-06-13 05:58:14    2.190000e+03\nForensic Profiles      Indiana          osc_lag                1  2008-09-16 15:16:23   2008-09-16 15:16:23    3.409000e+03\nForensic Profiles      Indiana          NA                     3  2007-04-05 08:51:55   2009-08-25 08:32:20    2.916000e+03\nForensic Profiles      Iowa             spike_dip              1  2002-06-06 18:33:59   2002-06-06 18:33:59    0.000000e+00\nForensic Profiles      Iowa             osc_lag                1  2008-09-16 15:19:51   2008-09-16 15:19:51    2.083000e+03\nForensic Profiles      Iowa             NA                     6  2001-09-13 04:17:46   2009-08-25 08:17:59    6.943333e+02\nForensic Profiles      Kansas           spike_dip              1  2002-10-17 18:01:57   2002-10-17 18:01:57    3.100000e+02\nForensic Profiles      Kansas           osc_lag                2  2008-09-16 15:03:27   2009-05-12 12:39:54    2.524000e+03\nForensic Profiles      Kansas           NA                    14  2008-07-09 09:33:49   2010-04-09 22:42:22    2.524000e+03\nForensic Profiles      Kentucky         osc_lag                1  2008-09-16 15:08:05   2008-09-16 15:08:05    2.398000e+03\nForensic Profiles      Kentucky         NA                     2  2008-07-09 09:33:43   2009-08-25 07:59:34    2.398000e+03\nForensic Profiles      Louisiana        spike_dip              5  2002-04-13 05:47:33   2003-08-03 13:36:09    3.680000e+01\nForensic Profiles      Louisiana        osc_lag                1  2008-09-16 15:20:50   2008-09-16 15:20:50    3.923000e+03\nForensic Profiles      Louisiana        NA                     9  2001-07-15 04:08:16   2009-08-25 08:29:57    8.717778e+02\nForensic Profiles      Maine            spike_dip              3  2006-09-06 20:55:22   2006-10-04 09:02:34    2.398667e+03\nForensic Profiles      Maine            cont_spike_dip         1  2006-10-09 04:01:30   2006-10-09 04:01:30    1.130000e+03\nForensic Profiles      Maine            osc_lag                1  2008-09-16 15:18:03   2008-09-16 15:18:03    1.542000e+03\nForensic Profiles      Maine            NA                    17  2006-08-03 02:44:50   2009-08-25 08:00:50    1.841294e+03\nForensic Profiles      Maryland         spike_dip              1  2002-10-03 07:23:52   2002-10-03 07:23:52    3.420000e+02\nForensic Profiles      Maryland         osc_lag                1  2008-09-16 15:15:40   2008-09-16 15:15:40    4.351000e+03\nForensic Profiles      Maryland         NA                     3  2002-08-27 05:55:52   2009-08-25 07:52:59    3.014667e+03\nForensic Profiles      Massachusetts    osc_lag                1  2008-09-16 15:04:08   2008-09-16 15:04:08    3.173000e+03\nForensic Profiles      Massachusetts    NA                     2  2008-07-09 09:35:10   2009-08-25 08:28:33    3.173000e+03\nForensic Profiles      Michigan         spike_dip             15  2001-11-05 20:35:14   2008-05-23 18:17:46    2.414200e+03\nForensic Profiles      Michigan         osc_lag                1  2008-09-16 15:05:50   2008-09-16 15:05:50    7.125000e+03\nForensic Profiles      Michigan         NA                     7  2001-07-15 04:16:43   2009-08-25 08:16:17    4.050857e+03\nForensic Profiles      Minnesota        osc_lag                1  2008-09-16 15:04:36   2008-09-16 15:04:36    3.990000e+03\nForensic Profiles      Minnesota        NA                     3  2008-07-09 09:37:58   2009-08-25 08:33:31    3.990000e+03\nForensic Profiles      Mississippi      spike_dip              2  2005-01-23 03:41:58   2005-02-23 11:06:07    5.000000e-01\nForensic Profiles      Mississippi      osc_lag                1  2008-09-16 15:19:34   2008-09-16 15:19:34    2.010000e+02\nForensic Profiles      Mississippi      NA                    77  2001-07-15 04:24:22   2009-08-25 08:17:29    5.311688e+00\nForensic Profiles      Missouri         osc_lag                1  2008-09-16 15:21:16   2008-09-16 15:21:16    6.680000e+03\nForensic Profiles      Missouri         NA                     2  2008-07-09 09:38:04   2009-08-25 08:07:44    6.680000e+03\nForensic Profiles      Montana          spike_dip              2  2002-10-03 07:38:42   2003-02-13 00:59:12    9.000000e+00\nForensic Profiles      Montana          osc_lag                2  2008-09-16 15:15:06   2009-05-12 12:41:15    2.120000e+02\nForensic Profiles      Montana          NA                    25  2001-07-15 04:24:31   2010-04-09 22:38:43    1.187200e+02\nForensic Profiles      Nebraska         spike_dip              2  2001-09-14 21:16:46   2002-02-08 02:55:59    7.000000e+00\nForensic Profiles      Nebraska         osc_lag                2  2008-09-16 15:09:35   2009-05-12 12:39:49    4.380000e+02\nForensic Profiles      Nebraska         NA                    23  2001-07-15 04:09:23   2010-04-09 22:42:09    2.672174e+02\nForensic Profiles      Nevada           spike_dip              1  2002-02-08 03:35:36   2002-02-08 03:35:36    1.170000e+02\nForensic Profiles      Nevada           osc_lag                1  2008-09-16 15:06:46   2008-09-16 15:06:46    2.094000e+03\nForensic Profiles      Nevada           NA                     3  2001-12-29 00:11:15   2009-08-25 08:02:14    1.435000e+03\nForensic Profiles      New Hampshire    spike_dip              2  2003-02-13 01:19:05   2003-06-25 06:58:07    5.500000e+00\nForensic Profiles      New Hampshire    osc_lag                1  2008-09-16 15:07:27   2008-09-16 15:07:27    4.550000e+02\nForensic Profiles      New Hampshire    NA                    10  2001-07-15 04:12:24   2009-08-25 08:11:06    9.100000e+01\nForensic Profiles      New Jersey       osc_lag                5  2008-10-12 11:31:42   2010-07-12 18:11:34    5.669000e+03\nForensic Profiles      New Jersey       NA                    17  2008-07-09 09:34:57   2010-08-19 16:15:27    5.669000e+03\nForensic Profiles      New Mexico       osc_lag                2  2008-09-16 15:03:11   2009-05-12 12:39:43    1.540000e+03\nForensic Profiles      New Mexico       NA                    14  2008-07-09 09:36:05   2010-04-09 22:40:25    1.540000e+03\nForensic Profiles      New York         spike_dip              1  2001-12-29 00:31:51   2001-12-29 00:31:51    2.234000e+03\nForensic Profiles      New York         osc_lag                1  2008-09-16 15:13:11   2008-09-16 15:13:11    2.217300e+04\nForensic Profiles      New York         NA                     3  2008-07-09 09:34:31   2010-01-18 20:33:35    2.217300e+04\nForensic Profiles      North Carolina   osc_lag                1  2008-09-16 15:14:33   2008-09-16 15:14:33    3.427000e+03\nForensic Profiles      North Carolina   NA                     2  2008-07-09 09:32:04   2009-08-25 08:14:39    3.427000e+03\nForensic Profiles      North Dakota     spike_dip              1  2003-02-13 01:07:34   2003-02-13 01:07:34    0.000000e+00\nForensic Profiles      North Dakota     osc_lag                2  2008-09-16 15:12:02   2009-05-12 12:39:32    2.160000e+02\nForensic Profiles      North Dakota     NA                    23  2001-07-15 04:06:22   2010-04-09 22:40:43    1.314783e+02\nForensic Profiles      Ohio             spike_dip              1  2002-02-08 03:05:31   2002-02-08 03:05:31    4.810000e+02\nForensic Profiles      Ohio             osc_lag                1  2008-09-16 15:07:13   2008-09-16 15:07:13    1.399200e+04\nForensic Profiles      Ohio             NA                     3  2001-12-29 00:29:54   2009-08-25 08:26:43    9.488333e+03\nForensic Profiles      Oklahoma         spike_dip              9  2002-08-27 03:04:00   2008-09-20 14:18:46    5.199556e+03\nForensic Profiles      Oklahoma         cont_spike_dip         1  2008-10-12 11:50:00   2008-10-12 11:50:00    1.042000e+03\nForensic Profiles      Oklahoma         osc_lag                1  2009-05-12 12:41:21   2009-05-12 12:41:21    9.640000e+02\nForensic Profiles      Oklahoma         NA                    19  2001-07-15 04:21:52   2010-04-09 22:42:35    7.144211e+02\nForensic Profiles      Oregon           osc_lag                2  2008-09-16 15:10:51   2009-05-12 12:40:17    5.161000e+03\nForensic Profiles      Oregon           NA                    14  2008-07-09 09:35:17   2010-04-09 22:38:24    5.161000e+03\nForensic Profiles      Pennsylvania     osc_lag                1  2008-09-16 15:12:20   2008-09-16 15:12:20    5.483000e+03\nForensic Profiles      Pennsylvania     NA                     2  2008-07-09 09:35:23   2009-08-25 07:57:42    5.483000e+03\nForensic Profiles      Rhode Island     spike_dip              1  2003-12-23 15:09:03   2003-12-23 15:09:03    0.000000e+00\nForensic Profiles      Rhode Island     NA                    12  2001-09-13 00:18:10   2003-08-03 15:26:50    0.000000e+00\nForensic Profiles      South Carolina   spike_dip              2  2003-02-02 15:11:34   2003-06-25 02:19:18    2.385000e+02\nForensic Profiles      South Carolina   osc_lag                1  2008-09-16 15:07:48   2008-09-16 15:07:48    3.888000e+03\nForensic Profiles      South Carolina   NA                     3  2008-07-09 09:32:38   2010-10-07 21:01:44    3.888000e+03\nForensic Profiles      South Dakota     spike_dip              3  2002-10-17 18:37:05   2003-08-03 18:38:24    4.333333e+00\nForensic Profiles      South Dakota     osc_lag                1  2009-05-12 12:40:58   2009-05-12 12:40:58    1.980000e+02\nForensic Profiles      South Dakota     NA                    25  2001-07-15 04:13:01   2010-04-09 22:41:08    1.425600e+02\nForensic Profiles      Tennessee        spike_dip              2  2002-02-08 03:56:33   2002-04-13 04:34:36    1.165000e+02\nForensic Profiles      Tennessee        osc_lag                3  2008-09-16 15:13:59   2012-09-27 16:19:52    2.651333e+03\nForensic Profiles      Tennessee        NA                    14  2001-07-15 04:11:23   2012-10-18 12:40:13    2.028357e+03\nForensic Profiles      Texas            osc_lag                1  2008-09-16 15:06:04   2008-09-16 15:06:04    1.767400e+04\nForensic Profiles      Texas            NA                     3  2008-07-09 09:33:12   2009-08-25 08:19:46    1.767400e+04\nForensic Profiles      Utah             osc_lag                1  2008-09-16 15:12:34   2008-09-16 15:12:34    3.270000e+02\nForensic Profiles      Utah             NA                     2  2008-07-09 09:36:18   2009-08-25 07:56:01    3.270000e+02\nForensic Profiles      Vermont          spike_dip              4  2001-07-15 04:19:37   2005-07-14 12:01:40    1.150000e+01\nForensic Profiles      Vermont          osc_lag                1  2008-09-16 15:18:25   2008-09-16 15:18:25    2.160000e+02\nForensic Profiles      Vermont          NA                    18  2001-08-22 05:47:08   2009-08-25 08:24:09    3.688889e+01\nForensic Profiles      Virginia         spike_dip              1  2002-10-03 07:25:03   2002-10-03 07:25:03    3.720000e+02\nForensic Profiles      Virginia         osc_lag                1  2008-09-16 15:05:19   2008-09-16 15:05:19    8.973000e+03\nForensic Profiles      Virginia         NA                     2  2008-07-09 09:36:40   2009-08-25 08:09:15    8.973000e+03\nForensic Profiles      Washington       spike_dip              2  2007-09-12 10:04:11   2008-04-09 09:01:02    6.000000e+02\nForensic Profiles      Washington       cont_spike_dip         1  2007-10-10 11:28:54   2007-10-10 11:28:54    5.990000e+02\nForensic Profiles      Washington       osc_lag                1  2008-09-16 15:18:43   2008-09-16 15:18:43    1.672000e+03\nForensic Profiles      Washington       NA                    12  2007-11-14 12:45:59   2009-08-25 08:24:54    9.747500e+02\nForensic Profiles      West Virginia    spike_dip             13  2003-02-13 00:34:21   2008-05-14 09:20:10    1.278462e+02\nForensic Profiles      West Virginia    osc_lag                1  2013-05-11 07:10:18   2013-05-11 07:10:18    7.380000e+02\nForensic Profiles      West Virginia    NA                    17  2001-07-15 04:05:59   2013-05-24 01:53:37    1.172941e+02\nForensic Profiles      Wisconsin        osc_lag                1  2008-09-16 15:13:41   2008-09-16 15:13:41    4.642000e+03\nForensic Profiles      Wisconsin        NA                     2  2008-07-09 09:35:30   2009-08-25 07:50:46    4.642000e+03\nForensic Profiles      Wyoming          spike_dip              2  2007-04-11 11:45:53   2010-04-09 22:39:20    4.250000e+01\nForensic Profiles      Wyoming          osc_lag                1  2009-05-12 12:39:39   2009-05-12 12:39:39    8.100000e+01\nForensic Profiles      Wyoming          NA                   199  2001-07-15 04:06:47   2009-08-25 07:58:54    1.057789e+01\nForensic Profiles      Puerto Rico      spike_dip              4  2010-09-02 17:21:59   2013-06-01 10:41:14    5.425000e+01\nForensic Profiles      Puerto Rico      cont_spike_dip         1  2011-10-22 09:39:59   2011-10-22 09:39:59    3.300000e+01\nForensic Profiles      Puerto Rico      osc_lag                1  2013-05-11 07:10:18   2013-05-11 07:10:18    4.100000e+01\nForensic Profiles      Puerto Rico      NA                    13  2010-01-05 05:42:22   2013-05-24 01:53:37    2.492308e+01\nForensic Profiles      DC/FBI Lab       osc_lag                1  2008-09-16 15:17:24   2008-09-16 15:17:24    1.649000e+03\nForensic Profiles      DC/FBI Lab       NA                     2  2008-07-09 09:37:38   2009-08-25 08:04:28    1.649000e+03\nForensic Profiles      DC/Metro PD      osc_lag                1  2021-08-08 19:06:00   2021-08-08 19:06:00    3.386000e+03\nForensic Profiles      DC/Metro PD      NA                     1  2021-08-09 03:58:09   2021-08-09 03:58:09    3.386000e+03\nForensic Profiles      U.S. Army        spike_dip              1  2007-12-12 09:05:10   2007-12-12 09:05:10    9.630000e+02\nForensic Profiles      U.S. Army        osc_lag                2  2004-08-11 20:53:04   2004-10-27 02:52:13    9.275000e+02\nForensic Profiles      U.S. Army        NA                    25  2004-08-12 02:11:13   2004-12-29 03:27:20    9.272800e+02\nArrestee Profiles      California       zero_error             6  2015-02-09 22:49:06   2015-03-21 23:28:48    0.000000e+00\nInvestigations Aided   California       spike_dip              1  2025-01-05 16:40:14   2025-01-05 16:40:14    1.304657e+06\nInvestigations Aided   California       cont_spike_dip         1  2025-01-16 20:53:11   2025-01-16 20:53:11    1.304657e+06\nInvestigations Aided   Delaware         spike_dip              1  2004-04-16 01:38:23   2004-04-16 01:38:23    1.000000e+00\nInvestigations Aided   Delaware         cont_spike_dip         1  2004-06-05 15:11:55   2004-06-05 15:11:55    1.000000e+00\nInvestigations Aided   Delaware         NA                    73  2004-06-27 06:22:41   2005-05-21 05:05:36    1.000000e+00\nInvestigations Aided   Hawaii           spike_dip              1  2004-04-16 01:55:34   2004-04-16 01:55:34    1.000000e+00\nInvestigations Aided   Hawaii           cont_spike_dip         1  2004-06-05 15:29:06   2004-06-05 15:29:06    1.000000e+00\nInvestigations Aided   Hawaii           NA                    73  2004-06-27 06:29:36   2005-04-27 06:34:58    1.000000e+00\nInvestigations Aided   Idaho            spike_dip              1  2003-06-25 05:09:04   2003-06-25 05:09:04    4.000000e+00\nInvestigations Aided   Idaho            cont_spike_dip         1  2003-08-03 12:53:50   2003-08-03 12:53:50    4.000000e+00\nInvestigations Aided   Idaho            NA                   137  2003-12-23 15:10:52   2006-06-29 02:47:50    4.000000e+00\nInvestigations Aided   Indiana          spike_dip              1  2001-11-05 20:02:54   2001-11-05 20:02:54    5.700000e+01\nInvestigations Aided   Indiana          cont_spike_dip         1  2001-12-29 00:30:48   2001-12-29 00:30:48    6.200000e+01\nInvestigations Aided   Indiana          NA                     1  2002-02-08 01:48:48   2002-02-08 01:48:48    6.200000e+01\nInvestigations Aided   Iowa             spike_dip              1  2003-06-25 05:01:36   2003-06-25 05:01:36    2.000000e+00\nInvestigations Aided   Kansas           spike_dip              2  2003-12-23 15:11:03   2004-04-16 02:19:56    2.950000e+01\nInvestigations Aided   Kansas           cont_spike_dip         2  2004-02-05 20:09:39   2004-06-05 15:58:20    2.950000e+01\nInvestigations Aided   Kansas           NA                     9  2004-06-27 06:30:07   2004-08-05 06:45:40    5.400000e+01\nInvestigations Aided   Louisiana        spike_dip              1  2004-06-05 16:12:54   2004-06-05 16:12:54    3.250000e+02\nInvestigations Aided   Louisiana        cont_spike_dip         1  2004-06-27 06:30:39   2004-06-27 06:30:39    3.250000e+02\nInvestigations Aided   Louisiana        NA                    46  2004-06-30 09:15:59   2004-12-29 03:29:29    3.250000e+02\nInvestigations Aided   Massachusetts    spike_dip              2  2001-09-13 00:18:01   2002-06-06 18:12:43    5.000000e+01\nInvestigations Aided   Massachusetts    cont_spike_dip         2  2001-09-13 04:17:52   2002-08-27 02:24:08    5.000000e+01\nInvestigations Aided   Massachusetts    zero_error             1  2001-11-05 20:25:21   2001-11-05 20:25:21    0.000000e+00\nInvestigations Aided   Massachusetts    cont_zero_error        1  2001-11-09 20:39:14   2001-11-09 20:39:14    0.000000e+00\nInvestigations Aided   Massachusetts    NA                     4  2001-07-15 04:11:54   2002-02-08 02:10:24    7.000000e+00\nInvestigations Aided   Michigan         spike_dip              5  2008-01-03 18:55:32   2008-05-23 18:59:32    2.441800e+03\nInvestigations Aided   Michigan         cont_spike_dip         3  2008-01-09 18:34:28   2008-06-11 15:55:27    2.474333e+03\nInvestigations Aided   Michigan         NA                     1  2007-12-12 09:03:36   2007-12-12 09:03:36    2.369000e+03\nInvestigations Aided   Mississippi      spike_dip              1  2004-08-11 20:53:32   2004-08-11 20:53:32    1.000000e+00\nInvestigations Aided   Mississippi      cont_spike_dip         1  2004-08-12 02:11:37   2004-08-12 02:11:37    1.000000e+00\nInvestigations Aided   Mississippi      NA                    11  2004-08-12 07:54:48   2004-09-15 08:05:23    1.000000e+00\nInvestigations Aided   Montana          spike_dip              1  2003-02-13 00:59:12   2003-02-13 00:59:12    3.000000e+00\nInvestigations Aided   Nebraska         spike_dip              1  2002-10-17 18:17:12   2002-10-17 18:17:12    3.000000e+00\nInvestigations Aided   Nebraska         cont_spike_dip         1  2003-02-13 01:08:13   2003-02-13 01:08:13    4.000000e+00\nInvestigations Aided   New Hampshire    spike_dip              1  2003-06-25 06:58:07   2003-06-25 06:58:07    1.000000e+00\nInvestigations Aided   North Dakota     spike_dip              1  2004-06-05 17:15:24   2004-06-05 17:15:24    2.000000e+00\nInvestigations Aided   North Dakota     cont_spike_dip         1  2004-06-27 06:32:10   2004-06-27 06:32:10    2.000000e+00\nInvestigations Aided   North Dakota     NA                    71  2004-06-30 09:16:13   2005-04-27 06:36:12    2.000000e+00\nInvestigations Aided   Ohio             spike_dip              1  2006-07-05 15:59:59   2006-07-05 15:59:59    3.120950e+05\nInvestigations Aided   Ohio             cont_spike_dip         1  2006-07-13 13:28:58   2006-07-13 13:28:58    3.120950e+05\nInvestigations Aided   Ohio             NA                     2  2006-07-20 04:43:59   2006-07-27 08:26:47    3.120950e+05\nInvestigations Aided   Oklahoma         spike_dip              4  2002-10-17 18:29:16   2008-09-18 00:33:09    3.239750e+03\nInvestigations Aided   Oklahoma         cont_spike_dip         1  2003-02-02 14:56:52   2003-02-02 14:56:52    1.000000e+01\nInvestigations Aided   Oklahoma         NA                     1  2003-12-23 15:12:11   2003-12-23 15:12:11    1.000000e+01\nInvestigations Aided   Rhode Island     spike_dip              1  2005-05-27 04:35:27   2005-05-27 04:35:27    1.000000e+00\nInvestigations Aided   Rhode Island     cont_spike_dip         1  2005-06-02 05:03:11   2005-06-02 05:03:11    1.000000e+00\nInvestigations Aided   Rhode Island     NA                     2  2005-06-09 23:27:12   2005-06-18 02:45:49    1.000000e+00\nInvestigations Aided   South Carolina   spike_dip              1  2006-03-15 22:20:41   2006-03-15 22:20:41    3.750000e+02\nInvestigations Aided   South Carolina   cont_spike_dip         1  2006-03-23 00:42:15   2006-03-23 00:42:15    3.750000e+02\nInvestigations Aided   South Carolina   zero_error             1  2005-06-24 08:24:39   2005-06-24 08:24:39    0.000000e+00\nInvestigations Aided   South Carolina   cont_zero_error        1  2005-06-30 08:39:15   2005-06-30 08:39:15    0.000000e+00\nInvestigations Aided   South Carolina   NA                    35  2005-07-14 11:54:00   2006-03-10 12:19:46    0.000000e+00\nInvestigations Aided   South Dakota     spike_dip              1  2005-08-26 04:56:50   2005-08-26 04:56:50    2.000000e+00\nInvestigations Aided   South Dakota     cont_spike_dip         1  2005-09-16 10:28:00   2005-09-16 10:28:00    2.000000e+00\nInvestigations Aided   South Dakota     NA                    12  2005-09-23 21:59:14   2005-12-17 09:07:45    2.000000e+00\nInvestigations Aided   Vermont          spike_dip              2  2008-12-10 17:11:35   2009-09-03 17:21:06    4.900000e+01\nInvestigations Aided   Vermont          cont_spike_dip         2  2009-01-14 20:15:24   2009-09-09 15:58:21    4.900000e+01\nInvestigations Aided   Vermont          zero_error             1  2009-08-25 08:24:09   2009-08-25 08:24:09    0.000000e+00\nInvestigations Aided   Vermont          NA                   218  2001-07-15 04:19:37   2009-03-11 16:25:59    7.522936e-01\nInvestigations Aided   West Virginia    spike_dip              1  2002-10-17 18:52:08   2002-10-17 18:52:08    2.000000e+00\nInvestigations Aided   West Virginia    cont_spike_dip         1  2003-02-13 00:34:21   2003-02-13 00:34:21    2.000000e+00\nInvestigations Aided   West Virginia    NA                    57  2003-08-03 18:56:16   2005-01-23 03:41:51    2.000000e+00\nInvestigations Aided   Wisconsin        spike_dip              1  2010-03-17 12:00:23   2010-03-17 12:00:23    2.267000e+03\nInvestigations Aided   Wisconsin        cont_spike_dip         1  2010-04-09 22:36:56   2010-04-09 22:36:56    2.346000e+03\nInvestigations Aided   Wyoming          spike_dip              2  2003-02-13 00:31:36   2004-10-27 02:53:45    4.000000e+00\nInvestigations Aided   Wyoming          cont_spike_dip         2  2003-06-25 09:03:14   2004-10-28 11:44:56    4.000000e+00\nInvestigations Aided   Wyoming          zero_error             1  2004-08-11 20:54:11   2004-08-11 20:54:11    0.000000e+00\nInvestigations Aided   Wyoming          cont_zero_error        1  2004-08-12 02:13:10   2004-08-12 02:13:10    0.000000e+00\nInvestigations Aided   Wyoming          NA                   178  2001-07-15 04:06:47   2007-06-13 05:59:56    4.134831e+00\nInvestigations Aided   Puerto Rico      spike_dip              1  2011-01-11 09:38:35   2011-01-11 09:38:35    1.900000e+01\nInvestigations Aided   Puerto Rico      cont_spike_dip         1  2011-01-27 07:55:31   2011-01-27 07:55:31    2.000000e+01\nNDIS Labs              Michigan         spike_dip              5  2008-01-03 18:43:43   2008-05-23 18:17:46    1.000000e+00\nNDIS Labs              Michigan         cont_spike_dip         2  2008-03-12 06:43:20   2008-05-14 09:16:40    1.000000e+00\nNDIS Labs              Oklahoma         spike_dip              3  2008-08-13 17:45:04   2008-09-18 00:33:09    1.100000e+01\n\n\nShow anomaly detection and logging code\n### Publication-ready Visualization\n\n# Prepare data for plot\nanomaly_plot_data &lt;- anomaly_log %&gt;%\n  group_by(metric, jurisdiction) %&gt;%\n  summarise(count = n(), .groups = \"drop\") %&gt;%\n  mutate(metric = factor(metric, levels = metric_order))\n\n# Custom colors matching scheme\nmetric_colors &lt;- c(\n  \"Offender Profiles\"    = \"#1f4e79\",\n  \"Arrestee Profiles\"    = \"#2e75b6\",\n  \"Forensic Profiles\"    = \"#5b9bd5\",\n  \"Investigations Aided\" = \"#c00000\",\n  \"NDIS Labs\"            = \"#7030a0\"\n)\n\n# Create stacked bar plot\np_anomaly_distribution &lt;- ggplot(anomaly_plot_data, \n                                aes(x = jurisdiction, y = count, fill = metric)) +\n  geom_bar(stat = \"identity\", position = \"stack\", color = \"black\", \n           linewidth = 0.3, width = 1) +\n  scale_fill_manual(name = \"Metric\", values = metric_colors) +\n  scale_y_continuous(limits = c(0, NA), expand = c(0, 0)) +\n  theme_minimal(base_size = 10) +\n  theme(\n    panel.grid = element_blank(),\n    axis.line = element_line(color = \"black\", linewidth = 0.4),\n    axis.ticks = element_line(color = \"black\", linewidth = 0.4),\n    axis.text = element_text(color = \"black\", size = 12),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    axis.title.x = element_text(size = 20, face = \"bold\", margin = margin(t = 15)),\n    axis.title.y = element_text(size = 20, face = \"bold\", margin = margin(t = 15)),\n    axis.title = element_text(size = 18, face = \"bold\"),\n    legend.title = element_text(size = 20, face = \"bold\"),\n    legend.text = element_text(size = 15),\n    legend.key.size = unit(0.5, \"cm\"),\n    legend.key.width = unit(0.4, \"cm\"),\n    legend.key.height = unit(0.4, \"cm\"),\n    aspect.ratio = 0.65\n  ) +\n  labs(\n    title = \" \",\n    x = \"Jurisdiction\",\n    y = \"Number of Anomalies\"\n  )\n\np_anomaly_distribution\n\n\n\n\n\n\n\n\n\nShow anomaly detection and logging code\n# Anomaly type distribution\nanomaly_type_summary &lt;- anomaly_log %&gt;%\n  group_by(anomaly_type) %&gt;%\n  summarise(\n    count = n(),\n    metrics_affected = n_distinct(metric),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(anomaly_type)\n\nprint(knitr::kable(anomaly_type_summary, format = \"simple\",\n                   caption = \"Anomaly Count by Detection Rule\"))\n\n\n\n\nTable: Anomaly Count by Detection Rule\n\nanomaly_type       count   metrics_affected\n----------------  ------  -----------------\nspike_dip            231                  4\ncont_spike_dip        47                  4\nzero_error            10                  2\ncont_zero_error        3                  1\nosc_lag              186                  2\nNA                  2842                  3\n\n\nShow anomaly detection and logging code\n# Export anomaly log for archival and transparency\nanomaly_export_path &lt;- here(\"data\", \"ndis\", \"intermediate\", \"anomaly_log.csv\")\nwrite.csv(anomaly_log, anomaly_export_path, row.names = FALSE)\n\n\n\nCompiled Data Growth\nValidation Approach:\nÂ· Cross-validation across all metrics\nÂ· Yearly aggregation using maximum values\nÂ· Consistency checks between profile types\nÂ· Scale-appropriate visualization\nCorrection Methods:\nÂ· Unified compilation from cleaned sources\nÂ· Dual-axis visualization for different scales\nÂ· Analysis-ready dataset creation\nÂ· ComprehensiveÂ trendÂ analysis\n\n\nShow compiled data visualization and correction code\n# Get yearly data\ngrowth_data_yearly &lt;- ndis_clean %&gt;%\n  mutate(\n    offender_profiles = ifelse(is.na(offender_profiles), 0, offender_profiles),\n    arrestee = ifelse(is.na(arrestee), 0, arrestee),\n    forensic_profiles = ifelse(is.na(forensic_profiles), 0, forensic_profiles),\n    investigations_aided = ifelse(is.na(investigations_aided), 0, investigations_aided)\n  ) %&gt;%\n  # Get max per jurisdiction per year\n  group_by(year, jurisdiction) %&gt;%\n  summarise(\n    offender = max(offender_profiles, na.rm = TRUE),\n    arrestee = max(arrestee, na.rm = TRUE),\n    forensic = max(forensic_profiles, na.rm = TRUE),\n    investigations = max(investigations_aided, na.rm = TRUE),\n    ndis_labs = max(ndis_labs, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  # Sum across jurisdictions per year\n  group_by(year) %&gt;%\n  summarise(\n    jurisdictions = n(),\n    offender_total = sum(offender, na.rm = TRUE),\n    arrestee_total = sum(arrestee, na.rm = TRUE),\n    forensic_total = sum(forensic, na.rm = TRUE),\n    investigations_total = sum(investigations, na.rm = TRUE),\n    ndis_labs_total = sum(ndis_labs, na.rm = TRUE),\n    total_profiles = offender_total + arrestee_total + forensic_total,\n    .groups = 'drop'\n  ) %&gt;%\n  arrange(year) %&gt;%\n  mutate(date = as.Date(paste0(year, \"-01-01\")))\n\n# Calculate scale factor for dual y-axes\nmax_dna &lt;- max(c(growth_data_yearly$offender_total, \n                 growth_data_yearly$arrestee_total, \n                 growth_data_yearly$forensic_total), na.rm = TRUE)\nmax_investigations &lt;- max(growth_data_yearly$investigations_total, na.rm = TRUE)\nscale_factor &lt;- max_dna / max_investigations\n\n# Prepare data for plotting\ndna_data &lt;- growth_data_yearly %&gt;%\n  select(date, offender_total, arrestee_total, forensic_total) %&gt;%\n  pivot_longer(\n    cols = c(offender_total, arrestee_total, forensic_total),\n    names_to = \"variable\",\n    values_to = \"count\"\n  ) %&gt;%\n  mutate(\n    variable = case_when(\n      variable == \"offender_total\" ~ \"Offender\",\n      variable == \"arrestee_total\" ~ \"Arrestee\", \n      variable == \"forensic_total\" ~ \"Forensic\"\n    ),\n    count_scaled = count \n  )\n\ninvestigations_data &lt;- growth_data_yearly %&gt;%\n  select(date, investigations_total) %&gt;%\n  mutate(\n    variable = \"Investigations\",\n    count_scaled = investigations_total * scale_factor\n  )\n\n#### Interactive Plot ####\n\np_interactive &lt;- plot_ly() %&gt;%\n  # DNA Profiles\n  add_trace(\n    data = dna_data %&gt;% filter(variable == \"Offender\"),\n    x = ~date, y = ~count,\n    type = 'scatter', mode = 'lines+markers',\n    name = 'Offender Profiles',\n    line = list(color = '#0072B2', width = 2),\n    marker = list(color = '#0072B2', size = 6),\n    yaxis = 'y1'\n  ) %&gt;%\n  add_trace(\n    data = dna_data %&gt;% filter(variable == \"Arrestee\"),\n    x = ~date, y = ~count,\n    type = 'scatter', mode = 'lines+markers',\n    name = 'Arrestee Profiles',\n    line = list(color = '#D55E00', width = 2),\n    marker = list(color = '#D55E00', size = 6),\n    yaxis = 'y1'\n  ) %&gt;%\n  add_trace(\n    data = dna_data %&gt;% filter(variable == \"Forensic\"),\n    x = ~date, y = ~count,\n    type = 'scatter', mode = 'lines+markers',\n    name = 'Forensic Profiles',\n    line = list(color = '#009E73', width = 2),\n    marker = list(color = '#009E73', size = 6),\n    yaxis = 'y1'\n  ) %&gt;%\n  # Investigations Aided\n  add_trace(\n    data = investigations_data,\n    x = ~date, y = ~investigations_total,\n    type = 'scatter', mode = 'lines+markers',\n    name = 'Investigations Aided',\n    line = list(color = '#CC79A7', width = 2),\n    marker = list(color = '#CC79A7', size = 6),\n    yaxis = 'y2'\n  ) %&gt;%\n  layout(\n    title = \"DNA Profiles and Investigations Aided Over Time (Yearly)\",\n    xaxis = list(\n      title = \"Year\",\n      tickformat = \"%Y\"\n    ),\n    yaxis = list(\n      title = \"DNA Profiles\",\n      side = 'left',\n      showgrid = TRUE,\n      zeroline = TRUE,\n      automargin = TRUE\n    ),\n    yaxis2 = list(\n      title = \"Investigations Aided\",\n      side = 'right',\n      overlaying = 'y',\n      anchor = 'x',\n      position = 1,\n      showgrid = FALSE,\n      zeroline = FALSE,\n      automargin = TRUE,\n      titlefont = list(size = 12),\n      tickfont = list(size = 10)\n    ),\n    legend = list(\n      x = 0.01,\n      y = 0.99,\n      bgcolor = 'rgba(255,255,255,0.9)',\n      bordercolor = 'black',\n      borderwidth = 1\n    ),\n    hovermode = 'x unified'\n  )\n\np_interactive\n\n\n\n\n\n\nShow compiled data visualization and correction code\n#### Publication-Ready Static Plot ####\n\n# Get the actual date range for proper x-axis limits\ndate_range &lt;- range(growth_data_yearly$date)\nextended_date_range &lt;- c(min(date_range) - years(1), max(date_range))\nlegend_start_date &lt;- extended_date_range[1]\n\ny_upper_limit &lt;- max_dna * 1.05\ny_lower_limit &lt;- 0\n\np_static &lt;- ggplot() +\n  geom_line(data = dna_data, \n            aes(x = date, y = count_scaled, color = variable), \n            linewidth = 1.2) +\n  geom_point(data = dna_data, \n             aes(x = date, y = count_scaled, color = variable), \n             size = 2) +\n  geom_line(data = investigations_data, \n            aes(x = date, y = count_scaled, color = variable), \n            linewidth = 1.2) +\n  geom_point(data = investigations_data, \n             aes(x = date, y = count_scaled, color = variable), \n             size = 2) +\n  scale_x_date(\n    date_breaks = \"1 years\",\n    date_labels = \"%Y\",\n    limits = extended_date_range, \n    expand = expansion(mult = 0.02)\n  ) +\n  scale_y_continuous(\n    name = \"DNA Profiles\",\n    labels = function(x) {\n      ifelse(x &gt;= 1e6, paste0(x/1e6, \"M\"), \n             ifelse(x &gt;= 1e3, paste0(x/1e3, \"K\"), x))\n    },\n    breaks = seq(0, max_dna, by = 2e6),\n    limits = c(y_lower_limit, y_upper_limit),\n    sec.axis = sec_axis(~./scale_factor, \n                        name = \"Investigations Aided\",\n                        labels = function(x) {\n                          ifelse(x &gt;= 1e6, paste0(x/1e6, \"M\"), \n                                 ifelse(x &gt;= 1e3, paste0(x/1e3, \"K\"), x))\n                        },\n                        breaks = seq(0, max_investigations, by = 100000))\n  ) +\n  scale_color_manual(\n    name = NULL,\n    values = c(\"Offender\" = \"#1f4e79\", \n               \"Arrestee\" = \"#2e75b6\", \n               \"Forensic\" = \"#5b9bd5\",\n               \"Investigations\" = \"#c00000\") \n  ) +\n  theme_minimal(base_size = 11) +\n  theme(\n    panel.grid = element_blank(),\n    axis.line = element_line(color = \"black\", linewidth = 0.5),\n    axis.ticks = element_line(color = \"black\", linewidth = 0.5),\n    axis.text = element_text(color = \"black\", size = 16),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    axis.title = element_text(size = 20, face = \"bold\"),\n    axis.title.x = element_text(color = \"black\", margin = margin(t = 10)),\n    axis.title.y.left = element_text(color = \"#1f4e79\", margin = margin(r = 10)),\n    axis.title.y.right = element_text(color = \"#c00000\", margin = margin(l = 10)),\n    legend.position = \"none\",\n    plot.margin = margin(5, 10, 5, 10),\n    aspect.ratio = 0.6\n  ) +\n  labs(\n    x = \"Year\",\n    title = \" \"\n  ) +\n  # DNA Profiles legend box\n  annotate(\"rect\", xmin = legend_start_date, \n           xmax = legend_start_date + years(6), \n           ymin = max_dna * 0.86, ymax = max_dna, \n           fill = \"white\", color = \"black\", alpha = 0.9, linewidth = 0.3) +\n  # Investigations legend box\n  annotate(\"rect\", xmin = legend_start_date, \n           xmax = legend_start_date + years(7), \n           ymin = max_dna * 0.74, ymax = max_dna * 0.80, \n           fill = \"white\", color = \"black\", alpha = 0.9, linewidth = 0.3) +\n  # DNA Profiles legend items\n  annotate(\"point\", \n           x = legend_start_date + years(0) + months(6), \n           y = c(max_dna * 0.97, max_dna * 0.93, max_dna * 0.89),\n           color = c(\"#1f4e79\", \"#2e75b6\", \"#5b9bd5\"), size = 1.5) +\n  annotate(\"text\", \n           x = legend_start_date + years(1), \n           y = c(max_dna * 0.97, max_dna * 0.93, max_dna * 0.89),\n           label = c(\"Offender Profiles\", \"Arrestee Profiles\", \"Forensic Profiles\"),\n           hjust = 0, size = 6) +\n  annotate(\"text\", \n           x = legend_start_date + years(0), \n           y = max_dna * 1.01, \n           label = \"DNA Profiles (Millions)\", \n           fontface = \"bold\", hjust = 0, size = 6, vjust = 0) +\n  # Investigations Aided legend\n  annotate(\"point\", \n           x = legend_start_date + years(0) + months(6), \n           y = max_dna * 0.77, \n           color = \"#c00000\", size = 1.5) +\n  annotate(\"text\", \n           x = legend_start_date + years(1), \n           y = max_dna * 0.77, \n           label = \"Investigations Aided\",\n           hjust = 0, size = 6) +\n  annotate(\"text\", \n           x = legend_start_date + years(0), \n           y = max_dna * 0.81, \n           label = \"Investigations (Thousands)\", \n           fontface = \"bold\", hjust = 0, size = 6 , vjust = 0)\n\np_static\n\n\n\n\n\n\n\n\n\n\n\nTemporal Coverage\nThe heat map visualizes the temporal coverage of NDIS data submissions across different jurisdictions over the years for the intermediate csv file (with outliers and reporting errors) and for the cleaned dataset. It highlights periods of active reporting and gaps in data submission.\n\n\nShow heatmap code\n# Prepare data for heatmap - CLEANED DATASET\ntemporal_coverage_clean &lt;- ndis_clean %&gt;%\n  mutate(year = year(capture_datetime)) %&gt;%\n  count(jurisdiction, year) %&gt;%\n  complete(jurisdiction, year = 2001:2025, fill = list(n = 0)) %&gt;%\n  filter(!is.na(jurisdiction)) %&gt;%\n  mutate(jurisdiction = factor(jurisdiction, levels = rev(sort(unique(jurisdiction)))))\n\n# Create the heatmap for cleaned data\nheatmap_after_clean &lt;- ggplot(temporal_coverage_clean, aes(x = year, y = jurisdiction, fill = n)) +\n  geom_tile(color = \"white\", linewidth = 0.3) +\n  scale_fill_viridis(\n    name = \"Snapshots\\nper Year\",\n    option = \"plasma\",\n    direction = -1,\n    breaks = c(0, 3, 6, 10),\n    labels = c(\"0\", \"3\", \"6\", \"10+\")\n  ) +\n  scale_x_continuous(\n    breaks = seq(2001, 2025, by = 1),\n    expand = expansion(mult = 0.01)\n  ) +\n  labs(\n    x = \"Year\",\n    y = \"Jurisdiction\",\n    title = \" \"\n  ) +\n  theme_minimal(base_size = 10) +\n  theme(\n    panel.grid = element_blank(),\n    axis.text.x = element_text(angle = 45, hjust = 1, size = 15),\n    axis.text.y = element_text(size = 15),\n    plot.title = element_text(face = \"bold\", size = 20, hjust = 0),\n    plot.subtitle = element_text(size = 18, hjust = 0),\n    legend.position = \"right\",\n    legend.key.height = unit(0.6, \"cm\"), \n    legend.key.width = unit(0.2, \"cm\"), \n    legend.text = element_text(size = 15),\n    legend.title = element_text(size = 18), \n    axis.title.x = element_text(size = 18, margin = margin(t = 12)),\n    axis.title.y = element_text(size = 18, margin = margin(r = 26))\n  )\n\nheatmap_after_clean\n\n\n\n\n\n\n\n\n\n\n\nComparison with peer-reviewed papers\nAs an additional check, we compared corrected national aggregates against published NDIS totals from FBI press releases and peer-reviewed articles. As shown in Figure 6, the reconstructed dataset aligns closely with these independent milestones, supporting the technical quality of the NDIS time series.\n\n\nShow peer-reviewed literature comparison\n# Preparation of growth_data_yearly \ngrowth_data_yearly &lt;- ndis_clean %&gt;%\n  mutate(year = year(capture_datetime)) %&gt;%\n  group_by(jurisdiction, year) %&gt;%\n  arrange(jurisdiction, capture_datetime) %&gt;%\n  mutate(\n    selection_priority = case_when(\n      year &lt;= 2018 ~ arrestee,\n      year &gt; 2018 ~ offender_profiles\n    )\n  ) %&gt;%\n  slice_max(order_by = selection_priority, n = 1, with_ties = FALSE) %&gt;%\n  ungroup() %&gt;%\n  group_by(year) %&gt;%\n  summarise(\n    offender_total = sum(offender_profiles, na.rm = TRUE),\n    arrestee_total = sum(arrestee, na.rm = TRUE),\n    forensic_total = sum(forensic_profiles, na.rm = TRUE),\n    investigations_total = sum(investigations_aided, na.rm = TRUE),\n    n_jurisdictions = n(),\n    .groups = 'drop'\n  ) %&gt;%\n  mutate(\n    total_profiles = offender_total + arrestee_total + forensic_total,\n    date = as.Date(paste0(year, \"-06-01\"))\n  )\n\n# Prepare data for plotting DNA profiles\ndna_data &lt;- growth_data_yearly %&gt;%\n  select(date, offender_total, arrestee_total, forensic_total, total_profiles) %&gt;%\n  pivot_longer(\n    cols = c(offender_total, arrestee_total, forensic_total, total_profiles),\n    names_to = \"variable\",\n    values_to = \"count\"\n  ) %&gt;%\n  mutate(\n    variable = case_when(\n      variable == \"offender_total\" ~ \"Offender\",\n      variable == \"arrestee_total\" ~ \"Arrestee\", \n      variable == \"forensic_total\" ~ \"Forensic\",\n      variable == \"total_profiles\" ~ \"Total\"\n    )\n  )\n\n# Prepare data for plotting investigations\ninvestigations_data &lt;- growth_data_yearly %&gt;%\n  select(date, investigations_total)\n\n# Create literature dataset\nliterature_data &lt;- tribble(\n  ~citation, ~asof_date, ~offender_profiles, ~arrestee_profiles, ~forensic_profiles, ~total_profiles, ~investigations_aided, ~short_label,\n  \"FBI Brochure\", \"2000-12-01\", 441181, NA, 21625, NA, 1573, \"FBI (Dec 2000)\",\n  \"FBI Brochure\", \"2002-12-01\", 1247163, NA, 46177, NA, 6670, \"FBI (Dec 2002)\",\n  \"FBI Brochure\", \"2004-12-01\", 2038514, NA, 93956, NA, 21266, \"FBI (Dec 2004)\",\n  \"FBI Brochure\", \"2006-12-01\", 3977435, 54313, 160582, NA, 45364, \"FBI (Dec 2006)\",\n  \"FBI Brochure\", \"2008-12-01\", 6399200, 140719, 248943, NA, 81955, \"FBI (Dec 2008)\",\n  \"FBI Brochure\", \"2010-12-01\", 8564705, 668849, 351951, NA, 130317, \"FBI (Dec 2010)\",\n  \"FBI Brochure\", \"2012-12-01\", 10086404, 1332721, 446689, NA, 190560, \"FBI (Dec 2012)\",\n  \"FBI Brochure\", \"2015-06-01\", 11822927, 2028734, 638162, NA, 274648, \"FBI (Jun 2015)\",\n  \"Ge et al., 2012\", \"2011-06-01\", NA, NA, NA, 10000000, 141300, \"Ge et al., 2012\",\n  \"Ge et al., 2014\", \"2013-05-01\", NA, NA, NA, 12000000, 185000, \"Ge et al., 2014\",\n  \"Wickenheiser, 2022\", \"2021-10-01\", 14836490, 4513955, 1144255, NA, 587773, \"Wickenheiser, 2022\",\n  \"Link et al., 2023\", \"2022-11-01\", NA, NA, NA, 21791620, 622955, \"Link et al., 2023\",\n  \"Greenwald & Phiri, 2024\", \"2024-02-01\", 17000000, 5000000, 1300000, NA, 680000, \"Greenwald & Phiri, 2024\"\n) %&gt;%\n  mutate(\n    asof_date = as.Date(asof_date),\n    total_profiles = ifelse(\n      is.na(total_profiles),\n      rowSums(select(., offender_profiles, arrestee_profiles, forensic_profiles), na.rm = TRUE),\n      total_profiles\n    )\n  )\n\n# Prepare literature data for DNA profiles\nliterature_dna &lt;- literature_data %&gt;%\n  select(short_label, asof_date, offender_profiles, arrestee_profiles, forensic_profiles, total_profiles) %&gt;%\n  pivot_longer(\n    cols = c(offender_profiles, arrestee_profiles, forensic_profiles, total_profiles),\n    names_to = \"variable\",\n    values_to = \"count\"\n  ) %&gt;%\n  filter(!is.na(count)) %&gt;%\n  mutate(\n    variable = case_when(\n      variable == \"offender_profiles\" ~ \"Offender\",\n      variable == \"arrestee_profiles\" ~ \"Arrestee\",\n      variable == \"forensic_profiles\" ~ \"Forensic\",\n      variable == \"total_profiles\" ~ \"Total\"\n    )\n  )\n\n# Prepare literature data for investigations\nliterature_investigations &lt;- literature_data %&gt;%\n  select(short_label, asof_date, investigations_aided) %&gt;%\n  filter(!is.na(investigations_aided))\n\n# Get date range\ndate_range &lt;- range(growth_data_yearly$date)\nextended_date_range &lt;- c(min(date_range) - years(1), max(date_range))\nlegend_start_date &lt;- extended_date_range[1]\n\n# Calculate y-axis limits for DNA profiles\nmax_dna &lt;- max(dna_data$count, na.rm = TRUE)\ny_upper_dna &lt;- max_dna * 1.05\n\n# Calculate y-axis limits for investigations\nmax_inv &lt;- max(investigations_data$investigations_total, na.rm = TRUE)\ny_upper_inv &lt;- max_inv * 1.05\n\n# Define colors for each plot type\noffender_color &lt;- \"#31688e\"\narrestee_color &lt;- \"#35b779\"\nforensic_color &lt;- \"#440154\"\ntotal_color &lt;- \"#22a884\"\ninvestigations_color &lt;- \"#fde724\"\n\n# Create individual plots for each DNA profile type\np_offender &lt;- ggplot() +\n  geom_line(data = dna_data %&gt;% filter(variable == \"Offender\"), \n            aes(x = date, y = count), \n            color = offender_color, linewidth = 0.5) +\n  geom_point(data = dna_data %&gt;% filter(variable == \"Offender\"), \n             aes(x = date, y = count), \n             color = offender_color, size = 1.0) +\n  geom_point(data = literature_dna %&gt;% filter(variable == \"Offender\"),\n             aes(x = asof_date, y = count),\n             shape = 4, size = 2.8, stroke = 1.0, color = offender_color) +\n  geom_label_repel(\n    data = literature_dna %&gt;% filter(variable == \"Offender\" & lubridate::year(asof_date) &lt; 2020),\n    aes(x = asof_date, y = count, label = short_label),\n    size = 45 / .pt,\n    nudge_y = 5000000,\n    box.padding = 0.35, point.padding = 0.5,\n    min.segment.length = 0.5, segment.color = \"gray50\",\n    direction = \"both\", force = 5, force_pull = 1,\n    max.overlaps = Inf,\n    fill = \"white\", label.size = 0.2, label.padding = unit(0.15, \"lines\")\n  ) +\n  geom_label_repel(\n    data = literature_dna %&gt;% filter(variable == \"Offender\" & lubridate::year(asof_date) &gt;= 2021),\n    aes(x = asof_date, y = count, label = short_label),\n    size = 45 / .pt,\n    nudge_y = 1000000,\n    box.padding = 0.35, point.padding = 0.5,\n    min.segment.length = 0.5, segment.color = \"gray50\",\n    direction = \"both\", force = 5, force_pull = 1,\n    max.overlaps = Inf,\n    fill = \"white\", label.size = 0.2, label.padding = unit(0.15, \"lines\")\n  ) +\n  scale_x_date(name = \"Year\", date_breaks = \"1 years\", date_labels = \"%Y\",\n               limits = extended_date_range, expand = expansion(mult = 0.02)) +\n  scale_y_continuous(name = \"Offender Profiles\",\n                     labels = function(x) ifelse(x &gt;= 1e6, paste0(x/1e6, \"M\"), \n                                                  ifelse(x &gt;= 1e3, paste0(x/1e3, \"K\"), x)),\n                     limits = c(0, max(literature_dna %&gt;% filter(variable == \"Offender\") %&gt;% pull(count), na.rm = TRUE) * 1.1),\n                     expand = expansion(mult = c(0, 0.05))) +\n  theme_minimal(base_size = 35) +\n  theme(\n    panel.grid.major.y = element_line(color = \"gray90\", linewidth = 0.3),\n    panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(),\n    axis.line = element_line(color = \"black\", linewidth = 0.3),\n    axis.ticks = element_line(color = \"black\", linewidth = 0.3),\n    axis.text = element_text(color = \"black\", size = 35),\n    axis.text.x = element_text(angle = 45, hjust = 1, margin = margin(t = -5)),\n    axis.text.y = element_text(margin = margin(r = -10)),\n    axis.title = element_text(size = 35, face = \"bold\"),\n    axis.title.x = element_blank(),\n    axis.title.y = element_text(margin = margin(r = 2)),\n    plot.title = element_blank(),\n    plot.title.position = \"plot\"\n  ) +\n  labs(x = \"Year\", y = NULL, title = \"Offender Profiles\")\n\np_offender\n\n\n\n\n\n\n\n\n\nShow peer-reviewed literature comparison\np_arrestee &lt;- ggplot() +\n  geom_line(data = dna_data %&gt;% filter(variable == \"Arrestee\"), \n            aes(x = date, y = count), \n            color = arrestee_color, linewidth = 0.5) +\n  geom_point(data = dna_data %&gt;% filter(variable == \"Arrestee\"), \n             aes(x = date, y = count), \n             color = arrestee_color, size = 1.0) +\n  geom_point(data = literature_dna %&gt;% filter(variable == \"Arrestee\"),\n             aes(x = asof_date, y = count),\n             shape = 4, size = 2.8, stroke = 1.0, color = arrestee_color) +\n  geom_label_repel(\n    data = literature_dna %&gt;% filter(variable == \"Arrestee\"),\n    aes(x = asof_date, y = count, label = short_label),\n    size = 45 / .pt,\n    nudge_y = 500000,\n    box.padding = 0.35, point.padding = 0.5,\n    min.segment.length = 0.5, segment.color = \"gray50\",\n    direction = \"both\", force = 5, force_pull = 1,\n    max.overlaps = Inf,\n    fill = \"white\", label.size = 0.2, label.padding = unit(0.15, \"lines\")\n  ) +\n  scale_x_date(name = \"Year\", date_breaks = \"1 years\", date_labels = \"%Y\",\n               limits = extended_date_range, expand = expansion(mult = 0.02)) +\n  scale_y_continuous(name = \"Arrestee Profiles\",\n                     labels = function(x) ifelse(x &gt;= 1e6, paste0(x/1e6, \"M\"), \n                                                  ifelse(x &gt;= 1e3, paste0(x/1e3, \"K\"), x)),\n                     limits = c(0, max(literature_dna %&gt;% filter(variable == \"Arrestee\") %&gt;% pull(count), na.rm = TRUE) * 1.1),\n                     expand = expansion(mult = c(0, 0.05))) +\n  theme_minimal(base_size = 35) +\n  theme(\n    panel.grid.major.y = element_line(color = \"gray90\", linewidth = 0.3),\n    panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(),\n    axis.line = element_line(color = \"black\", linewidth = 0.3),\n    axis.ticks = element_line(color = \"black\", linewidth = 0.3),\n    axis.text = element_text(color = \"black\", size = 35),\n    axis.text.x = element_text(angle = 45, hjust = 1, margin = margin(t = -5)),\n    axis.text.y = element_text(margin = margin(r = -10)),\n    axis.title = element_text(size = 35, face = \"bold\"),\n    axis.title.x = element_blank(),\n    axis.title.y = element_text(margin = margin(r = 2)),\n    plot.title = element_blank(),\n    plot.title.position = \"plot\") +\n  labs(x = \"Year\", y = NULL, title = \"Arrestee Profiles\")\n\np_arrestee\n\n\n\n\n\n\n\n\n\nShow peer-reviewed literature comparison\np_forensic &lt;- ggplot() +\n  geom_line(data = dna_data %&gt;% filter(variable == \"Forensic\"), \n            aes(x = date, y = count), \n            color = forensic_color, linewidth = 0.5) +\n  geom_point(data = dna_data %&gt;% filter(variable == \"Forensic\"), \n             aes(x = date, y = count), \n             color = forensic_color, size = 1.0) +\n  geom_point(data = literature_dna %&gt;% filter(variable == \"Forensic\"),\n             aes(x = asof_date, y = count),\n             shape = 4, size = 2.8, stroke = 1.0, color = forensic_color) +\n  geom_label_repel(\n    data = literature_dna %&gt;% filter(variable == \"Forensic\" & lubridate::year(asof_date) &lt; 2020),\n    aes(x = asof_date, y = count, label = short_label),\n    size = 45 / .pt,\n    nudge_y = 500000,\n    box.padding = 0.35, point.padding = 0.5,\n    min.segment.length = 0.5, segment.color = \"gray50\",\n    direction = \"both\", force = 5, force_pull = 1,\n    max.overlaps = Inf,\n    fill = \"white\", label.size = 0.2, label.padding = unit(0.15, \"lines\")\n  ) +\n  geom_label_repel(\n    data = literature_dna %&gt;% filter(variable == \"Forensic\" & lubridate::year(asof_date) &gt;= 2021),\n    aes(x = asof_date, y = count, label = short_label),\n    size = 45 / .pt,\n    nudge_y = 100000,\n    box.padding = 0.35, point.padding = 0.5,\n    min.segment.length = 0.5, segment.color = \"gray50\",\n    direction = \"both\", force = 5, force_pull = 1,\n    max.overlaps = Inf,\n    fill = \"white\", label.size = 0.2, label.padding = unit(0.15, \"lines\")\n  ) +\n  scale_x_date(name = \"Year\", date_breaks = \"1 years\", date_labels = \"%Y\",\n               limits = extended_date_range, expand = expansion(mult = 0.02)) +\n  scale_y_continuous(name = \"Forensic Profiles\",\n                     labels = function(x) ifelse(x &gt;= 1e6, paste0(x/1e6, \"M\"), \n                                                  ifelse(x &gt;= 1e3, paste0(x/1e3, \"K\"), x)),\n                     limits = c(0, max(literature_dna %&gt;% filter(variable == \"Forensic\") %&gt;% pull(count), na.rm = TRUE) * 1.1),\n                     expand = expansion(mult = c(0, 0.05))) +\n  theme_minimal(base_size = 35) +\n  theme(\n    panel.grid.major.y = element_line(color = \"gray90\", linewidth = 0.3),\n    panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(),\n    axis.line = element_line(color = \"black\", linewidth = 0.3),\n    axis.ticks = element_line(color = \"black\", linewidth = 0.3),\n    axis.text = element_text(color = \"black\", size = 35),\n    axis.text.x = element_text(angle = 45, hjust = 1, margin = margin(t = -5)),\n    axis.text.y = element_text(margin = margin(r = -10)),\n    axis.title = element_text(size = 35, face = \"bold\"),\n    axis.title.x = element_blank(),\n    axis.title.y = element_text(margin = margin(r = 2)),\n    plot.title = element_blank(),\n    plot.title.position = \"plot\") +\n  labs(x = \"Year\", y = NULL, title = \"Forensic Profiles\")\n\np_forensic\n\n\n\n\n\n\n\n\n\nShow peer-reviewed literature comparison\np_total &lt;- ggplot() +\n  geom_line(data = dna_data %&gt;% filter(variable == \"Total\"), \n            aes(x = date, y = count), \n            color = total_color, linewidth = 0.5) +\n  geom_point(data = dna_data %&gt;% filter(variable == \"Total\"), \n             aes(x = date, y = count), \n             color = total_color, size = 1.0) +\n  geom_point(data = literature_dna %&gt;% filter(variable == \"Total\"),\n             aes(x = asof_date, y = count),\n             shape = 4, size = 2.8, stroke = 1.0, color = total_color) +\n  geom_label_repel(\n    data = literature_dna %&gt;% filter(variable == \"Total\" & lubridate::year(asof_date) &gt;= 2020),\n    aes(x = asof_date, y = count, label = short_label),\n    size = 45 / .pt,\n    nudge_y = -1000000,\n    box.padding = 0.35, point.padding = 0.5,\n    min.segment.length = 0.5, segment.color = \"gray50\",\n    direction = \"y\", force = 5, force_pull = 1,\n    max.overlaps = Inf,\n    fill = \"white\", label.size = 0.2, label.padding = unit(0.15, \"lines\")\n  ) +\n  geom_label_repel(\n    data = literature_dna %&gt;% filter(variable == \"Total\" & lubridate::year(asof_date) &lt; 2021),\n    aes(x = asof_date, y = count, label = short_label),\n    size = 45 / .pt,\n    nudge_y = 6000000,\n    box.padding = 0.35, point.padding = 0.5,\n    min.segment.length = 0.5, segment.color = \"gray50\",\n    direction = \"both\", force = 5, force_pull = 1,\n    max.overlaps = Inf,\n    fill = \"white\", label.size = 0.2, label.padding = unit(0.15, \"lines\")\n  ) +\n  scale_x_date(name = \"Year\", date_breaks = \"1 years\", date_labels = \"%Y\",\n               limits = extended_date_range, expand = expansion(mult = 0.02)) +\n  scale_y_continuous(name = \"Total Profiles\",\n                     labels = function(x) ifelse(x &gt;= 1e6, paste0(x/1e6, \"M\"), \n                                                  ifelse(x &gt;= 1e3, paste0(x/1e3, \"K\"), x)),\n                     limits = c(0, max(literature_dna %&gt;% filter(variable == \"Total\") %&gt;% pull(count), na.rm = TRUE) * 1.1),\n                     expand = expansion(mult = c(0, 0.05))) +\n  theme_minimal(base_size = 35) +\n  theme(\n    panel.grid.major.y = element_line(color = \"gray90\", linewidth = 0.3),\n    panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(),\n    axis.line = element_line(color = \"black\", linewidth = 0.3),\n    axis.ticks = element_line(color = \"black\", linewidth = 0.3),\n    axis.text = element_text(color = \"black\", size = 35),\n    axis.text.x = element_text(angle = 45, hjust = 1, margin = margin(t = -5)),\n    axis.text.y = element_text(margin = margin(r = -10)),\n    axis.title = element_text(size = 35, face = \"bold\"),\n    axis.title.x = element_blank(),\n    axis.title.y = element_text(margin = margin(r = 2)),\n    plot.title = element_blank(),\n    plot.title.position = \"plot\" ) +\n  labs(x = \"Year\", y = NULL, title = \"Total Profiles\")\n\np_total\n\n\n\n\n\n\n\n\n\nShow peer-reviewed literature comparison\np_investigations_grid &lt;- ggplot() +\n  geom_line(data = investigations_data, \n            aes(x = date, y = investigations_total), \n            color = investigations_color, linewidth = 0.5) +\n  geom_point(data = investigations_data, \n             aes(x = date, y = investigations_total), \n             color = investigations_color, size = 1.0) +\n  geom_point(data = literature_investigations,\n             aes(x = asof_date, y = investigations_aided),\n             shape = 4, size = 2.8, stroke = 1.0, color = investigations_color) +\n  geom_label_repel(\n    data = literature_investigations %&gt;% filter(lubridate::year(asof_date) &lt; 2021),\n    aes(x = asof_date, y = investigations_aided, label = short_label),\n    size = 45 / .pt,\n    nudge_y = 100000,\n    box.padding = 0.35, point.padding = 0.5,\n    min.segment.length = 0.5, segment.color = \"gray50\",\n    direction = \"both\", force = 5, force_pull = 1,\n    max.overlaps = Inf,\n    fill = \"white\", label.size = 0.2, label.padding = unit(0.15, \"lines\")\n  ) +\n  geom_label_repel(\n    data = literature_investigations %&gt;% filter(lubridate::year(asof_date) &gt; 2021),\n    aes(x = asof_date, y = investigations_aided, label = short_label),\n    size = 45 / .pt,\n    nudge_x = -600,\n    nudge_y = 15000,\n    box.padding = 0.35, point.padding = 0.5,\n    min.segment.length = 0.5, segment.color = \"gray50\",\n    direction = \"x\",\n    force = 5, force_pull = 1,\n    max.overlaps = Inf,\n    fill = \"white\", label.size = 0.2, label.padding = unit(0.15, \"lines\")\n  ) +\n  geom_label_repel(\n    data = literature_investigations %&gt;% filter(lubridate::year(asof_date) == 2021),\n    aes(x = asof_date, y = investigations_aided, label = short_label),\n    size = 45 / .pt,\n    nudge_y = 10000,\n    nudge_x = 1000,\n    box.padding = 0.35, point.padding = 0.5,\n    min.segment.length = 0.5, segment.color = \"gray50\",\n    direction = \"both\",\n    force = 5, force_pull = 1,\n    max.overlaps = Inf,\n    fill = \"white\", label.size = 0.2, label.padding = unit(0.15, \"lines\")\n  ) +\n  scale_x_date(name = \"Year\", date_breaks = \"1 years\", date_labels = \"%Y\",\n               limits = extended_date_range, expand = expansion(mult = 0.02)) +\n  scale_y_continuous(name = \"Investigations Aided\",\n                     labels = function(x) ifelse(x &gt;= 1e6, paste0(x/1e6, \"M\"), \n                                                  ifelse(x &gt;= 1e3, paste0(x/1e3, \"K\"), x)),\n                     limits = c(0, max(literature_investigations %&gt;% pull(investigations_aided), na.rm = TRUE) * 1.1),\n                     expand = expansion(mult = c(0, 0.05))) +\n  theme_minimal(base_size = 35) +\n  theme(\n    panel.grid.major.y = element_line(color = \"gray90\", linewidth = 0.3),\n    panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(),\n    axis.line = element_line(color = \"black\", linewidth = 0.3),\n    axis.ticks = element_line(color = \"black\", linewidth = 0.3),\n    axis.text = element_text(color = \"black\", size = 35),\n    axis.text.x = element_text(angle = 45, margin = margin(t = -5)),\n    axis.text.y = element_text(margin = margin(r = -10)),\n    axis.title = element_text(size = 35, face = \"bold\"),\n    axis.title.x = element_text(margin = margin(t = 2)),\n    axis.title.y = element_text(margin = margin(r = 2)),\n    plot.title = element_blank(),\n    plot.title.position = \"plot\") +\n  labs(x = \"Year\", y = NULL, title = \"Investigations Aided\")\n\np_investigations_grid\n\n\n\n\n\n\n\n\n\nShow peer-reviewed literature comparison\nplots_grid &lt;- (p_offender + p_arrestee) / \n              (p_forensic + p_total) / \n              (p_investigations_grid)\n\nplots_grid &lt;- plots_grid + \n  plot_layout(heights = c(1, 1, 1)) &\n  theme(plot.title = element_text(size = 60, face = \"bold\"),\n        axis.title = element_text(size = 60, face = \"bold\"),\n        axis.text = element_text(size = 48))"
  },
  {
    "objectID": "qmd_root/ndis_analysis.html#data-growth",
    "href": "qmd_root/ndis_analysis.html#data-growth",
    "title": "NDIS Database",
    "section": "Variables Growth and Corrections",
    "text": "Variables Growth and Corrections\nThe National DNA Index System (NDIS) data for each jurisdiction is expected to show consistent growth over time. However, reporting issues create anomalies that require systematic detection and correction. This section documents the validation framework, with specific rules tailored to each metric following visual verification and analysis of the raw data."
  },
  {
    "objectID": "qmd_root/ndis_analysis.html#metric-specific-validation-rules",
    "href": "qmd_root/ndis_analysis.html#metric-specific-validation-rules",
    "title": "NDIS Database",
    "section": "Metric-Specific Validation Rules",
    "text": "Metric-Specific Validation Rules\n\nOffender Profiles\nFollowing visual verification and analysis of the raw data, the following rules were applied to the Offender Profiles metric:\n\nSpike-Dip Detection: Flags points where values drop below half the previous value or fall below half the next value\nContinuation Spike-Dip: Detects recovery points following flagged anomalies\nZero Error Detection: Flags any zero value appearing after positive values in the California jurisdiction\nContinuation Zero Error: Tracks consecutive zeros following the initial error\nUpdate Lag Detection: Identifies oscillations within 5-day windows where current values are lower than or equal to previous/next values\nValue Propagation: Additionally flags any data point with the same value as previously flagged anomalies within the jurisdiction\n\nAll flagged points are removed to produce the cleaned dataset.\n\n\nForensic Profiles\nFollowing visual verification and analysis of the raw data, the following rules were applied to the Forensic Profiles metric:\n\nSpike-Dip Detection: Flags points where values drop below half the previous or next value, or exceed 2.5 times the next value\nContinuation Spike-Dip: Detects recovery points following flagged anomalies\nZero Error Detection: Flags any zero value appearing after positive values\nContinuation Zero Error: Tracks consecutive zeros following the initial error\nUpdate Lag Detection: Identifies oscillations within 2-day windows where current values are lower than or equal to previous/next values\nValue Propagation: Additionally flags any data point with the same value as previously flagged anomalies within the jurisdiction\n\nAll flagged points are removed to produce the cleaned dataset.\n\n\nArrestee Profiles\nFollowing visual verification and analysis of the raw data (filtered to January 1, 2012 onwards), a focused rule set was applied to the Arrestee Profiles metric:\n\nZero Error Detection: Flags zero values appearing after positive values specifically in the California jurisdiction\nNo value propagation or other rules were applied based on the observed data patterns\n\nAll flagged zero error points are removed to produce the cleaned dataset.\n\n\nInvestigations Aided\nFollowing visual verification and analysis of the raw data, the following rules were applied to the Investigations Aided metric:\n\nSpike-Dip Detection: Flags points where values increase more than 10-fold relative to the previous value\nContinuation Spike-Dip: Detects recovery points following flagged anomalies\nZero Error Detection: Flags any zero value appearing after positive values\nContinuation Zero Error: Tracks consecutive zeros following the initial error\nValue Propagation: Additionally flags any data point with the same value as previously flagged anomalies within the jurisdiction\n\nAll flagged points are removed to produce the cleaned dataset.\n\n\nParticipating Laboratories (NDIS Labs)\nFollowing visual verification and analysis of the raw data, jurisdiction-specific rules were applied to the Participating Laboratories metric:\n\nSpike-Dip Detection (Oklahoma): Flags points where the value increases more than 3-fold relative to the previous value\nSpike-Dip Detection (Michigan): Flags points where the value drops to 25% or less of the previous value\nContinuation Spike-Dip: Detects recovery points following flagged anomalies\nNo value propagation rule applied for this metric\n\nAll flagged points are removed to produce the cleaned dataset.\n\nNote: All metrics employ a data deduplication step that retains only the first observation for each jurisdiction within the same capture datetime (rounded to seconds). Yearly summaries report the maximum value per jurisdiction per year, then aggregate across jurisdictions."
  }
]