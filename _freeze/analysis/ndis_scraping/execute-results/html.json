{
  "hash": "c5206b548d1d5a030195576e50fc09b9",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"NDIS Database\"\nsubtitle: \"Collecting and Parsing FBI National DNA Index System Statistics from Wayback Machine\"\nauthor: \"Tina Lasisi | JoÃ£o P. Donadio\"\ndate: today\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    code-fold: false\nexecute:\n  echo: true\n  warning: false\n---\n\n## Introduction {#introduction}\n\nThe National DNA Index System (NDIS) is the central database that allows accredited forensic laboratories across the United States to electronically exchange and compare DNA profiles. Maintained by the FBI as part of CODIS (Combined DNA Index System), NDIS tracks the accumulation of DNA records contributed by federal, state, and local laboratories.\n\nThis project focuses on systematically compiling the growth and evolution of NDIS by parsing historical statistics published on the FBIâ€™s website and preserved in the Internet Archiveâ€™s Wayback Machine. These snapshots contain tables reporting the number of DNA profiles stored in NDIS (offender, arrestee, forensic), as well as information on laboratory participation across jurisdictions.\n\n### Objectives {#objectives}\n\n1.  Develop a reproducible pipeline to extract NDIS statistics from archived FBI webpages in the Wayback Machine.\n\n2.  Identify and correct inconsistencies and data quality issues across historical snapshots.\n\n3.  Document the expansion of DNA profiles (offender, arrestee, forensic) over time.\n\n# Methodology\n\n![Project step-by-step](../output/figures/ndis/methodology.png)\n\n## Setup and Configuration {#setup-configuration}\n\n### System Requirements {#system-requirements}\n\n**Required Packages:**\n\n- Core: requests, beautifulsoup4, and lxml (scraping/parsing).\n\n- Data/Visualization: pandas and tqdm (progress tracking).\n\n::: {#setup .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Configuration code\"}\nimport sys\nimport subprocess\nimport importlib\n\nrequired_packages = [\n    'requests',         # API/HTTP\n    'beautifulsoup4',   # HTML parsing\n    'lxml',             # Faster parsing\n    'pandas',           # Data handling\n    'tqdm',              # Progress bars\n    'hashlib',\n    'collections',\n    'pathlib',\n    'datetime',\n    'os'\n]\n\nfor package in required_packages:\n    try:\n        importlib.import_module(package)\n        print(f\"âœ“ {package} already installed\")\n    except ImportError:\n        print(f\"Installing {package}...\")\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n\nprint(f\"ðŸ“š All required packages are installed.\")\n```\n:::\n\n\n### Project Structure {#project-structure}\n\n**Main Configurations:**\n\n- Directory paths for raw HTML (data\\ndis\\raw\\ndis_snapshots), metadata (data\\ndis\\raw\\ndis_metadata), and outputs (data\\ndis\\raw\\ndis_outputs).\n\n- Standardization mappings for jurisdiction names and known data typos.\n\n::: {#config .cell results='hold' execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Configuration code\"}\nfrom pathlib import Path\nimport re, json, requests, time, hashlib\nfrom datetime import datetime\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nfrom tqdm.auto import tqdm\nfrom datetime import datetime\nfrom collections import defaultdict\nimport os\n\n# Configuration\nBASE_DIR = Path(\"..\")  # Project root directory\nHTML_DIR = BASE_DIR / \"data\" / \"ndis\" / \"raw\" / \"ndis_snapshots\"    # Storage for downloaded HTML\nMETA_DIR = BASE_DIR / \"data\" / \"ndis\" / \"raw\" / \"ndis_metadata\"    # Metadata storage\nOUTPUT_DIR = BASE_DIR / \"data\" / \"ndis\" / \"raw\"       # Processed data output\n\nNDIS_SNAPSHOTS_DIR = HTML_DIR\nNDIS_SNAPSHOTS_DIR.mkdir(parents=True, exist_ok=True)\n\n# Create directory structure\nfor directory in [HTML_DIR, META_DIR, OUTPUT_DIR]:\n    directory.mkdir(parents=True, exist_ok=True)\n```\n:::\n\n\n::: {#5004fb69 .cell execution_count=3}\n\n::: {.cell-output .cell-output-stdout}\n```\nProject directories initialized:\n  - Working directory: /Users/tlasisi/GitHub/PODFRIDGE-Databases\n  - HTML storage: ../data/ndis/raw/ndis_snapshots\n  - Metadata directory: ../data/ndis/raw/ndis_metadata\n  - Output directory: ../data/ndis/raw\n```\n:::\n:::\n\n\n## Wayback Machine Snapshot Search {#WMSS}\n\nA function was developed to systematically search the Internet Archive's Wayback Machine for all preserved snapshots of FBI NDIS statistics pages using a comprehensive multi-phase approach.\n\n### Scraping Method {#scrap-method}\n\n1. **Multi-Phase Search Strategy**:\n\n-   First searches for snapshots from the pre-2007 era using state-specific URLs\n\n-   Then targets consolidated pages from post-2007 periods\n\n-   Handles both HTTP and HTTPS protocol variants\n\n2.  **Robust Error Handling**:\n\n-   Automatic retries with exponential backoff (1s â†’ 2s â†’ 4s delays)\n\n-   Deduplicates results by timestamp\n\n-   Failed requests are tracked and retried with increased retry attempts\n\n-   Preserves complete error context for troubleshooting\n\n### Technical Implementation {#technical-impl}\n\n1.  API Request\n\n2.  Converts JSON responses to clean DataFrame\n\n3.  Sorts chronologically (oldest â†’ newest)\n\n### Core Search Implementation {#core-search}\n\n- **make_request_with_retry()**: Implements exponential backoff (1s â†’ 2s â†’ 4s delays) for fault-tolerant API requests with configurable retry attempts.\n\n::: {#snapshot-search1 .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show search helpers function code\"}\ndef make_request_with_retry(params, max_retries=3, initial_delay=1):\n    API_URL = \"https://web.archive.org/cdx/search/cdx\"\n    delay = initial_delay\n    for attempt in range(max_retries):\n        try:\n            resp = requests.get(API_URL, params=params, timeout=30)\n            resp.raise_for_status()\n            print(f\"âœ“ Successful request for {params['url']} (offset: {params.get('offset', 0)})\")\n            return resp\n        except requests.exceptions.RequestException as e:\n            if attempt == max_retries - 1:\n                print(f\"âœ— Final attempt failed for {params['url']} (offset: {params.get('offset', 0)}): {str(e)}\")\n                return None\n            print(f\"! Attempt {attempt+1} failed for {params['url']}, retrying in {delay} seconds...\")\n            time.sleep(delay)\n            delay *= 2\n```\n:::\n\n\n#### Evolution of the NDIS Webpage (2001â€“2025)\n\nOver the years, the NDIS statistics webpage has evolved considerably in both structure and design.\nEarly versions (2001â€“2007) displayed state-level statistics through individual HTML pages (e.g., ne.htm for Nebraska), accessible via a clickable U.S. map interface. From 2008 onward, the FBI consolidated all states and agencies into unified pages, simplifying data retrieval and standardizing presentation formats.\nThis evolution reflects ongoing modernization of the FBIâ€™s online data systems, transitioning from fragmented, state-specific sources to more cohesive and accessible datasets.\n\n![Website Formats](../output/figures/ndis/snapshots/snapshots_years.png)\n\n#### Pre-2007 (Clickmap Era)\n\n- Each state/agency has its own page (e.g., ne.htm for Nebraska, dc.htm for DC/FBI Lab).\n\n- The search iterates through the known two-letter codes and queries Wayback for each URL individually.\n\n- **search_pre2007_snapshots()**: Searches individual state-specific pages using 50+ state codes (al.htm, ak.htm, etc.).\n\n::: {#snapshot-search2 .cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show pre-2007 search function code\"}\ndef search_pre2007_snapshots():\n    state_codes = [\"al\", \"ak\", \"az\", \"ar\", \"ca\", \"co\", \"ct\", \"de\", \"dc\",\n                   \"fl\", \"ga\", \"hi\", \"id\", \"il\", \"in\", \"ia\", \"ks\", \"ky\",\n                   \"la\", \"me\", \"md\", \"ma\", \"mi\", \"mn\", \"ms\", \"mo\", \"mt\",\n                   \"ne\", \"nv\", \"nh\", \"nj\", \"nm\", \"ny\", \"nc\", \"nd\", \"oh\",\n                   \"ok\", \"or\", \"pa\", \"pr\", \"ri\", \"sc\", \"sd\", \"tn\", \"tx\",\n                   \"army\", \"ut\", \"vt\", \"va\", \"wa\", \"wv\", \"wi\", \"wy\"]\n    all_rows = []\n    seen_timestamps = set()\n    total_saved = 0\n    \n    print(f\"Starting pre-2007 snapshot search for {len(state_codes)} state codes at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    \n    for i, code in enumerate(state_codes, 1):\n        url = f\"http://www.fbi.gov/hq/lab/codis/{code}.htm\"\n        offset = 0\n        state_snapshots = 0\n        has_more_results = True\n        \n        print(f\"\\n[{i}/{len(state_codes)}] Searching for {code.upper()} snapshots...\")\n        \n        while has_more_results:\n            params = {\n                \"url\": url,\n                \"matchType\": \"exact\",\n                \"output\": \"json\",\n                \"fl\": \"timestamp,original,mimetype,statuscode\",\n                \"filter\": [\"statuscode:200\", \"mimetype:text/html\"],\n                \"limit\": \"5000\",\n                \"offset\": str(offset)\n            }\n            resp = make_request_with_retry(params, max_retries=5, initial_delay=2)\n            if not resp: \n                print(f\"âœ— Failed to retrieve {code.upper()}\")\n                break\n                \n            data = resp.json()\n            \n            if len(data) <= 1:\n                print(f\"  No more results for {code.upper()} at offset {offset}\")\n                has_more_results = False\n                break\n                \n            new_snapshots = 0\n            for row in data[1:]:\n                timestamp = row[0]\n                if timestamp not in seen_timestamps:\n                    all_rows.append(row)\n                    seen_timestamps.add(timestamp)\n                    new_snapshots += 1\n                    state_snapshots += 1\n                    total_saved += 1\n            \n            if new_snapshots > 0:\n                print(f\"  â†’ Saved {new_snapshots} new snapshots for {code.upper()} (offset: {offset})\")\n            \n            # Check if we've reached the end of results\n            if len(data) < 5001:\n                print(f\"  Reached end of results for {code.upper()} at offset {offset}\")\n                has_more_results = False\n            else:\n                offset += 5000\n                time.sleep(1)  # Brief pause between requests\n        \n        if state_snapshots > 0:\n            print(f\"âœ“ Found {state_snapshots} total snapshots for {code.upper()}\")\n        else:\n            print(f\"âœ— No snapshots found for {code.upper()}\")\n    \n    print(f\"\\nPre-2007 search completed. Total snapshots saved: {total_saved}\")\n    return pd.DataFrame(all_rows, columns=[\"timestamp\",\"original\",\"mimetype\",\"status\"]), 0\n```\n:::\n\n\n#### Post-2007 (Consolidated Pages)\n\n- All state/agency records are on a single page per snapshot.\n\n- Searches use the known consolidated URL patterns per era.\n\n- **search_post2007_snapshots()**: Searches consolidated pages across 5 historical URL patterns with both HTTP and HTTPS variants.\n\n::: {#snapshot-search3 .cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show search function code\"}\ndef search_post2007_snapshots():\n    urls = [\n        \"https://www.fbi.gov/hq/lab/codis/stats.htm\",\n        \"https://www.fbi.gov/about-us/lab/codis/ndis-statistics\",\n        \"https://www.fbi.gov/about-us/lab/biometric-analysis/codis/ndis-statistics\",\n        \"https://www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics\",\n        \"https://le.fbi.gov/science-and-lab/biometrics-and-fingerprints/codis/codis-ndis-statistics\"\n    ]\n    \n    all_rows = []\n    seen_timestamps = set()\n    protocols = [\"http://\", \"https://\"]\n    total_saved = 0\n    \n    print(f\"Starting post-2007 snapshot search for {len(urls)} URLs at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    \n    for i, base_url in enumerate(urls, 1):\n        url_snapshots = 0\n        \n        for protocol in protocols:\n            current_url = base_url.replace(\"https://\",\"\").replace(\"http://\",\"\")\n            full_url = f\"{protocol}{current_url}\"\n            offset = 0\n            has_more_results = True\n            \n            print(f\"\\n[{i}/{len(urls)}] Searching for {full_url} snapshots...\")\n            \n            while has_more_results:\n                params = {\n                    \"url\": full_url,\n                    \"matchType\": \"exact\",\n                    \"output\": \"json\",\n                    \"fl\": \"timestamp,original,mimetype,statuscode\",\n                    \"filter\": [\"statuscode:200\", \"mimetype:text/html\"],\n                    \"limit\": \"5000\",\n                    \"offset\": str(offset)\n                }\n                resp = make_request_with_retry(params, max_retries=5, initial_delay=2)\n                if not resp: \n                    print(f\"âœ— Failed to retrieve {full_url}\")\n                    break\n                    \n                data = resp.json()\n                \n                if len(data) <= 1:\n                    print(f\"  No more results for {full_url} at offset {offset}\")\n                    has_more_results = False\n                    break\n                    \n                new_snapshots = 0\n                for row in data[1:]:\n                    timestamp = row[0]\n                    if timestamp not in seen_timestamps:\n                        all_rows.append(row)\n                        seen_timestamps.add(timestamp)\n                        new_snapshots += 1\n                        url_snapshots += 1\n                        total_saved += 1\n                \n                if new_snapshots > 0:\n                    print(f\"  â†’ Saved {new_snapshots} new snapshots for {full_url} (offset: {offset})\")\n                \n                if len(data) < 5001:\n                    print(f\"  Reached end of results for {full_url} at offset {offset}\")\n                    has_more_results = False\n                else:\n                    offset += 5000\n                    time.sleep(1)\n        \n        if url_snapshots > 0:\n            print(f\"âœ“ Found {url_snapshots} total snapshots for {base_url}\")\n        else:\n            print(f\"âœ— No snapshots found for {base_url}\")\n    \n    print(f\"\\nPost-2007 search completed. Total snapshots saved: {total_saved}\")\n    return pd.DataFrame(all_rows, columns=[\"timestamp\",\"original\",\"mimetype\",\"status\"]), 0\n```\n:::\n\n\n### Search Execution {#execution-search}\n\n- Calls both pre-2007 and post-2007 search functions to retrieve comprehensive NDIS records.\n\n- Combines results and removes duplicates based on timestamps.\n\n- Stores technical details (timestamps, URL variants, duplicate counts) in structured JSON metadata.\n\n- Provides detailed logging and progress tracking throughout the search process.\n\n::: {#execute-search .cell message='false' execution_count=7}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show search code\"}\npre2007_df, _ = search_pre2007_snapshots()\npost2007_df, _ = search_post2007_snapshots()\n\n# Combine all snapshots\nsnap_df = pd.concat([pre2007_df, post2007_df]).drop_duplicates(\"timestamp\").sort_values(\"timestamp\").reset_index(drop=True)\n\n# Save search metadata\nsearch_meta = {\n    \"search_performed\": datetime.now().isoformat(),\n    \"total_snapshots\": len(snap_df),\n    \"time_span\": {\n        \"first\": snap_df[\"timestamp\"].min(),\n        \"last\": snap_df[\"timestamp\"].max()\n    },\n    \"url_variants\": snap_df[\"original\"].nunique()\n}\n\nwith open(META_DIR / \"search_metadata.json\", \"w\") as f:\n    json.dump(search_meta, f, indent=2)\n```\n:::\n\n\n\n\n## Snapshot Downloader {#downloaderSnap}\n\nThis system provides a robust method for downloading historical webpage snapshots from the Internet Archive's Wayback Machine, specifically designed for the FBI NDIS statistics pages.\n\n### Download Methods {#download-method}\n\nThe download system implements a sequential approach optimized for reliability and respectful API usage:\n\n- **Resilient Downloading**: Automatic retries with exponential backoff (2s â†’ 4s â†’ 8s â†’ 16s â†’ 32s delays) and extended 60-second timeouts for reliable network handling\n\n- **Smart File Management**: Context-aware naming scheme using timestamp + state/scope identifier (e.g., 20040312_ne.html for pre-2007, 20150621_ndis.html for post-2007)\n\n- **Duplicate Prevention**: Automatically skips already downloaded files to prevent redundant operations\n\n- **Progress Tracking**: Real-time download status with completion counters and detailed success/failure reporting\n\n- **Rate Limiting**: 1.5-second delays between requests to avoid overloading the Wayback Machine servers\n\n::: {#download-helper-codes .cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show downloader helpers code\"}\ndef download_with_retry(url, save_path, max_retries=4, initial_delay=2):\n    \"\"\"\n    Download an archived snapshot with retries and exponential backoff.\n    \"\"\"\n    delay = initial_delay\n    for attempt in range(max_retries):\n        try:\n            resp = requests.get(url, timeout=120)\n            resp.raise_for_status()\n            \n            # Validate content is reasonable\n            if len(resp.content) < 500:\n                if b\"error\" in resp.content.lower() or b\"not found\" in resp.content.lower():\n                    raise requests.exceptions.RequestException(f\"Possible error page: {len(resp.content)} bytes\")\n            \n            # Save to disk\n            with open(save_path, \"wb\") as f:\n                f.write(resp.content)\n            print(f\"âœ“ Downloaded: {save_path}\")\n            return True\n        except requests.exceptions.RequestException as e:\n            if attempt == max_retries - 1:\n                print(f\"âœ— Final attempt failed for {url}: {str(e)}\")\n                return False\n            print(f\"! Attempt {attempt+1} failed, retrying in {delay}s...\")\n            time.sleep(delay)\n            delay *= 2\n    return False\n\n\ndef snapshot_to_filepath(row):\n    \"\"\"\n    Map a snapshot record to a local filename.\n    Format: {timestamp}_{state_or_scope}.html\n    \"\"\"\n    ts = row[\"timestamp\"]\n    original = row[\"original\"]\n    \n    # derive name from FBI URL pattern\n    if \"/codis/\" in original and original.endswith(\".htm\"):\n        # Pre-2007: use last part (state code or army/dc)\n        suffix = Path(original).stem\n    else:\n        # Post-2007: consolidated page, use 'ndis'\n        suffix = \"ndis\"\n    \n    save_dir = HTML_DIR\n    save_dir.mkdir(parents=True, exist_ok=True)  # Ensure directory exists\n    \n    return save_dir / f\"{ts}_{suffix}.html\"\n```\n:::\n\n\n### Download Execution {#download-exec}\n\nThe download execution phase performs bulk retrieval of historical NDIS snapshots with comprehensive error handling:\n\n- **Sequential Processing**: Iterates through snapshot DataFrame chronologically, processing each file individually for maximum reliability\n\n- **URL Construction**: Uses identity flag (id_) in archive URLs to retrieve unmodified original content: https://web.archive.org/web/{timestamp}id_/{original}\n\n- **Binary Preservation**: Saves files as binary content to maintain original encoding and prevent character corruption\n\n- **Comprehensive Logging**: Provides real-time progress updates with attempt counters and final success/failure statistics\n\n- **Flexible Limiting**: Optional download limits for testing or partial processing\n\n::: {#execute-download .cell message='false' execution_count=10}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show download execution code\"}\ndef download_snapshots(snap_df, limit=None):\n    \"\"\"\n    Iterate over snapshot DataFrame and download archived HTML pages.\n    \"\"\"\n    total = len(snap_df) if limit is None else min(limit, len(snap_df))\n    print(f\"Starting download of {total} snapshots at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n    \n    successes, failures = 0, 0\n    failed_downloads = []\n    \n    for i, row in enumerate(snap_df.head(total).itertuples(index=False), 1):\n        ts, original, _, _ = row\n        save_path = snapshot_to_filepath(row._asdict())\n        \n        if save_path.exists() and save_path.stat().st_size > 1000:\n            print(f\"- [{i}/{total}] Already exists: {save_path}\")\n            successes += 1\n            continue\n        \n        archive_url = f\"https://web.archive.org/web/{ts}id_/{original}\"\n        print(f\"- [{i}/{total}] Downloading {archive_url}\")\n        \n        if download_with_retry(archive_url, save_path):\n            successes += 1\n        else:\n            failures += 1\n            failed_downloads.append({\n                'timestamp': ts,\n                'url': original,\n                'archive_url': archive_url\n            })\n        \n        if i % 50 == 0:\n            print(f\"Progress: {i}/{total} ({i/total*100:.1f}%) - {successes} success, {failures} failures\")\n        \n        time.sleep(1.5)\n    \n    # Save failure report\n    if failed_downloads:\n        failure_report = {\n            'download_completed': datetime.now().isoformat(),\n            'total_attempted': total,\n            'successful': successes,\n            'failed': failures,\n            'failed_downloads': failed_downloads\n        }\n        with open(META_DIR / \"download_failures.json\", 'w') as f:\n            json.dump(failure_report, f, indent=2)\n    \n    print(f\"\\nDownload completed. Success: {successes}, Failures: {failures}\")\n    return successes, failures\n\n\nsuccesses, failures = download_snapshots(snap_df, limit=None)\n```\n:::\n\n\n### Download Validation {#download-validation}\n\nPost-download validation ensures data integrity and identifies potential issues:\n\n- **File Existence Verification**: Checks that all expected files were successfully downloaded to the target directory\n\n- **Content Quality Assessment**: Validates HTML content by examining file headers for proper HTML tags\n\n- **Error Categorization**: Separates missing files from corrupted/non-HTML files for targeted remediation\n\n- **Metadata Generation**: Creates JSON validation reports with detailed statistics and file counts\n\n- **Actionable Reporting**: Provides clear feedback on download success rates and files requiring attention\n\n::: {#verify-downloads .cell results='hold' execution_count=11}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show download validation code\"}\ndef validate_downloads(snap_df):\n    \"\"\"\n    Validate downloaded files exist and are HTML-like.\n    \"\"\"\n    missing, bad_html = [], []\n    \n    for row in snap_df.itertuples(index=False):\n        save_path = snapshot_to_filepath(row._asdict())\n        if not save_path.exists():\n            missing.append(save_path)\n            continue\n        try:\n            file_size = save_path.stat().st_size\n            if file_size < 1000:\n                bad_html.append(save_path)\n                continue\n                \n            with open(save_path, \"rb\") as f:\n                start = f.read(500).lower()\n            if b\"<html\" not in start and b\"<!doctype\" not in start:\n                bad_html.append(save_path)\n        except Exception:\n            bad_html.append(save_path)\n    \n    print(f\"Validation results â†’ Missing: {len(missing)}, Bad HTML: {len(bad_html)}\")\n    return missing, bad_html\n\n# Run validation\nmissing, bad_html = validate_downloads(snap_df)\n\n# Save validation summary\nvalidation_meta = {\n    \"validation_performed\": datetime.now().isoformat(),\n    \"missing_files\": len(missing),\n    \"bad_html_files\": len(bad_html),\n    \"total_snapshots\": len(snap_df),\n    \"successful_downloads\": successes,\n    \"success_rate\": f\"{(successes/len(snap_df))*100:.1f}%\"\n}\nwith open(META_DIR / \"validation_metadata.json\", \"w\") as f:\n    json.dump(validation_meta, f, indent=2)\n```\n:::\n\n\n::: {#d7afcc23 .cell execution_count=12}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show validation report code\"}\n# Save and print report\nreport_path = META_DIR / f\"validation_metadata.json\"\nwith open(report_path, 'w') as f:\n    json.dump(validation_meta, f, indent=2)\n\nprint(f\"\\n{'='*60}\")\nprint(\"DOWNLOAD VALIDATION REPORT\")\nprint(f\"{'='*60}\")\nprint(f\"  Missing files: {validation_meta['missing_files']}/{validation_meta['total_snapshots']}\")\nprint(f\"  Bad HTML files: {validation_meta['bad_html_files']}/{validation_meta['total_snapshots']}\")\nprint(f\"  Successful downloads: {validation_meta['successful_downloads']}/{validation_meta['total_snapshots']} ({validation_meta['success_rate']})\")\n\nprint(f\"\\nFull report: {report_path}\")\n```\n:::\n\n\n## Data Extraction {#extraction-pipe}\n\nThe data extraction pipeline converts downloaded HTML snapshots into structured tabular data, handling the evolution of FBI NDIS reporting formats across different time periods.\n\n### Extraction Overview {#extraction-overview}\nThe extraction system processes three distinct eras of NDIS reporting:\n\n1. Pre-2007 Era: Basic statistics without date metadata or arrestee data\n\n2. 2007-2011 Era: Includes \"as of\" dates but no arrestee profiles\n\n3. Post-2012 Era: Complete format with all profile types and consistent dating\n\n**Key Features:**\n\n- **Era-Aware Processing:** Automatically routes files to appropriate parsers based on timestamp\n\n- **Metadata Recovery:** Extracts report dates from \"as of\" statements when available\n\n- **Complete Traceability:** Links each record to its source HTML file and original URL\n\n- **Robust Error Handling:** Processes files individually to prevent single failures from stopping the entire batch\n\n### Core Parser Functions {#parser-functions}\nEssential text processing utilities for NDIS data extraction:\n\n- **HTML Cleaning**: Removes navigation, scripts, and styling elements to focus on data content\n\n- **Date Extraction**: Identifies and parses \"as of\" dates using multiple pattern variations\n\n- **Text Normalization**: Standardizes whitespace and jurisdiction name formatting\n\n- **Encoding Handling**: Manages various character encodings found in historical snapshots\n\n::: {#helper-functions .cell execution_count=13}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show setup and normalization functions code\"}\ndef extract_ndis_metadata(html_content):\n    \"\"\"\n    Extract key metadata from NDIS HTML content including report dates\n    \n    Returns:\n    --------\n    dict:\n        - report_month: Month from \"as of\" statement (None if not found)\n        - report_year: Year from \"as of\" statement (None if not found)  \n        - clean_text: Normalized text content\n    \"\"\"\n    \n    # Multiple patterns to catch different \"as of\" formats\n    date_patterns = [\n        r'[Aa]s of ([A-Za-z]+) (\\d{4})',           # \"as of November 2008\"\n        r'[Aa]s of ([A-Za-z]+) (\\d{1,2}), (\\d{4})', # \"as of November 15, 2008\"\n        r'Statistics as of ([A-Za-z]+) (\\d{4})',    # \"Statistics as of November 2008\"\n        r'Statistics as of ([A-Za-z]+) (\\d{1,2}), (\\d{4})' # \"Statistics as of November 15, 2008\"\n    ]\n    \n    report_month = None\n    report_year = None\n    \n    # Find first occurrence of any date pattern\n    for pattern in date_patterns:\n        date_match = re.search(pattern, html_content)\n        if date_match:\n            month_str = date_match.group(1)\n            if len(date_match.groups()) == 2:  # Month + Year only\n                year_str = date_match.group(2)\n            else:  # Month + Day + Year\n                year_str = date_match.group(3)\n            \n            # Convert month name to number\n            try:\n                month_num = pd.to_datetime(f\"{month_str} 1, 2000\").month\n                report_month = month_num\n                report_year = int(year_str)\n                break\n            except:\n                continue\n    \n    # Clean HTML and normalize text\n    soup = BeautifulSoup(html_content, 'lxml')\n    \n    # Remove scripts, styles, and navigation elements\n    for element in soup(['script', 'style', 'nav', 'header', 'footer']):\n        element.decompose()\n    \n    # Get clean text with normalized whitespace\n    clean_text = re.sub(r'\\s+', ' ', soup.get_text(' ', strip=True))\n    \n    return {\n        'report_month': report_month,\n        'report_year': report_year, \n        'clean_text': clean_text\n    }\n\ndef standardize_jurisdiction_name(name):\n    \"\"\"\n    Clean and standardize jurisdiction names for consistency\n    \"\"\"\n    if not name:\n        return name\n        \n    # Remove common prefixes and suffixes\n    name = re.sub(r'^.*?(Back to top|Tables by NDIS Participant|ation\\.)\\s*', \n                 '', name, flags=re.I).strip()\n    \n    # Standardize known variants\n    replacements = {\n        'D.C./FBI Lab': 'DC/FBI Lab',\n        'D.C./Metro PD': 'DC/Metro PD', \n        'US Army': 'U.S. Army',\n        'D.C.': 'DC'\n    }\n    \n    for old, new in replacements.items():\n        name = name.replace(old, new)\n    \n    return name.strip()\n\ndef extract_original_url_from_filename(html_file):\n    \"\"\"\n    Reconstruct original URL from filename and timestamp\n    \"\"\"\n    filename = html_file.name\n    timestamp = filename.split('_')[0]  # Get timestamp part\n    \n    # Determine URL pattern based on filename suffix\n    if filename.endswith('_ndis.html'):\n        # Post-2007 consolidated format - use most common URL pattern\n        return \"https://www.fbi.gov/services/laboratory/biometric-analysis/codis/ndis-statistics\"\n    else:\n        # Pre-2007 state-specific format\n        state_code = filename.split('_')[1].replace('.html', '')\n        return f\"http://www.fbi.gov/hq/lab/codis/{state_code}.htm\"\n```\n:::\n\n\n### Era-Specific Parsers {#era-parsers}\n\nTime-period-adapted parsing logic that accounts for format evolution:\n\n**Pre-2007 Parser:**\n\n- Extracts basic statistics from state-specific pages\n\n- Uses timestamp-derived year (no report dates available)\n\n- Sets arrestee counts to 0 (not reported in this era)\n\n- Handles missing NDIS labs and investigations data\n\n**2008-2011 Parser:**\n\n- Processes consolidated pages with \"Back to top\" section dividers\n\n- Extracts month and year from report dates\n\n- Handles missing arrestee data (sets to 0)\n\n- Multiple pattern matching for jurisdiction identification\n\n**2012-2016 Parser:**\n\n- First era with arrestee data extraction\n\n- Processes consolidated pages with \"Back to top\" section dividers\n\n- Multiple pattern matching for jurisdiction identification\n\n- Complete jurisdiction coverage with standardized names\n\n**Post-2017 Parser:**\n\n- Modern format with consistent structure and all fields\n\n- Robust regex pattern for reliable extraction\n\n- Full feature extraction including arrestee profiles\n\n- Complete jurisdiction coverage with standardized names\n\n::: {#parser-functions .cell execution_count=14}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show parser functions code\"}\ndef parse_pre2007_ndis(text, timestamp, html_file, report_month=None, report_year=None):\n    \"\"\"\n    Parse NDIS snapshots from 2001-2007 era (state-specific pages)\n    HTML has table structure with state name followed by data rows\n    \"\"\"\n    records = []\n    \n    # Clean up the text for better matching\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Try multiple patterns to extract state name\n    jurisdiction = None\n    \n    # Pattern 1: State name in large font (from graphic alt text or heading)\n    # Looking for patterns like \"Graphic of Pennsylvania Pennsylvania\" or just the state name\n    state_pattern1 = r'(?:Graphic of|alt=\")([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*?)(?:\"|>|\\s+Offender)'\n    state_match = re.search(state_pattern1, text, re.IGNORECASE)\n    \n    if state_match:\n        jurisdiction = state_match.group(1).strip()\n    \n    # Pattern 2: Try to extract from filename as fallback\n    if not jurisdiction:\n        # Extract state code from filename (e.g., \"20010715040342_pa.html\")\n        filename = html_file.name\n        state_code_match = re.search(r'_([a-z]{2,5})\\.html$', filename)\n        if state_code_match:\n            state_code = state_code_match.group(1)\n            # Map common state codes to names\n            state_map = {\n                'pa': 'Pennsylvania', 'nc': 'North Carolina', 'ct': 'Connecticut',\n                'wv': 'West Virginia', 'ks': 'Kansas', 'nd': 'North Dakota',\n                'wy': 'Wyoming', 'ky': 'Kentucky', 'la': 'Louisiana',\n                'dc': 'DC/FBI Lab', 'de': 'Delaware', 'ne': 'Nebraska',\n                'sc': 'South Carolina', 'tn': 'Tennessee', 'ma': 'Massachusetts',\n                'fl': 'Florida', 'nh': 'New Hampshire', 'sd': 'South Dakota',\n                'me': 'Maine', 'hi': 'Hawaii', 'nm': 'New Mexico',\n                'al': 'Alabama', 'tx': 'Texas', 'mi': 'Michigan',\n                'ut': 'Utah', 'ar': 'Arkansas', 'az': 'Arizona',\n                'mo': 'Missouri', 'ny': 'New York', 'mn': 'Minnesota',\n                'vt': 'Vermont', 'id': 'Idaho', 'oh': 'Ohio',\n                'ok': 'Oklahoma', 'or': 'Oregon', 'ca': 'California',\n                'il': 'Illinois', 'wi': 'Wisconsin', 'ms': 'Mississippi',\n                'wa': 'Washington', 'mt': 'Montana', 'in': 'Indiana',\n                'co': 'Colorado', 'va': 'Virginia', 'ga': 'Georgia',\n                'ak': 'Alaska', 'md': 'Maryland', 'nj': 'New Jersey',\n                'nv': 'Nevada', 'ri': 'Rhode Island', 'ia': 'Iowa',\n                'army': 'U.S. Army'\n            }\n            jurisdiction = state_map.get(state_code, state_code.upper())\n    \n    if jurisdiction:\n        jurisdiction = standardize_jurisdiction_name(jurisdiction)\n        \n        # Extract individual values with more flexible patterns\n        # These patterns work with the table structure in your example\n        offender_match = re.search(r'Offender\\s+Profiles?\\s+([\\d,]+)', text, re.IGNORECASE)\n        forensic_match = re.search(r'Forensic\\s+(?:Samples?|Profiles?)\\s+([\\d,]+)', text, re.IGNORECASE)\n        \n        # NDIS labs can appear as \"NDIS Participating Labs\" or just \"Number of CODIS Labs\"\n        ndis_labs_match = re.search(r'(?:NDIS\\s+Participating\\s+Labs?|Number\\s+of\\s+CODIS\\s+Labs?)\\s+(\\d+)', text, re.IGNORECASE)\n        \n        investigations_match = re.search(r'Investigations?\\s+Aided\\s+([\\d,]+)', text, re.IGNORECASE)\n        \n        if offender_match and forensic_match:\n            records.append({\n                'timestamp': timestamp,\n                'report_month': None,  # Not available pre-2007\n                'report_year': None,   # Not available pre-2007\n                'jurisdiction': jurisdiction,\n                'offender_profiles': int(offender_match.group(1).replace(',', '')),\n                'arrestee': 0,  # Not reported pre-2007\n                'forensic_profiles': int(forensic_match.group(1).replace(',', '')),\n                'ndis_labs': int(ndis_labs_match.group(1)) if ndis_labs_match else 0,\n                'investigations_aided': int(investigations_match.group(1).replace(',', '')) if investigations_match else 0\n            })\n    \n    return records\n\ndef parse_2008_2011_ndis(text, timestamp, html_file, report_month=None, report_year=None):\n    \"\"\"\n    Parse NDIS snapshots from 2008-2011 era\n    Consolidated page with state anchors and \"Back to top\" links\n    No arrestee data in this period\n    \"\"\"\n    records = []\n    \n    # Clean up the text\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Split by \"Back to top\" to isolate each state section\n    sections = re.split(r'Back\\s+to\\s+top', text, flags=re.IGNORECASE)\n    \n    for section in sections:\n        # Look for state name pattern (appears as anchor or bold text)\n        # Try multiple patterns to catch different HTML formats\n        jurisdiction = None\n        \n        # Pattern 1: <a name=\"State\"></a><strong>State</strong>\n        state_match = re.search(r'<a\\s+name=\"([^\"]+)\"[^>]*>.*?(?:<strong>|<b>)\\s*([A-Z][^<]+?)(?:</strong>|</b>)', section, re.IGNORECASE)\n        if state_match:\n            jurisdiction = state_match.group(2).strip()\n        \n        # Pattern 2: Just the state name in bold/strong tags before \"Statistical Information\"\n        if not jurisdiction:\n            state_match = re.search(r'(?:<strong>|<b>)\\s*([A-Z][^<]+?)(?:</strong>|</b>).*?Statistical\\s+Information', section, re.IGNORECASE)\n            if state_match:\n                jurisdiction = state_match.group(1).strip()\n        \n        # Pattern 3: State name without tags before \"Statistical Information\"\n        if not jurisdiction:\n            state_match = re.search(r'([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\s+Statistical\\s+Information', section)\n            if state_match:\n                jurisdiction = state_match.group(1).strip()\n        \n        if jurisdiction:\n            jurisdiction = standardize_jurisdiction_name(jurisdiction)\n            \n            # Extract values\n            offender_match = re.search(r'Offender\\s+Profiles?\\s+([\\d,]+)', section, re.IGNORECASE)\n            forensic_match = re.search(r'Forensic\\s+(?:Samples?|Profiles?)\\s+([\\d,]+)', section, re.IGNORECASE)\n            ndis_labs_match = re.search(r'NDIS\\s+Participating\\s+Labs?\\s+(\\d+)', section, re.IGNORECASE)\n            investigations_match = re.search(r'Investigations?\\s+Aided\\s+([\\d,]+)', section, re.IGNORECASE)\n            \n            if offender_match and forensic_match:\n                records.append({\n                    'timestamp': timestamp,\n                    'report_month': report_month,\n                    'report_year': report_year,\n                    'jurisdiction': jurisdiction,\n                    'offender_profiles': int(offender_match.group(1).replace(',', '')),\n                    'arrestee': 0,  # Not reported 2008-2011\n                    'forensic_profiles': int(forensic_match.group(1).replace(',', '')),\n                    'ndis_labs': int(ndis_labs_match.group(1)) if ndis_labs_match else 0,\n                    'investigations_aided': int(investigations_match.group(1).replace(',', '')) if investigations_match else 0\n                })\n    \n    return records\n\n\ndef parse_2012_2016_ndis(text, timestamp, html_file, report_month=None, report_year=None):\n    \"\"\"\n    Parse NDIS snapshots from 2012-2016 era\n    Includes arrestee data for the first time\n    \"\"\"\n    records = []\n    \n    # Clean up the text\n    text = re.sub(r'\\s+', ' ', text)\n    \n    # Split by \"Back to top\" to isolate each state section\n    sections = re.split(r'Back\\s+to\\s+top', text, flags=re.IGNORECASE)\n    \n    for section in sections:\n        jurisdiction = None\n        \n        # Look for state name patterns\n        # Pattern 1: <a name=\"State\"></a><b>State</b>\n        state_match = re.search(r'<a\\s+name=\"([^\"]+)\"[^>]*>.*?<b>([^<]+?)</b>', section, re.IGNORECASE)\n        if state_match:\n            jurisdiction = state_match.group(2).strip()\n        \n        # Pattern 2: Just bold state name\n        if not jurisdiction:\n            state_match = re.search(r'<b>([A-Z][^<]+?)</b>.*?Statistical\\s+Information', section, re.IGNORECASE)\n            if state_match:\n                jurisdiction = state_match.group(1).strip()\n        \n        # Pattern 3: State name without tags\n        if not jurisdiction:\n            state_match = re.search(r'([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\s+Statistical\\s+Information', section)\n            if state_match:\n                jurisdiction = state_match.group(1).strip()\n        \n        if jurisdiction:\n            jurisdiction = standardize_jurisdiction_name(jurisdiction)\n            \n            # Extract values INCLUDING arrestee which appears starting 2012\n            offender_match = re.search(r'Offender\\s+Profiles?\\s+([\\d,]+)', section, re.IGNORECASE)\n            arrestee_match = re.search(r'Arrestee\\s+([\\d,]+)', section, re.IGNORECASE)\n            forensic_match = re.search(r'Forensic\\s+Profiles?\\s+([\\d,]+)', section, re.IGNORECASE)\n            ndis_labs_match = re.search(r'NDIS\\s+Participating\\s+Labs?\\s+(\\d+)', section, re.IGNORECASE)\n            investigations_match = re.search(r'Investigations?\\s+Aided\\s+([\\d,]+)', section, re.IGNORECASE)\n            \n            if offender_match and forensic_match:\n                records.append({\n                    'timestamp': timestamp,\n                    'report_month': report_month,\n                    'report_year': report_year,\n                    'jurisdiction': jurisdiction,\n                    'offender_profiles': int(offender_match.group(1).replace(',', '')),\n                    'arrestee': int(arrestee_match.group(1).replace(',', '')) if arrestee_match else 0,\n                    'forensic_profiles': int(forensic_match.group(1).replace(',', '')),\n                    'ndis_labs': int(ndis_labs_match.group(1)) if ndis_labs_match else 0,\n                    'investigations_aided': int(investigations_match.group(1).replace(',', '')) if investigations_match else 0\n                })\n    \n    return records\n\n\ndef parse_post2017_ndis(text, timestamp, html_file, report_month=None, report_year=None):\n    \"\"\"\n    Parse NDIS snapshots from 2017+ era\n    Modern format with consistent structure and all fields\n    Keep using your existing working pattern for this era\n    \"\"\"\n    records = []\n    \n    # This is your existing working pattern - don't change it\n    pattern = re.compile(\n        r'([A-Z][\\w\\s\\.\\-\\'\\/&\\(\\)]+?)Statistical Information'\n        r'.*?Offender Profiles\\s+([\\d,]+)'\n        r'.*?Arrestee\\s+([\\d,]+)'\n        r'.*?Forensic Profiles\\s+([\\d,]+)'\n        r'.*?NDIS Participating Labs\\s+(\\d+)'\n        r'.*?Investigations Aided\\s+([\\d,]+)',\n        re.IGNORECASE | re.DOTALL\n    )\n    \n    for match in pattern.finditer(text):\n        records.append({\n            'timestamp': timestamp,\n            'report_month': report_month,\n            'report_year': report_year,\n            'jurisdiction': standardize_jurisdiction_name(match.group(1)),\n            'offender_profiles': int(match.group(2).replace(',', '')),\n            'arrestee': int(match.group(3).replace(',', '')),\n            'forensic_profiles': int(match.group(4).replace(',', '')),\n            'ndis_labs': int(match.group(5)),\n            'investigations_aided': int(match.group(6).replace(',', ''))\n        })\n    \n    return records\n```\n:::\n\n\n### Output Schema {#output-schema}\n\nEach extracted record contains the following standardized fields:\n\n| Field | Description | Availability |\n| :--- | :--- | :--- |\n| `timestamp` | Wayback capture timestamp (YYYYMMDDHHMMSS) | All eras |\n| `report_month` | Month from \"as of\" statement | 2007+ only |\n| `report_year` | Year from \"as of\" statement | 2007+ only |\n| `jurisdiction` | Standardized state/agency name | All eras |\n| `offender_profiles` | DNA profiles from convicted offenders | All eras |\n| `arrestee` | DNA profiles from arrestees | 2012+ only |\n| `forensic_profiles` | Crime scene DNA profiles | All eras |\n| `ndis_labs` | Number of participating laboratories | All eras |\n| `investigations_aided` | Cases assisted by DNA matches | All eras |\n\n### Batch Processing {#batch-processing}\n\nThe complete extraction workflow:\n\n**File Discovery**\n\n- Scans download directory for HTML files\n\n- Sorts chronologically for consistent processing\n\n- Tracks progress with detailed logging\n\n**Individual File Processing**\n\n- Reads HTML content with encoding fallback\n\n- Extracts metadata and cleans content\n\n- Routes to era-appropriate parser based on timestamp\n\n- Captures source file information for traceability\n\n**Data Consolidation**\n\n- Combines all records into single DataFrame\n\n- Adds derived timestamp columns (capture_date, year)\n\n- Validates data integrity and completeness\n\n- Sorts by capture date and jurisdiction for consistency\n\n::: {#batch-processing .cell execution_count=15}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show batch processing function code\"}\ndef process_ndis_snapshot(html_file):\n    \"\"\"\n    Convert single NDIS HTML file to structured data\n    Routes to appropriate parser based on timestamp year\n    \"\"\"\n    try:\n        # Read HTML content with multiple encoding attempts\n        content = None\n        for encoding in ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']:\n            try:\n                content = html_file.read_text(encoding=encoding)\n                break\n            except UnicodeDecodeError:\n                continue\n        \n        if content is None:\n            # Final fallback with error ignoring\n            content = html_file.read_text(encoding='latin-1', errors='ignore')\n        \n        # Check if file has reasonable content\n        if len(content.strip()) < 100:\n            print(f\"âš   Small file detected: {html_file.name} ({len(content)} chars)\")\n            return []\n        \n        # Extract metadata\n        metadata = extract_ndis_metadata(content)\n        timestamp = html_file.stem.split('_')[0]\n        \n        try:\n            year = int(timestamp[:4])\n        except ValueError:\n            print(f\"âš   Invalid timestamp in filename: {html_file.name}\")\n            return []\n        \n        # Route to appropriate parser with fallback\n        records = []\n        parser_used = None\n        \n        if year <= 2007:\n            records = parse_pre2007_ndis(metadata['clean_text'], timestamp, html_file,\n                                       metadata['report_month'], metadata['report_year'])\n            parser_used = \"pre2007\"\n        elif year <= 2011:\n            records = parse_2008_2011_ndis(metadata['clean_text'], timestamp, html_file,\n                                         metadata['report_month'], metadata['report_year'])\n            parser_used = \"2008-2011\"\n        elif year <= 2016:\n            records = parse_2012_2016_ndis(metadata['clean_text'], timestamp, html_file,\n                                         metadata['report_month'], metadata['report_year'])\n            parser_used = \"2012-2016\"\n        else:\n            records = parse_post2017_ndis(metadata['clean_text'], timestamp, html_file,\n                                        metadata['report_month'], metadata['report_year'])\n            parser_used = \"post2017\"\n        \n        if records:\n            # Add parser info for debugging\n            for record in records:\n                record['parser_used'] = parser_used\n                record['source_file'] = html_file.name\n            return records\n        else:\n            print(f\"âš   No records extracted from {html_file.name} (year: {year}, parser: {parser_used})\")\n            return []\n            \n    except Exception as e:\n        print(f\"âŒ Error processing {html_file.name}: {str(e)}\")\n        return []\n\n\ndef process_all_snapshots():\n    \"\"\"\n    Process all downloaded snapshots into a single DataFrame\n    \n    Returns:\n    --------\n    pd.DataFrame\n        Combined dataset with all snapshots\n    \"\"\"\n    all_records = []\n    html_files = sorted(NDIS_SNAPSHOTS_DIR.glob(\"*.html\"))\n    \n    print(f\"Processing {len(html_files)} HTML snapshots...\")\n    \n    successful_files = 0\n    failed_files = 0\n    failed_list = []\n    \n    for html_file in tqdm(html_files, desc=\"Extracting NDIS data\"):\n        records = process_ndis_snapshot(html_file)\n        if records:\n            all_records.extend(records)\n            successful_files += 1\n        else:\n            failed_files += 1\n            failed_list.append(html_file.name)\n    \n    print(f\"Processing complete: {successful_files} successful, {failed_files} failed\")\n    \n    # Save failure report\n    if failed_list:\n        failure_report_path = META_DIR / \"processing_failures.json\"\n        with open(failure_report_path, 'w') as f:\n            json.dump({\n                'timestamp': datetime.now().isoformat(),\n                'total_files': len(html_files),\n                'successful': successful_files,\n                'failed': failed_files,\n                'failed_files': failed_list[:100]  # Limit to first 100\n            }, f, indent=2)\n        print(f\"Failure report saved: {failure_report_path}\")\n    \n    if not all_records:\n        print(\"Warning: No records extracted!\")\n        return pd.DataFrame()\n    \n    df = pd.DataFrame(all_records)\n    \n    # Add derived date columns\n    df['capture_date'] = pd.to_datetime(df['timestamp'], format='%Y%m%d%H%M%S', errors='coerce')\n    df['capture_year'] = df['capture_date'].dt.year\n    \n    # Remove any records with invalid dates\n    invalid_dates = df['capture_date'].isna().sum()\n    if invalid_dates > 0:\n        print(f\"âš   Removed {invalid_dates} records with invalid dates\")\n        df = df[df['capture_date'].notna()]\n    \n    return df.sort_values(['capture_date', 'jurisdiction']).reset_index(drop=True)\n```\n:::\n\n\n### Export & Validation {#exp-valid}\n\nStructured output generation with comprehensive quality control:\n\n**Validation Checks:**\n\n- Verifies all required columns are present\n\n- Checks for null values in critical fields\n\n- Validates numeric ranges (non-negative counts)\n\n- Confirms timestamp format consistency\n\n- Ensures jurisdiction names contain valid characters\n\n**Export Features**\n\n- Saves as UTF-8 encoded CSV for maximum compatibility\n\n- Generates timestamped filenames for version control\n\n- Creates metadata summary with file statistics\n\n- Performs round-trip validation to confirm data integrity\n\n**Quality Metrics**\n\n- Records total file count and processing success rate\n\n- Tracks temporal coverage (earliest to latest snapshots)\n\n- Documents jurisdiction coverage across time periods\n\n- Reports data completeness by era and field\n\n::: {#export-data .cell execution_count=16}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show execution function code\"}\ndef export_ndis_data(df, output_dir=OUTPUT_DIR):\n    \"\"\"\n    Export processed NDIS data with comprehensive metadata\n    \"\"\"\n    if df.empty:\n        print(\"Warning: DataFrame is empty, skipping export\")\n        return None\n    \n    # Generate output filename\n    export_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    csv_path = output_dir / f\"ndis_data_raw.csv\"\n    \n    # Export main dataset\n    df.to_csv(csv_path, index=False, encoding='utf-8')\n    \n    # Calculate export metadata\n    file_size_mb = csv_path.stat().st_size / (1024 * 1024)\n    \n    export_metadata = {\n        'export_timestamp': export_timestamp,\n        'export_path': str(csv_path.resolve()),\n        'record_count': len(df),\n        'file_size_mb': round(file_size_mb, 2),\n        'unique_snapshots': df['timestamp'].nunique(),\n        'unique_jurisdictions': df['jurisdiction'].nunique(),\n        'date_coverage': {\n            'earliest_capture': df['capture_date'].min().isoformat(),\n            'latest_capture': df['capture_date'].max().isoformat(),\n            'span_years': df['capture_year'].max() - df['capture_year'].min() + 1\n        },\n        'data_completeness': {\n            'with_report_dates': len(df[df['report_year'].notna()]),\n            'with_arrestee_data': len(df[df['arrestee'] > 0]),\n            'total_investigations_aided': int(df['investigations_aided'].sum())\n        }\n    }\n    \n    # Save metadata\n    metadata_path = META_DIR / f\"ndis_export_metadata_{export_timestamp}.json\"\n    import json\n    with open(metadata_path, 'w') as f:\n        json.dump(export_metadata, f, indent=2, default=str)\n    \n    print(f\"âœ“ Data exported: {csv_path}\")\n    print(f\"âœ“ Metadata saved: {metadata_path}\")\n    print(f\"âœ“ Export summary: {len(df):,} records, {file_size_mb:.1f} MB\")\n    \n    return export_metadata\n```\n:::\n\n\n**Data integrity validation**\n\n- **Schema Checking:** Ensures proper field types and formats.\n\n- **Null Validation:** Confirms mandatory fields are populated.\n\n- **Value Sanity Checks:** Verifies non-negative numbers.\n\n::: {#verify-export .cell execution_count=17}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show validation function code\"}\ndef validate_extracted_data(df):\n    \"\"\"\n    Comprehensive validation of extracted NDIS data\n    \"\"\"\n    print(\"Validating extracted data...\")\n    \n    # Required columns check\n    required_cols = [\n        'timestamp', 'jurisdiction',\n        'offender_profiles', 'arrestee', 'forensic_profiles', \n        'ndis_labs', 'investigations_aided'\n    ]\n    \n    validation_results = {}\n    \n    # Check column presence\n    missing_cols = set(required_cols) - set(df.columns)\n    validation_results['missing_columns'] = list(missing_cols)\n    \n    if missing_cols:\n        print(f\"âœ— Missing required columns: {missing_cols}\")\n        return validation_results\n    \n    # Data quality checks\n    validation_results['total_records'] = len(df)\n    validation_results['unique_timestamps'] = df['timestamp'].nunique()\n    validation_results['unique_jurisdictions'] = df['jurisdiction'].nunique()\n    validation_results['date_range'] = {\n        'earliest': df['capture_date'].min().strftime('%Y-%m-%d'),\n        'latest': df['capture_date'].max().strftime('%Y-%m-%d')\n    }\n       \n    # Value range checks\n    numeric_cols = ['offender_profiles', 'arrestee', 'forensic_profiles', 'ndis_labs', 'investigations_aided']\n    negative_values = {}\n    for col in numeric_cols:\n        negative_count = (df[col] < 0).sum()\n        if negative_count > 0:\n            negative_values[col] = negative_count\n    validation_results['negative_values'] = negative_values\n    \n    # Era-specific validation\n    pre2007_count = len(df[df['capture_year'] < 2007])\n    arrestee_pre2012 = len(df[(df['capture_year'] < 2012) & (df['arrestee'] > 0)])\n    \n    validation_results['era_checks'] = {\n        'pre2007_records': pre2007_count,\n        'arrestee_before_2012': arrestee_pre2012  # Should be 0\n    }\n    \n    # Print summary\n    print(f\"âœ“ Total records: {validation_results['total_records']:,}\")\n    print(f\"âœ“ Unique snapshots: {validation_results['unique_timestamps']}\")\n    print(f\"âœ“ Unique jurisdictions: {validation_results['unique_jurisdictions']}\")\n    print(f\"âœ“ Date range: {validation_results['date_range']['earliest']} to {validation_results['date_range']['latest']}\")\n    \n    if validation_results['negative_values']:\n        print(f\"! Negative values found: {validation_results['negative_values']}\")\n    \n    if validation_results['era_checks']['arrestee_before_2012'] > 0:\n        print(f\"! Data integrity issue: {validation_results['era_checks']['arrestee_before_2012']} arrestee records found before 2012\")\n    \n    return validation_results\n```\n:::\n\n\n#### Extraction Execution\n\n::: {#extract-execution .cell execution_count=18}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show main execution code\"}\nif __name__ == \"__main__\":\n    # Process all snapshots\n    ndis_data = process_all_snapshots()\n    \n    if not ndis_data.empty:\n        # Validate data quality\n        validation_results = validate_extracted_data(ndis_data)\n        \n        # Export if validation passes\n        export_metadata = export_ndis_data(ndis_data)\n        print(\"\\n\" + \"=\"*50)\n        print(\"EXTRACTION COMPLETE\")\n        print(\"=\"*50)\n        print(f\"Records extracted: {len(ndis_data):,}\")\n        print(f\"Time span: {ndis_data['capture_year'].min()}-{ndis_data['capture_year'].max()}\")\n        print(f\"Jurisdictions: {ndis_data['jurisdiction'].nunique()}\")\n    else:\n        print(\"No data extracted - check HTML files and parsing logic\")\n```\n:::\n\n\n[View Analyses â†’](ndis_analysis.qmd)\n\n",
    "supporting": [
      "ndis_scraping_files"
    ],
    "filters": [],
    "includes": {}
  }
}